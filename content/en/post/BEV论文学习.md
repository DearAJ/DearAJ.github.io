---
date: 2025-02-17T11:00:59-04:00
description: ""
featured_image: "/images/BEVthesis/lucky.jpg"
tags: ["LLM", "CV", "BEV"]
title: "BEV 论文学习"
---

## Vision-Centric BEV Perception: A Survey

许多方法被提出以**解决从透视视图（Perspective View, PV）到 BEV 的转换问题**，本文将它们分为基于几何、基于深度、基于 MLP 和基于 Transformer 的四类方法。

此外，本文还探讨了 BEV 感知的扩展应用，如多任务学习、多模态融合和语义占据预测等。

#### 1. 背景介绍
BEV 感知的核心任务是将 PV 中的**图像序列转换为BEV特征，并在BEV空间中进行感知任务**（如3D目标检测和语义地图生成），能够提供精确的定位和绝对尺度信息，便于多视图、多模态和时间序列数据的融合。

但由于摄像头通常安装在车辆上，捕捉到的图像是透视视图，如何将 PV 转换为 BEV 仍然是一个具有挑战性的问题。

#### 3. 主要方法分类
1. **基于几何的方法**

   + **优势**：这类方法主要依赖于**逆透视映射（IPM）**，通过几何变换将 PV 图像转换为 BEV 图像。
   + **缺陷**：但 IPM 假设地面是平坦的，因此在复杂场景中（如存在高度变化的物体）会产生失真。为了减少失真，一些方法引入了**语义信息**或使用 **GAN** 。

2. **基于深度的方法**

   通过深度估计将 2D 特征提升到 3D 空间，然后通过降维得到 BEV 表示。深度估计可以是显式的（如通过深度图）或隐式的（如通过任务监督）。

   + **点云方法**：将深度图转换为伪 LiDAR 点云，然后使用 LiDAR 检测器进行 3D 检测
   + **体素方法**：将 2D 特征映射到 3D 体素空间，并通过体素特征进行 BEV 感知

3. **基于MLP的方法**

   + **优势**：MLP 方法不依赖于摄像头的几何参数，而是通过学习隐式表示来完成视图转换。
   + **缺陷**：尽管 MLP 具有通用逼近能力，但由于缺乏深度信息和遮挡问题，视图转换仍然具有挑战性。

4. **基于Transformer的方法**：

   + **优势**：利用 Transformer 的交叉注意力机制，直接构建BEV查询并在透视图像中搜索对应的特征。Transformer 方法具有强大的关系建模能力和数据依赖性，能够处理复杂的多视图输入。

   根据查询的粒度，Transformer 方法可以分为稀疏查询、密集查询和混合查询三类。

#### 4. 扩展应用
1. **多任务学习**：BEV 表示可以同时支持多个下游任务，如 3D 目标检测、语义地图分割和运动预测。一些方法提出了统一的框架，通过共享的 BEV 特征同时完成多个任务。

2. **多模态融合**：BEV 表示为多模态数据（如摄像头、LiDAR 和雷达）的融合提供了便利。通过将不同模态的数据转换到 BEV 空间，可以在物理对应关系的基础上进行特征融合。

3. **语义占据预测**：基于 BEV 感知，语义占据预测任务旨在为 3D 空间中的每个占据区域分配语义标签。这类任务能够提供更细粒度的几何和语义信息，适用于复杂的自动驾驶场景。

#### 5. 实验与结果
本文对各类方法在 KITTI、nuScenes 和 Waymo 等数据集上的性能进行了详细比较。

1. 基于 **Transformer** 的方法在 3D 目标检测和语义分割任务中表现优异，尤其是在处理多视图输入时。

2. 引入**时间信息的模型**（如 BEVDet4D 和 BEVFormer）在速度和方向预测方面也有显著提升。

#### 6. 总结与未来方向
1. 进一步提高 BEV 感知的分辨率和计算效率。
2. 探索更高效的多模态融合方法。
3. 利用长时历史信息进行更精确的时间融合。
4. 开发更强大的语义占据预测模型。



&nbsp;

&nbsp;

## Delving into the Devils of Bird's-eye-view Perception: A Review, Evaluation and Recipe

#### 1. **论文概述**
探讨了自动驾驶领域中 BEV 感知的最新研究，还提供了实用的技巧和指南，帮助提升BEV感知任务的性能。

BEV感知在自动驾驶中扮演着重要角色，因为它能够将多传感器（如摄像头、LiDAR等）的数据统一到一个视角下，便于后续的路径规划和控制模块使用。

#### 2. **研究背景与动机**
+ 优势
  + ·**直观且易于融合**：BEV视角能够直观地表示周围环境，且便于多传感器数据的融合。
  + **适合后续模块**：BEV视角下的物体表示非常适合后续的路径规划和控制模块。

+ 问题
  + **视角转换**：如何从透视视角（Perspective View）转换到BEV视角，并重建丢失的3D信息。
  + **BEV网格中的真值标注**：如何获取BEV网格中的真值标注。
  + **多源特征融合**：如何设计管道以融合来自不同传感器和视角的特征。
  + **算法适应性与泛化**：如何在不同传感器配置下适应和泛化算法。

#### 3. **BEV感知的分类**
根据 *输入数据* 的不同，BEV感知研究可以分为三类：
- **BEV Camera**：基于摄像头的3D物体检测和分割。
- **BEV LiDAR**：基于点云的3D物体检测和分割。
- **BEV Fusion**：多传感器的融合机制。

#### 4. **BEV感知的动机**
- **重要性**：

  1. BEV感知能否在学术界和工业界产生实际影响？
  2. 摄像头和LiDAR解决方案之间存在显著的性能差距，如何通过摄像头解决方案达到或超越LiDAR的性能是一个重要的研究方向。

- **研究空间**：

  1. BEV感知中是否存在开放性问题？
  2. 如何从摄像头和LiDAR输入中学习到鲁棒且可泛化的特征表示？

- **研究条件**：

  数据集和基准测试是否已经准备好？大规模且多样化的数据集（如Waymo、nuScenes等）为BEV感知研究提供了坚实的基础。

#### 5. **BEV感知的方法论**
- **BEV Camera**：基于摄像头的3D感知

  核心问题是如何从2D图像中重建3D信息。论文介绍了2D特征提取、视角转换和3D解码器的通用流程。
- **BEV LiDAR**：基于LiDAR的3D感知

  主要分为**Pre-BEV**和**Post-BEV**两种特征提取方式。

  1. Pre-BEV在3D空间中进行特征提取
  2. Post-BEV则直接在BEV空间中进行特征提取。
- **BEV Fusion**：多传感器融合的BEV感知

  核心问题是如何在BEV空间中对齐和融合来自不同传感器的特征。论文还介绍了时间融合的重要性，如何利用历史BEV特征来推断物体的运动状态。

#### 6. **工业界的BEV感知设计**
介绍了特斯拉、Mobileye、Horizon Robotics等公司的BEV感知架构。

这些架构通常包括输入数据、特征提取器、视角转换模块、特征融合模块、时空模块和预测头。工业界的BEV感知设计通常采用Transformer或ViDAR（视觉雷达）进行视角转换，并通过时空融合模块增强感知系统的鲁棒性。

#### 7. **实验评估与实用技巧**
- **数据增强**：在BEV Camera任务中，常见的图像增强方法（如颜色抖动、翻转、多尺度调整等）可以直接应用。在LiDAR分割任务中，随机旋转、缩放、翻转等增强方法也能显著提升模型性能。
- **BEV编码器**：BEVFormer++通过引入BEV查询、空间交叉注意力和时间自注意力机制来提升BEV特征的质量。
- **损失函数**：
  1. 在BEV Camera任务中，除了3D检测的损失函数外，还可以通过2D检测损失和深度监督来提升模型性能。
  2. 在LiDAR分割任务中，Geo损失和Lovasz损失能够有效提升模型的边界识别能力。

#### 8. **未来研究方向**
- **深度估计**：如何设计更准确的深度估计器，特别是在摄像头输入的情况下。
- **多传感器融合机制**：如何设计新颖的融合机制，更好地对齐来自不同传感器的特征表示。
- **参数无关设计**：如何设计参数无关的网络，使得算法性能不受传感器位置或姿态变化的影响，提升算法的泛化能力。
- **基础模型的应用**：如何将基础模型（如Transformer、ViT等）的成功经验应用到BEV感知中，提升多任务学习的能力。





&nbsp;

&nbsp;

[Hierarchical End-to-End Autonomous Driving: Integrating BEV Perception with Deep Reinforcement Learning](https://arxiv.org/abs/2409.17659)

https://blog.csdn.net/soaring_casia/article/details/142756723

## Hierarchical End-to-End Autonomous Driving: Integrating BEV Perception with Deep Reinforcement Learning

#### 1. 研究背景与动机
自动驾驶技术近年来取得了显著进展，传统的模块化方法（如感知、预测、规划和控制）虽然具有较高的可解释性，但**存在误差传播和计算复杂度高的问题**，并且现有的端到端方法通常**忽略了深度强化学习（DRL）中的特征提取与感知模块之间的关键联系**。

本文提出了一种基于 BEV 表示的端到端自动驾驶框架，将DRL的特征提取网络直接映射到感知阶段，提升了系统的可解释性和性能。

#### 2. 研究贡献
1. **基于BEV的特征提取网络**：提出了一个基于鸟瞰图和环绕摄像头的特征提取网络，能够获取车辆周围环境的完整信息，并统一车辆、道路和图像输入的坐标系转换，显著提升了端到端自动驾驶控制的性能。
2. **语义分割解码**：通过语义分割任务解码从环绕摄像头提取的高维环境特征，并将解码信息可视化为环境中的其他车辆，提升了DRL的可解释性。
3. **实验验证**：在CARLA仿真环境中对提出的算法进行了广泛评估，结果表明基于BEV的特征提取网络显著提升了DRL策略网络的性能，减少了碰撞率并提高了驾驶控制的准确性。

#### 3. 方法概述
本文提出的框架结合了BEV表示和深度强化学习，具体方法如下：
- **问题建模**：将自动驾驶任务建模为部分可观测马尔可夫决策过程（POMDP），定义了状态空间、观测空间、动作空间和奖励函数。
- **深度强化学习**：采用近端策略优化（PPO）算法作为核心强化学习方法，结合 Actor-Critic 架构，输入包括道路特征、车辆特征、导航特征以及环绕摄像头的图像。
- **BEV特征提取网络**：通过“Lift”和“Splat”两个步骤，将2D图像转换为3D表示，并投影到BEV网格中，生成统一的环境表示。
- **语义分割解码**：通过语义分割任务解码BEV特征提取网络输出的潜在特征，生成鸟瞰图语义分割结果，提升了系统的可解释性。

#### 4. 实验与结果
本文在CARLA仿真环境中进行了大量实验，验证了所提出方法的有效性：
- **不同地图下的评估**：在7个不同的CARLA地图上测试了算法的性能，结果表明基于BEV的特征提取网络显著降低了碰撞率，提高了相似性和时间步长。
- **高密度交通环境下的评估**：在高密度交通环境下，本文提出的方法（特别是Ours-6）表现出更好的碰撞避免能力，碰撞率平均降低了18%。
- **可解释性评估**：通过语义分割解码BEV特征提取网络的潜在特征，展示了系统在决策过程中的可解释性。

#### 5. 结论
本文的创新点在于将BEV表示与深度强化学习相结合，通过语义分割解码提升了系统的可解释性。实验结果表明，该方法在复杂交通环境下表现出色，尤其是在高密度交通场景中，碰撞率显著降低。然而，本文的实验主要基于仿真环境，未来在实际道路测试中的表现仍需进一步验证。此外，BEV特征提取网络的复杂性可能会增加计算开销，如何在保证性能的同时降低计算成本也是一个值得探讨的问题。未来的工作将集中在改进深度预测和相机参数集成，以进一步提高BEV特征提取的准确性和鲁棒性，并探索在实际驾驶环境中的应用。

总体而言，本文为端到端自动驾驶提供了一种新的思路，具有较高的学术和应用价值。
