---
date: 2025-01-20T11:00:59-04:00
description: "LLM 是 Large Language Model（大型语言模型）的缩写，是一种基于人工智能技术的自然语言处理模型。它通过大量的文本数据进行训练，能够理解和生成人类语言，广泛应用于文本生成、翻译、问答、摘要等任务。"
featured_image: "/images/LLM/taytay.HEIC"
tags: ["deep learning"]
title: "LLM"
---

https://www.bilibili.com/video/BV1uNk1YxEJQ?spm_id_from=333.788.videopod.episodes&vd_source=80aea28698fb0235b45699fc7e6fcdac&p=2

# 概述

## 大模型的演变

大模型的训练整体上分为三个阶段：

1. **预训练**

   在这个阶段它会学习各种不同种类的语料，学习到语言的统计规律和一般知识。

   但是大模型在这个阶段只是学会了补全句子，却没有学会怎么样去领会人类的意图（类似成语接龙）。

2. **SFT（监督微调）**

   在这个阶段大模型可以学习各种人类的对话语料，甚至是非常专业的垂直领域知识。

   但是模型的回答有时候可能并不符合人类的偏好，它可能会输出一些涉黄、涉政、涉暴或者种族歧视等言论。

3. **RLHF（基于人类反馈的强化学习）**

   在这个阶段大模型会针对同一问题进行多次回答，人类会对这些回答打分。

   大模型会在此阶段学习到如何输出分数最高的回答，使得回答更符合人类的偏好。

## 分类

1. 大语言模型（LLM）

   专注于自然语言处理（NLP），旨在处理语言、文章、对话等自然语言文本。

2. 多模态模型

   多模态大模型能够同时处理和理解来自不同感知通道（如文本、图像、音频、视频等）的数据，在这些模态之间建立关联和交互。

## 工作流程

### 分词化与词表映射

分词化（Tokenization）是指将段落和句子分割成更小的分词（token）的过程。

<!--more-->

分词化有不同的粒度分类：

- ﻿词粒度（Word-Level Tokenization）分词化：适用于大多数西方语言，如英语。
- ﻿字符粒度（Character-Level）分词化：中文最直接的分词方法，它是以单个汉字为单位进行分词化。
- ﻿子词粒度（Subword-Level）分词化：将单词分解成更小的单位，比如词根、词缀等。

每一个token都会通过预先设置好的词表，映射为一个 token id，这是token 的“身份证”，一句话最终会被表示为一个元素为token id的列表，供计算机进行下一步处理。





### 文本生成过程

大语言模型根据给定的文本**预测下一个token**。

大模型进行推理时，基于现有的token，根据概率最大原则预测出下一个最有可能的token，然后将该预测的token加入到输入序列中，并将更新后的输入序列继续输入大模型预测下一个token，这个过程叫做**自回归**。

直到输出特殊token（如<EOS>，end of sentence，专门用来控制推理何时结束）或输出长度达到阈值。





