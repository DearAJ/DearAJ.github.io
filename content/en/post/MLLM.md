---
date: 2025-02-09T11:00:59-04:00
description: "MLLM是一种结合了多种模态数据（如文本、图像、音频等）的大型语言模型，通过深度学习算法，将不同模态的数据融合处理，从而实现对复杂信息的综合理解和创造。"
featured_image: "/images/MLLM/lucky.jpg"
tags: ["LLM", "NLP", "CV"]
title: "MLLM"
---

# 1基础

## 1. 特征提取

### **一、CV中的特征提取**
#### **1. 传统方法（手工设计特征）**
**(1) 低级视觉特征**：颜色、纹理、 边缘与形状...

**(2) 中级语义特征**：SIFT（尺度不变特征变换）、SURF（加速鲁棒特征）、LBP（局部二值模式）...

&nbsp;

#### **2. 深度学习方法（自动学习特征）**

##### **(1) 卷积神经网络（CNN）**
- **核心思想**：通过卷积层提取局部特征，池化层降低维度，全连接层进行分类。
- **经典模型**：LeNet-5、AlexNet、VGGNet、ResNet(使用残差可以训练更深的网络)...

  ![1](/images/MLLM/1.png)

##### **(2) 视觉Transformer（ViT）**
- **核心思想**：将图像分割为小块（patches），通过自注意力机制建模全局关系。
- **优势**：无需局部卷积先验，直接建模长距离依赖; 在ImageNet等任务上超越传统CNN。

<!--more-->

&nbsp;


### **二、NLP中的特征提取**

#### **1. 传统方法（基于统计与规则）**
**(1) 词袋模型（Bag of Words, BoW）**: 将文本表示为词汇表中单词的出现频率。

**(2) TF-IDF（词频-逆文档频率）**: 衡量单词在文档中的重要性（TF-IDF值 = 词频 × 逆文档频率）。

**(3) N-gram模型**: 统计连续N个词的组合频率（如Bi-gram、Tri-gram）。

##### **(4) 词嵌入（预训练词向量）**
- **Word2Vec**（2013）：
  - 通过Skip-Gram或CBOW模型，将词映射为低维稠密向量。
  - 相似词在向量空间中距离相近（如“国王-王后≈男人-女人”）。
- **GloVe**（2014）：
  - 基于全局词共现矩阵，结合统计信息和词向量学习。

**(5) 局限性**：无法建模长距离上下文依赖; 词向量静态，无法处理一词多义。

&nbsp;

#### **2. 深度学习方法（上下文感知特征）**

深度学习通过神经网络自动捕捉文本的语义和上下文信息。

##### **(1) 循环神经网络（RNN）**
- **核心思想**：按顺序处理文本，通过隐藏状态传递上下文信息。

- **变体**：
  
  **LSTM**：引入门控机制（输入门、遗忘门、输出门），解决长距离依赖问题; 
  
  **GRU**：简化版LSTM，合并部分门控单元。

##### **(2) Transformer与自注意力**
- **核心思想**：通过自注意力机制（Self-Attention）建模全局上下文关系；并行计算，避免RNN的顺序处理瓶颈。
- **经典模型**：BERT，GPT...

&nbsp;

&nbsp;

### **三、CV与NLP的关联与融合**

1. **多模态学习**：结合图像特征（CNN/ViT）和文本特征（Transformer），实现跨模态任务（如图像描述、视觉问答）。
2. **共享表示空间**：通过对比学习（如CLIP），将图像和文本映射到同一向量空间。
3. **统一架构**：现代多模态模型（如MLLM）使用Transformer统一处理图像和文本。

&nbsp;

## 2. Tansformer

通过 **Cross-Attention**，Transformer 能够灵活地融合不同来源的信息，成为现代多模态模型（如 MLLM）和复杂序列任务的核心组件。

&nbsp;

### 一、自注意力（Self-Attention）的计算过程
1. **输入**：一个序列的嵌入向量（例如单词的向量表示）。
2. **生成 Q, K, V**：
   - 通过线性变换生成 **Query（Q）**、**Key（K）**、**Value（V）** 矩阵。
3. **计算注意力分数**：
   - 对每个 Query，计算它与所有 Key 的点积，再除以 √d_k（d_k 是 Key 的维度），得到注意力分数。
   - 公式：`Attention(Q, K, V) = softmax(QK^T / √d_k) V`
4. **加权聚合**：通过 softmax 归一化后的权重对 Value 加权求和，得到每个位置的输出。

**作用**：Self-Attention 让模型关注同一序列中不同位置的相关性（例如句子中代词与实体的关系）。

&nbsp;

&nbsp;

### 二、交叉注意力（Cross-Attention）的原理

Cross-Attention 是 Self-Attention 的扩展，用于 **不同序列之间的交互**。例如：
- **编码器-解码器结构**：解码器使用 Cross-Attention 将编码器的输出（如源语言特征）与解码器的输入（如目标语言特征）结合。
- **多模态任务**：将文本与图像、音频等不同模态的特征融合。

#### 1. **Cross-Attention 的输入来源**
- **Query（Q）**：来自一个序列（例如解码器的当前输入）。
- **Key（K）和 Value（V）**：来自另一个序列（例如编码器的输出）。

&nbsp;

#### 2. **计算步骤**
假设有两个序列：
- **序列 A**（如编码器输出）：生成 Key 和 Value。
- **序列 B**（如解码器输入）：生成 Query。

**公式**：
$$
CrossAttention(Q_B, K_A, V_A) = softmax(Q_B K_A^T / √d_k) V_A
$$

- **物理意义**：序列 B 的每个位置（Query）通过计算与序列 A 所有位置（Key）的相似度，决定从序列 A 的 Value 中聚合多少信息。

&nbsp;

#### 3. **在 Transformer 中的位置**
- 在 **解码器层** 中，Cross-Attention 位于 Self-Attention 层之后：
  1. 解码器的 Self-Attention 层处理目标序列的内部关系。
  2. Cross-Attention 层将编码器的输出（Key/Value）与解码器的当前状态（Query）结合。
  3. 前馈网络进一步处理。

&nbsp;

&nbsp;

### 三、Cross-Attention 的**数学细节**

1. **维度说明**：
   - 假设编码器输出的特征维度为 `(seq_len_A, d_model)`，解码器输入的特征维度为 `(seq_len_B, d_model)`。
   - 通过线性变换生成：
     - `Q_B: (seq_len_B, d_k)`
     - `K_A: (seq_len_A, d_k)`
     - `V_A: (seq_len_A, d_v)`
2. **计算步骤**：
   - 计算 `Q_B` 和 `K_A` 的点积：`(seq_len_B, seq_len_A)`。
   - 通过 softmax 归一化，得到注意力权重矩阵。
   - 用权重矩阵对 `V_A` 加权求和，得到输出 `(seq_len_B, d_v)`。

Cross-Attention 通过动态计算 Query（目标序列）与 Key/Value（源序列）的注意力权重，实现信息的灵活融合。



&nbsp;

&nbsp;

&nbsp;

&nbsp;

# 2多模态大模型串讲

+ **对齐align**：将不同模态的向量，尽量聚合为同一个向量/尽量靠近。

+ **迁移学习**：在一个大的数据集上，跑自己的模型，得到source model；之后再在自己的数据上微调。
+ **zero-shot**：融合不同类别的特征，加上自己的**描述**，让模型去推理出之前没见过的类。  



&nbsp;

## CLIP

+ clip的核心思想：

  通过**海量**的**弱监督文本**对通过**对比学习**，将图片和文本通过各自的**预训练模型**获得的编码向量在向量空间上**对齐**。







