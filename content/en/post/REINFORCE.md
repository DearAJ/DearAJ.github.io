---
date: 2025-04-01T12:00:59-05:00
description: ""
featured_image: "/images/REINFORCE/pia.jpg"
tags: ["RL"]
title: "REINFORCE"
---

Q-learning、DQN 算法都是**基于价值**（value-based）的方法

+ Q-learning 是处理有限状态的算法
+ DQN 可以用来解决连续状态的问题

在强化学习中，除了基于值函数的方法，还有一支非常经典的方法，那就是**基于策略**（policy-based）的方法。

&nbsp;

对比 value-based 和 policy-based

+ 基于值函数：主要是学习**值函数**，然后根据值函数导出一个策略，学习过程中并不存在一个显式的策略；
+ 基于策略：直接显式地学习一个**目标策略**。策略梯度是基于策略的方法的基础。

&nbsp;

## 1. 策略梯度

+ 将策略参数化：寻找一个最优策略并最大化这个策略在环境中的期望回报，即调整策略参数使平均回报最大化。

+ 策略学习的目标函数

  ![1](/images/REINFORCE/1.png)

  + J(θ) 是策略的目标函数（想要最大化的量）；
  + πθ 是参数为θ的随机性策略，并且处处可微（可以理解为AI的决策规则）；
  + Vπθ(s0) 指从初始状态 s₀ 开始，**遵循策略π能获得的预期总回报**；
  + Es0 是对所有可能的初始状态求期望。

  <!--more-->

+ 对目标函数求导：

  ![2](/images/REINFORCE/2.png)

  + **状态分布νπθ(s)**

    策略 πθ 下状态 s 的**稳态分布**（即在长期运行中，状态 s 出现的概率）

  + **状态-动作值函数Qπθ(s,a)**

    在状态 s 下执行动作 a 后，**按策略 πθ 继续执行能获得的期望回报**

  + **策略梯度 ∇θπθ(a∣s)**

    πθ(a∣s) 是策略在状态 s 下选择动作 a 的概率。

  在每一个状态下，梯度的修改是让策略更多地去采样到带来较高值的动作，更少地去采样到带来较低值的动作。

  注：期望E的下标是πθ，所以策略梯度算法为在线策略（on-policy）算法，即必须使用**当前策略**采样得到的数据来计算梯度。

&nbsp;

## 2. REINFORCE

+ 策略梯度（有限步数的环境）

  ![3](/images/REINFORCE/3.png)

+ 采用蒙特卡洛方法来估计 Qπθ(s,a)。

+ 具体算法流程

  ![4](/images/REINFORCE/4.png)

