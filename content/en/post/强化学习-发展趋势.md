---
date: 2025-04-06T04:00:59-07:00
description: ""
featured_image: "/images/RLdev/pia.jpg"
tags: ["RL"]
title: "强化学习-发展趋势"
---

#### 1阶段

**RL**：一种解决马尔可夫决策过程的方法。

**分类**：value-based算法（DQN）和 policy-based算法（PPO)。

**应用方向**：多智能体强化学习， 安全强化学习等等。

&nbsp;

#### 2 阶段

强化学习应用的论文描述严格（必须有以下内容）：

1. 非常准确的状态空间和动作空间定义
2. 必须存在状态转移函数，不允许单步决策，也就是一个动作就gameover
3. 必须有过程奖励，且需要存在牺牲短期的过程奖励而获取最大累计回报的case案例

**应用方向**：游戏AI（份额不大）

&nbsp;

#### 3 阶段

RL 落地的真正难点在于**问题的真实构建**，而非近似构建或策略求解等等方面的问题。从原先任务只有求解策略的过程是强化学习，变成了 **构建问题+求解策略** 统称为强化学习。



典型如 offline model-based RL 和 RLHF，其中核心的模块变成了**通过神经网络模拟状态转移函数和奖励函数**，策略求解反而在方法论中被一句带过。

这个过程可以被解耦，变成跟强化学习毫无相关的名词概念，例如世界模型概念等。对于RL，没有有效的交互环境下的就没法达到目标，有这种有效交互环境的实际应用场景却非常少。<!--more-->导致把决策问题的过程步骤：问题建模、样本收集、策略训练、策略部署的周期拉得更长了，这几个步骤不是跟在线强化一样那么紧凑，是断开了链路的。**中间过程的任何一个步骤都变成了强化学习！**

于是神奇的事情发生了：**中间过程的任何一个步骤都变成了强化学习！**

&nbsp;

#### 4 阶段：猜测未来

监督学习优化的是 非参分布下的含参loss function
强化学习优化的是 含参分布下的非参loss (cost/reward) function

![1](/images/RLdev/1.png)



&nbsp;



