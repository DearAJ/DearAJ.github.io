---
date: 2025-04-04T04:00:59-07:00
description: ""
featured_image: "/images/RLchain/pia.jpg"
tags: ["RL"]
title: "强化学习-工具链"
---

## 1. gym

官方文档：https://www.gymlibrary.dev

+ 最小例子 `CartPole-v0`

  ```python3
  import gymenv = gym.make('CartPole-v0')
  env.reset()
  for _ in range(1000):
      env.render()
      env.step(env.action_space.sample()) # take a random action
  ```

&nbsp;

### 观测 (Observations)

在 Gym 仿真中，每一次回合开始，需要先执行 `reset()` 函数，返回**初始观测信息，然后根据标志位 `done` 的状态，来决定是否进行下一次回合**。代码表示：

&nbsp;

 `env.step()` 函数对每一步进行仿真，返回 4 个参数：

- **观测** Observation (Object)：当前 step 执行后，环境的观测(类型为对象)。例如，从相机获取的像素点，机器人各个关节的角度或棋盘游戏当前的状态等；

- **奖励** Reward (Float): 执行上一步动作(action)后，智体(agent)获得的奖励，不同的环境中奖励值变化范围也不相同，但是强化学习的目标就是使得总奖励值最大；

- **完成** Done (Boolen): 表示是否需要将环境重置 `env.reset`。

  <!--more-->

  大多数情况下，当 `Done` 为 `True`时，就表明当前回合(episode)或者试验(tial)结束。例如当机器人摔倒或者掉出台面，就应当终止当前回合进行重置(reset);

- **信息** Info (Dict): 针对调试过程的诊断信息。在标准的智体仿真评估当中不会使用到这个 info。

总结来说，这就是一个强化学习的基本流程：在每个时间点上，智体执行 action，环境返回上一次 action 的观测和奖励。

&nbsp;

### 空间（Spaces）

每次执行的动作(action)都是从环境动作空间中随机进行选取的.

在 Gym 的仿真环境中，有**运动空间 `action_space`** 和**观测空间 `observation_space`** 两个指标，程序中被定义为 `Space`类型，用于描述有效的运动和观测的格式和范围。

```python
import gymenv = gym.make('CartPole-v0')
print(env.action_space)#> Discrete(2)
print(env.observation_space)#> Box(4,)
[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.[0m
Discrete(2)
Box(4,)
```

&nbsp;

### 注册表

Gym 是一个包含各种各样强化学习仿真环境的大集合，并且封装成通用的接口暴露给用户，查看所有环境的代码如下：

```python
from gym import envsprint(envs.registry.all())
dict_values([EnvSpec(Copy-v0), EnvSpec(RepeatCopy-v0), EnvSpec(ReversedAddition-v0), EnvSpec(ReversedAddition3-v0), EnvSpec(DuplicatedInput-v0), EnvSpec(Reverse-v0), EnvSpec(CartPole-v0), EnvSpec(CartPole-v1), EnvSpec(MountainCar-v0), EnvSpec(MountainCarContinuous-v0), EnvSpec(Pendulum-v0), EnvSpec(Acrobot-v1), EnvSpec(LunarLander-v2), EnvSpec(LunarLanderContinuous-v2), EnvSpec(BipedalWalker-v2),...
```

Gym 支持将用户制作的环境写入到注册表中，需要执行 `gym.make()` 和在启动时注册 `register`，如:

```python3
register(
    id='CartPole-v0',
    entry_point='gym.envs.classic_control:CartPoleEnv',
    max_episode_steps=200,
    reward_threshold=195.0,
)
```

&nbsp;

###  gym 基本函数接口

1. make()：生成环境对象
2.  reset()：环境复位初始化函数。将环境的状态恢复到初始状态。
3.  env.state：查看环境当前状态。
4.  env.step()：单步执行/智能体与环境之间的一次交互，即智能体在当前状态s下执行一次动作a，环境相应地更新至状态s'，并向智能体反馈及时奖励r。
5.  env.render()：环境显示。以图形化的方式显示环境当前状态，在智能体与环境的持续交互过程中，该图形化显示也是相应地持续更新的。
6. env.close()：关闭环境。
7. env.sample_space.sample(): 对动作空间进行随机采样。
8. env.seed()：指定随机种子。

&nbsp;

&nbsp;

&nbsp;

## 2. stable-baselines3

强化学习资源：[https://stable-baselines3.readthedocs.io/en/master/guide/rl.html](https://stable-baselines3.readthedocs.io/en/master/guide/rl.html)

强化学习训练技巧：[https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html](https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html)

PPO 官方文档：[https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html)

&nbsp;

### 1. PPO 源码阅读

#### train()

**输入**：使用预先收集的rollout buffer数据

**输出**：更新策略网络参数

**核心操作**：`n_epochs` 次的批量梯度更新，包含策略损失、值函数损失和熵正则项

```python
def train(self) -> None:
        """
        Update policy using the currently gathered rollout buffer.
        """
        # 预处理

        # train for n_epochs epochs
        for epoch in range(self.n_epochs):
          			# 训练
            		# 策略评估
                # 优势归一化

                # PPO核心损失:
                # 1. Clipped Surrogate Loss
                # 2. 值函数损失
                # 3. 熵正则项
                
								# 多组件加权损失
                loss = policy_loss + self.ent_coef * entropy_loss + self.vf_coef * value_loss
								
                # Early Stopping
                # Optimization step
                self.policy.optimizer.zero_grad()
                loss.backward()
                # Clip grad norm
                
            if not continue_training:
                break

        self._n_updates += self.n_epochs
        explained_var = explained_variance(self.rollout_buffer.values.flatten(), self.rollout_buffer.returns.flatten())

        # Logs 监控指标
```

1. **预处理**

   - **动态学习率**：`_update_learning_rate` 根据训练进度调整优化器学习率
   - **Clip范围计算**：
     - 策略网络clip范围 `clip_range`（随时间衰减）
     - 值函数clip范围 `clip_range_vf`（可选）

   ```python
    # Update optimizer learning rate
           self._update_learning_rate(self.policy.optimizer)
           # Compute current clip range
           clip_range = self.clip_range(self._current_progress_remaining)
           # Optional: clip range for the value function
           if self.clip_range_vf is not None:
               clip_range_vf = self.clip_range_vf(self._current_progress_remaining)
   
           entropy_losses = []
           pg_losses, value_losses = [], []
           clip_fractions = []
   
           continue_training = True
   ```

2. **训练循环**

   策略评估、损失计算、参数更新

   ```python
   approx_kl_divs = []
               # Do a complete pass on the rollout buffer
               for rollout_data in self.rollout_buffer.get(self.batch_size):
                   actions = rollout_data.actions
                   if isinstance(self.action_space, spaces.Discrete):
                       # Convert discrete action from float to long
                       actions = rollout_data.actions.long().flatten()
   
                   # Re-sample the noise matrix because the log_std has changed
                   # TODO: investigate why there is no issue with the gradient
                   # if that line is commented (as in SAC)
                   if self.use_sde:
                       self.policy.reset_noise(self.batch_size)
   ```

3.  **策略评估**

   计算当前策略下动作的**价值估计**、**对数概率**和**熵**

   ```python
   values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)
   values = values.flatten()
   ```

4. **优势归一化**

   标准化优势值以稳定训练

   ```python
   advantages = rollout_data.advantages
   advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
   ```

5. **PPO核心损失**

   - **Clipped Surrogate Loss**：

     ```python
     # ratio between old and new policy, should be one at the first iteration
     	ratio = th.exp(log_prob - rollout_data.old_log_prob)
     
     	# clipped surrogate loss
     	policy_loss_1 = advantages * ratio
     	policy_loss_2 = advantages * th.clamp(ratio, 1 - clip_range, 1 + clip_range)
     	policy_loss = -th.min(policy_loss_1, policy_loss_2).mean()
     ```

   - **值函数损失**（支持clip）：

     ```python
     if self.clip_range_vf is None:
     	# No clipping
     	values_pred = values
     else:
     	# Clip the different between old and new value
     	# NOTE: this depends on the reward scaling
     	values_pred = rollout_data.old_values + th.clamp(
         values - rollout_data.old_values, -clip_range_vf, clip_range_vf)
       
     # Value loss using the TD(gae_lambda) target
     value_loss = F.mse_loss(rollout_data.returns, values_pred)
     value_losses.append(value_loss.item())
     ```

   - **熵正则项**：

     ```python
     # Entropy loss favor exploration
     if entropy is None:
     		# Approximate entropy when no analytical form
         entropy_loss = -th.mean(-log_prob)
     else:
     		entropy_loss = -th.mean(entropy)
     
     entropy_losses.append(entropy_loss.item())
     ```

6. **优化**

   + **Early Stopping**：当近似KL散度超过阈值时终止训练

     ```python
     # Calculate approximate form of reverse KL Divergence for early stopping
                     # see issue #417: https://github.com/DLR-RM/stable-baselines3/issues/417
                     # and discussion in PR #419: https://github.com/DLR-RM/stable-baselines3/pull/419
                     # and Schulman blog: http://joschu.net/blog/kl-approx.html
                     with th.no_grad():
                         log_ratio = log_prob - rollout_data.old_log_prob
                         approx_kl_div = th.mean((th.exp(log_ratio) - 1) - log_ratio).cpu().numpy()
                         approx_kl_divs.append(approx_kl_div)
     
                     if self.target_kl is not None and approx_kl_div > 1.5 * self.target_kl:
                         continue_training = False
                         if self.verbose >= 1:
                             print(f"Early stopping at step {epoch} due to reaching max kl: {approx_kl_div:.2f}")
                         break
     ```

   + **梯度裁剪**

     ```python
     # Clip grad norm
                     th.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)
                     self.policy.optimizer.step()
     ```

7. **监控指标**

   ```python
   self.logger.record("train/entropy_loss", np.mean(entropy_losses))
           self.logger.record("train/policy_gradient_loss", np.mean(pg_losses))
           self.logger.record("train/value_loss", np.mean(value_losses))
           self.logger.record("train/approx_kl", np.mean(approx_kl_divs))
           self.logger.record("train/clip_fraction", np.mean(clip_fractions))
           self.logger.record("train/loss", loss.item())
           self.logger.record("train/explained_variance", explained_var)
           if hasattr(self.policy, "log_std"):
               self.logger.record("train/std", th.exp(self.policy.log_std).mean().item())
   
           self.logger.record("train/n_updates", self._n_updates, exclude="tensorboard")
           self.logger.record("train/clip_range", clip_range)
           if self.clip_range_vf is not None:
               self.logger.record("train/clip_range_vf", clip_range_vf)
   ```

&nbsp;

&nbsp;

+ #### DummyVecEnv

  序列化的环境封装类，实现了环境的自动reset

  &nbsp;

+ #### BaseAlgorithm

  1. **构造函数**：初始化环境env
  2. **_setup_learn函数**：记录过程
  3. **_setup_lr_schedule函数**：用来适应learning rate可变的情况
  4. **_update_learning_rate**：记录 learning_rate
  5. **其他**：实现了 save 和 load 函数

  &nbsp;

+ #### OnPolicyAlgorithm

  继承自BaseAlgorithm，实现了环境中数据的获取collect_rollout，网络的初始化（_setup_model函数），以及训练的整体架构

  1. **构造函数**：初始化
  2. **collect_rollout()**：从网络中获得当前状态的 action、values 和 log_probs；进行一次 step；连续进行 n_rollout_steps 次，每次的结果存入 roll_buffer；最后使用 GAE 计算优势函数。
  3. **learn()**：调用 collect_rollouts 与环境交互后更新参数，调用 train() 更新网络。

  &nbsp;

+ #### BasePolicy

  Policy 网络通常是如下所示的结构，由一个特征提取器和全连接神经网络构成

  ![1](/images/RLchain/1.jpg)

  需要实现的虚函数： **_predict函数**（根据观测返回一个action）

  &nbsp;

+ #### ActorCriticPolicy

  定义在 stable_baselines3.common.policies 里，输入状态，输出 value（实数），action（与分布有关），log_prob（实数）

  实现具体网络的构造（在构造函数和 _build 函数中），forward 函数（一口气返回value, action, log_prob）和 evaluate_actions（不返回 action，但是会返回分布的熵）

  + **_build_mlp_extractor 函数**

    需要定义 forward 函数（share_features_extractor 情况下）， forward_actor 和 forward_critic 函数（不 share 的情况下）

    如果想要定制中间层形状的话，可以传入 net_arch 参数自定义

  &nbsp;

+ #### BaseCallback

  用来实时检测训练是否需要继续的类

&nbsp;

&nbsp;

#### common/policies.py - 实现特征提取

+ 在 Stable Baselines3 的 PPO 中实现**自定义特征提取网络**：
  1. 定义自定义特征提取器类：继承 `BaseFeaturesExtractor`，并实现 `forward` 方法
  2. 配置 PPO 的 policy_kwargs
  3. 创建 PPO 模型并训练

```text
观测输入 (obs)
    │
    ↓
preprocess_obs()          # 图像归一化/转置
    │
    ↓
features_extractor()      # 自定义特征提取 (如输出256维)
    │
    ↓
mlp_extractor()           # 分离 latent_pi (Actor) 和 latent_vf (Critic)
    │                      # 示例：latent_pi=[256], latent_vf=[128]
    ├─→ actor_net() → 动作分布
    └─→ value_net() → 值预测
```

&nbsp;

1. **BaseModel 基类**

   - **功能**：模型基类，定义特征提取和预处理流程。

     输入参数的 param features_extractor 即为 Network to extract features

   - **特征提取器初始化**：

     ```python
     def make_features_extractor(self) -> BaseFeaturesExtractor:
         return self.features_extractor_class(self.observation_space, **self.features_extractor_kwargs)
     ```

     *`->` 符号用于**类型注解（type annotations）**，表示函数或方法的**返回类型***

   - **特征提取**：

     ```python
     def extract_features(self, obs: th.Tensor) -> th.Tensor:
         # 预处理观测数据（如图像归一化）
         preprocessed_obs = preprocess_obs(obs, self.observation_space, normalize_images=self.normalize_images)
         # 调用特征提取器
         return self.features_extractor(preprocessed_obs)
     ```

   &nbsp;

2. **ActorCriticPolicy 类（核心策略类）**

   - **功能**：
     - 从观测数据中提取特征（通过自定义特征提取器）
     - 生成动作分布（Actor 网络）
     - 计算状态价值（Critic 网络）
     - 支持多种动作分布（高斯分布、分类分布等）
     - 支持状态依赖探索（gSDE）

   1.  **网络架构定义**

      ```python
      def __init__(self, ...):
          # 特征提取器（如自定义的CustomFeatureExtractor）
          self.features_extractor = features_extractor_class(...)
          self.features_dim = self.features_extractor.features_dim  # 特征维度（如256）
      ```

      

   2. **网络构建**

      ```python
      def _build(self, lr_schedule):
          # 创建MLP提取器（分离Actor/Critic网络）
          self.mlp_extractor = MlpExtractor(
              self.features_dim,
              net_arch=self.net_arch,       # 例如 pi=[256], vf=[128]
              activation_fn=self.activation_fn
          )
          
          # 创建动作分布网络
          if Gaussian分布:
              self.action_net, self.log_std = ...  # 均值网络和log标准差
          elif 分类分布:
              self.action_net = ...                # 动作logits
          
          # 价值网络
          self.value_net = nn.Linear(mlp_extractor.latent_dim_vf, 1)
          
          # 正交初始化权重
          if self.ortho_init:
              for module in [特征提取器, MLP提取器, action_net, value_net]:
                  module.apply(正交初始化)
      ```

      

   3. **特征提取与推理**

      ```python
      def _get_latent(self, obs):
          # 特征提取流程
          features = self.extract_features(obs)          # 自定义特征提取器
          latent_pi, latent_vf = self.mlp_extractor(features)  # Actor/Critic专用特征
          return latent_pi, latent_vf, latent_sde
      ```

      ```python
      def forward(self, obs):
          latent_pi, latent_vf, _ = self._get_latent(obs)
          values = self.value_net(latent_vf)            # 价值预测
          distribution = self._get_action_dist_from_latent(latent_pi)  # 动作分布
          return actions, values, log_prob
      ```

      

   4. **动作分布生成**

      ```python
      def _get_action_dist_from_latent(self, latent_pi):
          mean_actions = self.action_net(latent_pi)  # 通过Actor网络得到动作参数
          
          # 根据分布类型生成最终分布
          if 高斯分布:
              return 高斯分布(mean_actions, self.log_std)
          elif 分类分布:
              return 分类分布(logits=mean_actions)
          # ... 其他分布处理
      ```

   5. **动作评估**

      ```python
      def evaluate_actions(self, obs, actions):
          # 计算给定动作的对数概率和价值
          latent_pi, latent_vf, _ = self._get_latent(obs)
          distribution = self._get_action_dist_from_latent(latent_pi)
          log_prob = distribution.log_prob(actions)
          values = self.value_net(latent_vf)
          return values, log_prob, entropy
      ```



&nbsp;

&nbsp;

## 3. [Carla](https://github.com/carla-simulator/carla)

官方文档：[https://carla.readthedocs.io/en/0.9.9/#getting-started](https://carla.readthedocs.io/en/0.9.9/#getting-started)

Carla是一款开源的 *自动驾驶* 仿真器，它基本可以用来帮助训练自动驾驶的所有模块，包括感知系统，Localization，规划系统等等。许多自动驾驶公司在进行实际路跑前都要在这Carla上先进行训练。

### 1. 基本架构

#### Client-Server 的交互形式

Carla主要分为Server与Client两个模块

1. **Server 端**

   基于 UnrealEnigne3D 渲染建立仿真世界。

2. **Client 端**

   由用户控制，用来调整、变化仿真世界：不同时刻到底该如何运转（比如天气是什么样，有多少辆车在跑，速度是多少）。用户通过书写 Python/C++ 脚本来向 Server 端输送指令指导世界的变化，Server 根据用户的指令去执行。

   另外，Client 端也可以接受 Server 端的信息，譬如某个照相机拍到的路面图片。

&nbsp;

#### 核心模块

1. **Traffic Manager**: 

   模拟类似现实世界负责的交通环境。通过这个模块，用户可以定义N多不同车型、不同行为模式、不同速度的车辆在路上与你的自动驾驶汽车（Ego-Vehicle）一起玩耍。

2. **Sensors:** 

   Carla 里面有各种各样模拟真实世界的传感器模型，包括相机、激光雷达、声波雷达、IMU、GNSS等等。为了让仿真更接近真实世界，它里面的相机拍出的照片甚至有畸变和动态模糊效果。用户一般将这些Sensor attach到不同的车辆上来收集各种数据。

3. **Recorder：** 

   该模块用来记录仿真每一个时刻（Step)的状态，可以用来回顾、复现等等。

4. **ROS bridge：** 

   该模块可以让 Carla 与 ROS、Autoware 交互，使得在仿真里测试自动驾驶系统变得可能。

5. **Open Assest**：

   这个模块为仿真世界添加 customized 的物体库，比如可以在默认的汽车蓝图里再加一个真实世界不存在、外形酷炫的小飞汽车，用来给 Client 端调用。

&nbsp;

&nbsp;

### 2. [API 使用](https://carla.readthedocs.io/en/0.9.9/python_api/)

0. #### 启动

   ```python
    ./CarlaUE4.sh
   ```

1. #### Client and World

   + 创建 Client，并且设置一个 timeout 时间防止连接时间过久。

     ```python3
     # 其中2000是端口，2.0是秒数
     client = carla.Client('localhost', 2000)
     client.set_timeout(2.0)
     ```

   + 通过构建的 Client 来获取仿真世界（World)。如果想让仿真世界有任何变化，都要对这个获取的world进行操作。

     ```python3
     world = client.get_world()
     ```

   + 改变世界的天气

     ```python3
     weather = carla.WeatherParameters(cloudiness=10.0,
                                       precipitation=10.0,
                                       fog_density=10.0)
     world.set_weather(weather)
     ```

   &nbsp;

2. #### Actor 与 Blueprint

   Actor 是在仿真世界里则代表可以移动的物体，包括汽车，传感器（因为传感器要安在车身上）以及行人。

   1. **生成（spawn) Actor**

      如果想生成一个Actor, 必须要先定义它的蓝图（Blueprint）

      ```python3
      # 拿到这个世界所有物体的蓝图
      blueprint_library = world.get_blueprint_library()
      # 从浩瀚如海的蓝图中找到奔驰的蓝图
      ego_vehicle_bp = blueprint_library.find('vehicle.mercedes-benz.coupe')
      # 给我们的车加上特定的颜色
      ego_vehicle_bp.set_attribute('color', '0, 0, 0')
      ```

      构建好蓝图以后，下一步便是选定它的出生点。

      可以给固定的位子，也可以赋予随机的位置，不过这个位置必须是空的位置，比如你不能将奔驰扔在一棵树上。

      ```python3
      # 找到所有可以作为初始点的位置并随机选择一个
      transform = random.choice(world.get_map().get_spawn_points())
      # 在这个位置生成汽车
      ego_vehicle = world.spawn_actor(ego_vehicle_bp, transform)
      ```

   2. **操纵（Handling）Actor**

      汽车生成以后，便可以随意挪动它的初始位置，定义它的动态参数。

      ```python3
      # 给它挪挪窝
      location = ego_vehicle.get_location()
      location.x += 10.0
      ego_vehicle.set_location(location)
      # 把它设置成自动驾驶模式
      ego_vehicle.set_autopilot(True)
      # 我们可以甚至在中途将这辆车“冻住”，通过抹杀它的物理仿真
      # actor.set_simulate_physics(False)
      ```

   3. **注销（Destroy) Actor**

      当这个脚本运行完后要记得将这个汽车销毁掉，否则它会一直存在于仿真世界，可能影响其他脚本的运行。

      ```python3
      # 如果注销单个Actor
      ego_vehicle.destroy()
      # 如果你有多个Actor 存在list里，想一起销毁。
      client.apply_batch([carla.command.DestroyActor(x) for x in actor_list])
      ```

   &nbsp;

3. #### Sensor搭建

   ![2](/images/RLchain/2.png)

   - **Camera构建**

     与汽车类似，我们先创建蓝图，再定义位置，然后再选择我们想要的汽车安装上去。不过，这里的位置都是相对汽车中心点的位置（以米计量）。

     ```python3
     camera_bp = blueprint_library.find('sensor.camera.rgb')
     camera_transform = carla.Transform(carla.Location(x=1.5, z=2.4))
     camera = world.spawn_actor(camera_bp, camera_transform, attach_to=ego_vehicle)
     ```

     再对相机定义它的 callback function，定义每次仿真世界里传感器数据传回来后，我们要对它进行什么样的处理。如，只简单地将文件存在硬盘里。

     ```python3
     camera.listen(lambda image: image.save_to_disk(os.path.join(output_path, '%06d.png' % image.frame)))
     ```

     

   - **Lidar构建**

     Lidar 可以设置的参数比较多，现阶段设置一些常用参数即可。

     ```python3
     lidar_bp = blueprint_library.find('sensor.lidar.ray_cast')
     lidar_bp.set_attribute('channels', str(32))
     lidar_bp.set_attribute('points_per_second', str(90000))
     lidar_bp.set_attribute('rotation_frequency', str(40))
     lidar_bp.set_attribute('range', str(20))
     ```

     接着把 lidar 放置在奔驰上, 定义它的 callback function.

     ```python3
     lidar_location = carla.Location(0, 0, 2)
     lidar_rotation = carla.Rotation(0, 0, 0)
     lidar_transform = carla.Transform(lidar_location, lidar_rotation)
     lidar = world.spawn_actor(lidar_bp, lidar_transform, attach_to=ego_vehicle)
     lidar.listen(lambda point_cloud: \
                 point_cloud.save_to_disk(os.path.join(output_path, '%06d.ply' % point_cloud.frame)))
     ```

   

4. #### 观察者（spectator）放置

   观察仿真界面时，自己的视野并不会随我们造的小车子移动，所以经常会跟丢它。

   **解决办法**：把 spectator 对准汽车，这样小汽车就永远在我们的视野里了。

   ```python
   spectator = world.get_spectator()
   transform = ego_vehicle.get_transform()
   spectator.set_transform(carla.Transform(transform.location + carla.Location(z=20),
                                                       carla.Rotation(pitch=-90)))
   ```

   

5. #### 查看储存的照片与3D点云

   查看点云图需要另外安装 meshlab, 然后进入 meshlab 后选择 import mesh：

   ```text
   sudo apt-get update -y
   sudo apt-get install -y meshlab
   meshlab
   ```

   







