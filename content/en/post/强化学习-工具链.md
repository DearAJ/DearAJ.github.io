---
date: 2025-04-04T04:00:59-07:00
description: ""
featured_image: "/images/RLchain/pia.jpg"
tags: ["RL"]
title: "强化学习-工具链"
---

## 1. gym

官方文档：https://www.gymlibrary.dev

+ 最小例子 `CartPole-v0`

  ```python3
  import gymenv = gym.make('CartPole-v0')
  env.reset()
  for _ in range(1000):
      env.render()
      env.step(env.action_space.sample()) # take a random action
  ```

&nbsp;

### 观测 (Observations)

在 Gym 仿真中，每一次回合开始，需要先执行 `reset()` 函数，返回**初始观测信息，然后根据标志位 `done` 的状态，来决定是否进行下一次回合**。代码表示：

&nbsp;

 `env.step()` 函数对每一步进行仿真，返回 4 个参数：

- **观测** Observation (Object)：当前 step 执行后，环境的观测(类型为对象)。例如，从相机获取的像素点，机器人各个关节的角度或棋盘游戏当前的状态等；

- **奖励** Reward (Float): 执行上一步动作(action)后，智体(agent)获得的奖励，不同的环境中奖励值变化范围也不相同，但是强化学习的目标就是使得总奖励值最大；

- **完成** Done (Boolen): 表示是否需要将环境重置 `env.reset`。

  <!--more-->

  大多数情况下，当 `Done` 为 `True`时，就表明当前回合(episode)或者试验(tial)结束。例如当机器人摔倒或者掉出台面，就应当终止当前回合进行重置(reset);

- **信息** Info (Dict): 针对调试过程的诊断信息。在标准的智体仿真评估当中不会使用到这个 info。

总结来说，这就是一个强化学习的基本流程：在每个时间点上，智体执行 action，环境返回上一次 action 的观测和奖励。

&nbsp;

### 空间（Spaces）

每次执行的动作(action)都是从环境动作空间中随机进行选取的.

在 Gym 的仿真环境中，有**运动空间 `action_space`** 和**观测空间 `observation_space`** 两个指标，程序中被定义为 `Space`类型，用于描述有效的运动和观测的格式和范围。

```python
import gymenv = gym.make('CartPole-v0')
print(env.action_space)#> Discrete(2)
print(env.observation_space)#> Box(4,)
[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.[0m
Discrete(2)
Box(4,)
```

&nbsp;

### 注册表

Gym 是一个包含各种各样强化学习仿真环境的大集合，并且封装成通用的接口暴露给用户，查看所有环境的代码如下：

```python
from gym import envsprint(envs.registry.all())
dict_values([EnvSpec(Copy-v0), EnvSpec(RepeatCopy-v0), EnvSpec(ReversedAddition-v0), EnvSpec(ReversedAddition3-v0), EnvSpec(DuplicatedInput-v0), EnvSpec(Reverse-v0), EnvSpec(CartPole-v0), EnvSpec(CartPole-v1), EnvSpec(MountainCar-v0), EnvSpec(MountainCarContinuous-v0), EnvSpec(Pendulum-v0), EnvSpec(Acrobot-v1), EnvSpec(LunarLander-v2), EnvSpec(LunarLanderContinuous-v2), EnvSpec(BipedalWalker-v2),...
```

Gym 支持将用户制作的环境写入到注册表中，需要执行 `gym.make()` 和在启动时注册 `register`，如:

```python3
register(
    id='CartPole-v0',
    entry_point='gym.envs.classic_control:CartPoleEnv',
    max_episode_steps=200,
    reward_threshold=195.0,
)
```

&nbsp;

###  gym 基本函数接口

1. make()：生成环境对象
2.  reset()：环境复位初始化函数。将环境的状态恢复到初始状态。
3.  env.state：查看环境当前状态。
4.  env.step()：单步执行/智能体与环境之间的一次交互，即智能体在当前状态s下执行一次动作a，环境相应地更新至状态s'，并向智能体反馈及时奖励r。
5.  env.render()：环境显示。以图形化的方式显示环境当前状态，在智能体与环境的持续交互过程中，该图形化显示也是相应地持续更新的。
6. env.close()：关闭环境。
7. env.sample_space.sample(): 对动作空间进行随机采样。
8. env.seed()：指定随机种子。

&nbsp;

&nbsp;

&nbsp;

## 2. stable-baselines3

强化学习资源：https://stable-baselines3.readthedocs.io/en/master/guide/rl.html

强化学习训练技巧：https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html

PPO 官方文档：https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html

&nbsp;

### 1. PPO 源码阅读

#### train()

**输入**：使用预先收集的rollout buffer数据

**输出**：更新策略网络参数

**核心操作**：`n_epochs` 次的批量梯度更新，包含策略损失、值函数损失和熵正则项

```python
def train(self) -> None:
        """
        Update policy using the currently gathered rollout buffer.
        """
        # 预处理

        # train for n_epochs epochs
        for epoch in range(self.n_epochs):
          			# 训练
            		# 策略评估
                # 优势归一化

                # PPO核心损失:
                # 1. Clipped Surrogate Loss
                # 2. 值函数损失
                # 3. 熵正则项
                
								# 多组件加权损失
                loss = policy_loss + self.ent_coef * entropy_loss + self.vf_coef * value_loss
								
                # Early Stopping
                # Optimization step
                self.policy.optimizer.zero_grad()
                loss.backward()
                # Clip grad norm
                
            if not continue_training:
                break

        self._n_updates += self.n_epochs
        explained_var = explained_variance(self.rollout_buffer.values.flatten(), self.rollout_buffer.returns.flatten())

        # Logs 监控指标
```

1. **预处理**

   - **动态学习率**：`_update_learning_rate` 根据训练进度调整优化器学习率
   - **Clip范围计算**：
     - 策略网络clip范围 `clip_range`（随时间衰减）
     - 值函数clip范围 `clip_range_vf`（可选）

   ```python
    # Update optimizer learning rate
           self._update_learning_rate(self.policy.optimizer)
           # Compute current clip range
           clip_range = self.clip_range(self._current_progress_remaining)
           # Optional: clip range for the value function
           if self.clip_range_vf is not None:
               clip_range_vf = self.clip_range_vf(self._current_progress_remaining)
   
           entropy_losses = []
           pg_losses, value_losses = [], []
           clip_fractions = []
   
           continue_training = True
   ```

2. **训练循环**

   策略评估、损失计算、参数更新

   ```python
   approx_kl_divs = []
               # Do a complete pass on the rollout buffer
               for rollout_data in self.rollout_buffer.get(self.batch_size):
                   actions = rollout_data.actions
                   if isinstance(self.action_space, spaces.Discrete):
                       # Convert discrete action from float to long
                       actions = rollout_data.actions.long().flatten()
   
                   # Re-sample the noise matrix because the log_std has changed
                   # TODO: investigate why there is no issue with the gradient
                   # if that line is commented (as in SAC)
                   if self.use_sde:
                       self.policy.reset_noise(self.batch_size)
   ```

3.  **策略评估**

   计算当前策略下动作的**价值估计**、**对数概率**和**熵**

   ```python
   values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)
   values = values.flatten()
   ```

4. **优势归一化**

   标准化优势值以稳定训练

   ```python
   advantages = rollout_data.advantages
   advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
   ```

5. **PPO核心损失**

   - **Clipped Surrogate Loss**：

     ```python
     # ratio between old and new policy, should be one at the first iteration
     	ratio = th.exp(log_prob - rollout_data.old_log_prob)
     
     	# clipped surrogate loss
     	policy_loss_1 = advantages * ratio
     	policy_loss_2 = advantages * th.clamp(ratio, 1 - clip_range, 1 + clip_range)
     	policy_loss = -th.min(policy_loss_1, policy_loss_2).mean()
     ```

   - **值函数损失**（支持clip）：

     ```python
     if self.clip_range_vf is None:
     	# No clipping
     	values_pred = values
     else:
     	# Clip the different between old and new value
     	# NOTE: this depends on the reward scaling
     	values_pred = rollout_data.old_values + th.clamp(
         values - rollout_data.old_values, -clip_range_vf, clip_range_vf)
       
     # Value loss using the TD(gae_lambda) target
     value_loss = F.mse_loss(rollout_data.returns, values_pred)
     value_losses.append(value_loss.item())
     ```

   - **熵正则项**：

     ```python
     # Entropy loss favor exploration
     if entropy is None:
     		# Approximate entropy when no analytical form
         entropy_loss = -th.mean(-log_prob)
     else:
     		entropy_loss = -th.mean(entropy)
     
     entropy_losses.append(entropy_loss.item())
     ```

6. **优化**

   + **Early Stopping**：当近似KL散度超过阈值时终止训练

     ```python
     # Calculate approximate form of reverse KL Divergence for early stopping
                     # see issue #417: https://github.com/DLR-RM/stable-baselines3/issues/417
                     # and discussion in PR #419: https://github.com/DLR-RM/stable-baselines3/pull/419
                     # and Schulman blog: http://joschu.net/blog/kl-approx.html
                     with th.no_grad():
                         log_ratio = log_prob - rollout_data.old_log_prob
                         approx_kl_div = th.mean((th.exp(log_ratio) - 1) - log_ratio).cpu().numpy()
                         approx_kl_divs.append(approx_kl_div)
     
                     if self.target_kl is not None and approx_kl_div > 1.5 * self.target_kl:
                         continue_training = False
                         if self.verbose >= 1:
                             print(f"Early stopping at step {epoch} due to reaching max kl: {approx_kl_div:.2f}")
                         break
     ```

   + **梯度裁剪**

     ```python
     # Clip grad norm
                     th.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)
                     self.policy.optimizer.step()
     ```

7. **监控指标**

   ```python
   self.logger.record("train/entropy_loss", np.mean(entropy_losses))
           self.logger.record("train/policy_gradient_loss", np.mean(pg_losses))
           self.logger.record("train/value_loss", np.mean(value_losses))
           self.logger.record("train/approx_kl", np.mean(approx_kl_divs))
           self.logger.record("train/clip_fraction", np.mean(clip_fractions))
           self.logger.record("train/loss", loss.item())
           self.logger.record("train/explained_variance", explained_var)
           if hasattr(self.policy, "log_std"):
               self.logger.record("train/std", th.exp(self.policy.log_std).mean().item())
   
           self.logger.record("train/n_updates", self._n_updates, exclude="tensorboard")
           self.logger.record("train/clip_range", clip_range)
           if self.clip_range_vf is not None:
               self.logger.record("train/clip_range_vf", clip_range_vf)
   ```

&nbsp;

&nbsp;

+ #### DummyVecEnv

  序列化的环境封装类，实现了环境的自动reset

  &nbsp;

+ #### BaseAlgorithm

  1. **构造函数**：初始化环境env
  2. **_setup_learn函数**：记录过程
  3. **_setup_lr_schedule函数**：用来适应learning rate可变的情况
  4. **_update_learning_rate**：记录 learning_rate
  5. **其他**：实现了 save 和 load 函数

  &nbsp;

+ #### OnPolicyAlgorithm

  继承自BaseAlgorithm，实现了环境中数据的获取collect_rollout，网络的初始化（_setup_model函数），以及训练的整体架构

  1. **构造函数**：初始化
  2. **collect_rollout()**：从网络中获得当前状态的 action、values 和 log_probs；进行一次 step；连续进行 n_rollout_steps 次，每次的结果存入 roll_buffer；最后使用 GAE 计算优势函数。
  3. **learn()**：调用 collect_rollouts 与环境交互后更新参数，调用 train() 更新网络。

  &nbsp;

+ #### BasePolicy

  Policy 网络通常是如下所示的结构，由一个特征提取器和全连接神经网络构成

  ![1](/Users/aijunyang/DearAJ.github.io/static/images/RLchain/1.jpg)

  需要实现的虚函数： **_predict函数**（根据观测返回一个action）

  &nbsp;

+ #### ActorCriticPolicy

  定义在 stable_baselines3.common.policies 里，输入状态，输出 value（实数），action（与分布有关），log_prob（实数）

  实现具体网络的构造（在构造函数和 _build 函数中），forward 函数（一口气返回value, action, log_prob）和 evaluate_actions（不返回 action，但是会返回分布的熵）

  + **_build_mlp_extractor 函数**

    需要定义 forward 函数（share_features_extractor 情况下）， forward_actor 和 forward_critic 函数（不 share 的情况下）

    如果想要定制中间层形状的话，可以传入 net_arch 参数自定义

  &nbsp;

+ #### BaseCallback

  用来实时检测训练是否需要继续的类

&nbsp;

&nbsp;

&nbsp;

## 3. [Carla](https://github.com/carla-simulator/carla)

Carla是一款开源的 *自动驾驶* 仿真器，它基本可以用来帮助训练自动驾驶的所有模块，包括感知系统，Localization, 规划系统等等。许多自动驾驶公司在进行实际路跑前都要在这Carla上先进行训练。

### 1. 基本架构

#### Client-Server 的交互形å式

Carla主要分为Server与Client两个模块

1. **Server 端**

   基于 UnrealEnigne3D 渲染建立仿真世界。

2. **Client 端**

   由用户控制，用来调整、变化仿真世界：不同时刻到底该如何运转（比如天气是什么样，有多少辆车在跑，速度是多少）。用户通过书写 Python/C++ 脚本来向 Server 端输送指令指导世界的变化，Server 根据用户的指令去执行。

   另外，Client 端也可以接受 Server 端的信息，譬如某个照相机拍到的路面图片。

&nbsp;

#### 核心模块

1. **Traffic Manager**: 

   模拟类似现实世界负责的交通环境。通过这个模块，用户可以定义N多不同车型、不同行为模式、不同速度的车辆在路上与你的自动驾驶汽车（Ego-Vehicle）一起玩耍。

2. **Sensors:** 

   Carla 里面有各种各样模拟真实世界的传感器模型，包括相机、激光雷达、声波雷达、IMU、GNSS等等。为了让仿真更接近真实世界，它里面的相机拍出的照片甚至有畸变和动态模糊效果。用户一般将这些Sensor attach到不同的车辆上来收集各种数据。

3. **Recorder：** 

   该模块用来记录仿真每一个时刻（Step)的状态，可以用来回顾、复现等等。

4. **ROS bridge：** 

   该模块可以让 Carla 与 ROS、Autoware 交互，使得在仿真里测试自动驾驶系统变得可能。

5. **Open Assest**：

   这个模块为仿真世界添加 customized 的物体库，比如可以在默认的汽车蓝图里再加一个真实世界不存在、外形酷炫的小飞汽车，用来给 Client 端调用。
