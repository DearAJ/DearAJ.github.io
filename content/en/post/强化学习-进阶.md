---
date: 2025-02-14T11:00:59-04:00
description: "强化学习专注于如何让智能体（Agent）通过与环境的交互来学习最优策略，以最大化累积奖励。它的核心思想是试错学习，智能体通过尝试不同的行动，观察结果并获得奖励或惩罚，从而逐步改进自己的行为策略。"
featured_image: "/images/RL2/lucky.jpg"
tags: ["RL"]
title: "强化学习-进阶"
---

## 总述

![1](/images/RL2/1.png)

+ #### **基础工具**

  1. **基本概念**：state, action, reward, return, episode, policy, mdp...
  2. **贝尔曼公式**：用于评价策略
  3. **贝尔曼最优公式**：强化学习的最终目标是求解最优策略

+ #### 算法/方法

  4. **值迭代、策略迭代—— truncated policy iteration**：值和策略update不断迭代
  5. **Monte Carlo Learning**：无模型学习
  6. **随即近似理论**：from non-incremental to incremental
  7. **时序差分方法(TD)**
  8. **值函数估计**：tabular representation to function representation，引入神经网络
  9. **Policy Gradient Methods**：from value-based to policy-based
  10. **Actor-Critic Methods**：policy-based + value-based



&nbsp;

## 1 基本概念

#### 1. 专有名词

1. **grid-world**：小机器人在网格里走路

2. **state**：agent在环境中的状态，用s1、s2...表示；s是列向量，可表示速度、加速度等

3. **state space**：即把所有的state放在一起的集合

4. **action**：可采取的行动，如往上走、往右走...

5. **action-space**：所有的action放在一起的集合，用A表示

6. **state transition**：采取一个action后，从一个state转到另一个state的过程；定义了agent与环境的一种交互行为

7. **forbidden area**：进去后受到惩罚/不可进入

   <!--more-->

8. **tabular representation**：使用表格描述state transition

9. **state transition probability**：使用条件概率来表述tate transition，用于描述随机性

10. **policy**：使用箭头表示，告诉agent在某state时应该采取哪个action。基于policy，可以得到path

11. **mathematical representation**：条件概率，用π表示某状态对应的策略

12. **stochastic policies**：某状态对应多个不同概率的action

13. **reward**：一个数(标量)；正奖励负惩罚

    ![2](/images/RL2/2.png)

    可视为 *human-machine interface*，即人类与机器交互的一种手段，引导机器该怎么做。

    同时也可以用 tabular representation 来表示reward，但只能表示唯一的reward；

    还可以使用 mathematical representation，用条件概率来表示。

14. **trajectory**：一个state-action-reward链

15. **return**：沿着trajectory所得到的reward总和，用于评估策略

16. **discounted rate**、**discounted return**：

    ![3](/images/RL2/3.png)

    gamma较小，比较近视，更加注重最近的一些reward；反之gamma较大，比较远视。

17. **terminal states**：终止状态

18. **episode/trail**：通常被定义为一个会终止的 trajectory，这些任务被称为episodic tasks

19. **continuing task**：有些任务是不会结束，永远持续的/或时间比较长

    + 统一方法：把 episodic tasks 转为continuing task
      1. 把 target state 视为 absorbing state，不论采取什么 action 都会再回到这个状态，并且 reward 为0；
      2. 将 target state 视为一个普通的状态，可离开可留下。

&nbsp;

#### 2. 马尔可夫决策过程 (MDP markov decision process)

+ MDP 的关键组成：

  ![4](/images/RL2/4.png)

+ 用圆和边来表示 markov process：

  ![5](/images/RL2/5.png)

当 policy 确定后，markov process 就成了 markov decision process



&nbsp;

&nbsp;

## 2 贝尔曼公式

核心概念 state value、基础工具 bellman equation

#### 1. 计算return

1. **方法一：用定义**

   ![6](/images/RL2/6.png)

2. **方法二**：Bootstrapping!

   ![7](/images/RL2/7.png)

   从某状态出发的return，依赖于从其他状态出发的return

   简易贝尔曼公式：![8](/images/RL2/8.png)

&nbsp;

#### 2. state value

+ 对于单步过程：

  ![9](/images/RL2/9.png)

state value 就是 Gt(discounted return) 的**期望值**

&nbsp;

#### 3. 贝尔曼公式

描述了不同状态的 state value 之间的关系，可用于计算 state value

1. **推导**

   ![10](/images/RL2/10.png)

   > [!IMPORTANT]
   >
   > 1. 如上所示，即为 Bellman equation，表示了不同状态的 state-value functions（左边是s的，右边是s'的）
   > 2. 有两项组成：当前 reward 和未来的 reward
   > 3. 该式子对所有的状态都成立！！！（若有 n 个状态，则会有 n 个这样的式子）
   > 4. 通过这 n 个式子，联立可求出 state value —— Bootstrapping!
   > 5. π 是给定的 policy，解决这个问题的过程就是 policy evaluation
   > 6. p 代表 dynamic model，可能已知或未知

   + 以下图 s1 为例：

     ![11](/images/RL2/11.png)

     联立所有的式子，并代入具体的gamma，即可求出所有的 state value；

     state value 越高，代表策略越好

2. **Matrix-vector form**

   将所有状态的贝尔曼公式放在一起，并写成矩阵形式：

   ![12](/images/RL2/12.png)

   + 例如：

     ![13](/images/RL2/13.png)

3. **求解 state value**

   1. 为什么要求解 state value？

      给定 policy，求解 state value 的过程叫 **policy evaluation**，即评估策略好不好，它是 RL 的基础问题。

   2. 解析方法

      1. **closed-form solution**

         ![14](/images/RL2/14.png)

         逆难求，实际很少用

      2. **iterative solution**

         ![15](/images/RL2/15.png)

         ![16](/images/RL2/16.png)

&nbsp;

#### 4. action value

 ![17](/images/RL2/17.png)

+ 区别：

  + **state value**：从一个状态出发，可以得到的平均 return
  + **action value**：从一个状态出发，并且选择了一个 action 后得到的平均 return

  ![18](/images/RL2/18.png)

  求 action value 的平均，可得 state value ；同理已知 state value 可求 action value

+ 例子：

  ![19](/images/RL2/19.png)

  虽然往右走，但所有的 action 都是可以计算的，**非零**！未来可变，详见后面的课程。

最后选择 action value 最大的那个 policy。

&nbsp;

#### 5. 小结

![20](/images/RL2/20.png)









