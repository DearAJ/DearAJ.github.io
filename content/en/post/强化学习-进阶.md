---
date: 2025-02-14T11:00:59-04:00
description: "强化学习专注于如何让智能体（Agent）通过与环境的交互来学习最优策略，以最大化累积奖励。它的核心思想是试错学习，智能体通过尝试不同的行动，观察结果并获得奖励或惩罚，从而逐步改进自己的行为策略。"
featured_image: "/images/RL2/lucky.jpg"
tags: ["RL"]
title: "强化学习-进阶"
---

## 总述

![1](/images/RL2/1.png)

+ #### **基础工具**

  1. **基本概念**：state, action, reward, return, episode, policy, mdp...
  2. **贝尔曼公式**：用于评价策略
  3. **贝尔曼最优公式**：强化学习的最终目标是求解最优策略

+ #### 算法/方法

  4. **值迭代、策略迭代—— truncated policy iteration**：值和策略update不断迭代
  5. **Monte Carlo Learning**：无模型学习
  6. **随即近似理论**：from non-incremental to incremental
  7. **时序差分方法(TD)**
  8. **值函数估计**：tabular representation to function representation，引入神经网络
  9. **Policy Gradient Methods**：from value-based to policy-based
  10. **Actor-Critic Methods**：policy-based + value-based



&nbsp;

## 1 基本概念

#### 1. 专有名词

1. **grid-world**：小机器人在网格里走路

2. **state**：agent在环境中的状态，用s1、s2...表示；s是列向量，可表示速度、加速度等

3. **state space**：即把所有的state放在一起的集合

4. **action**：可采取的行动，如往上走、往右走...

5. **action-space**：所有的action放在一起的集合，用A表示

6. **state transition**：采取一个action后，从一个state转到另一个state的过程；定义了agent与环境的一种交互行为

7. **forbidden area**：进去后受到惩罚/不可进入

   <!--more-->

8. **tabular representation**：使用表格描述state transition

9. **state transition probability**：使用条件概率来表述tate transition，用于描述随机性

10. **policy**：使用箭头表示，告诉agent在某state时应该采取哪个action。基于policy，可以得到path

11. **mathematical representation**：条件概率，用π表示某状态对应的策略

12. **stochastic policies**：某状态对应多个不同概率的action

13. **reward**：一个数(标量)；正奖励负惩罚

    ![2](/images/RL2/2.png)

    可视为 *human-machine interface*，即人类与机器交互的一种手段，引导机器该怎么做。

    同时也可以用 tabular representation 来表示reward，但只能表示唯一的reward；

    还可以使用 mathematical representation，用条件概率来表示。

14. **trajectory**：一个state-action-reward链

15. **return**：沿着trajectory所得到的reward总和

16. **discounted rate**、**discounted return**：

    ![3](/images/RL2/3.png)

    gamma较小，比较近视，更加注重最近的一些reward；反之gammma较大，比较远视。

17. **terminal states**：终止状态

18. **episode/trail**：通常被定义为一个会终止的 trajectory，这些任务被称为episodic tasks

19. **continuing task**：有些任务是不会结束，永远持续的/或时间比较长

    + 统一方法：把 episodic tasks 转为continuing task
      1. 把 target state 视为 absorbing state，不论采取什么 action 都会再回到这个状态，并且 reward 为0；
      2. 将 target state 视为一个普通的状态，可离开可留下。



#### 2. MDP (markov decision process)

+ MDP 的关键组成：

  ![4](/images/RL2/4.png)

+ 用圆和边来表示 markov process：

  ![5](/images/RL2/5.png)

当 policy 确定后，markov process 就成了 markov decision process













