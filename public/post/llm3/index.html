<!DOCTYPE html>
<html lang="en-US">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>LLM - 3.指令理解阶段(核心) - 指令微调 | HomePage</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="LLM 是 Large Language Model（大型语言模型）的缩写，是一种基于人工智能技术的自然语言处理模型。它通过大量的文本数据进行训练，能够理解和生成人类语言，广泛应用于文本生成、翻译、问答、摘要等任务。">
    <meta name="generator" content="Hugo 0.140.2">
    
    
    
      <meta name="robots" content="noindex, nofollow">
    
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



    

    
      

    

    

    
      <link rel="canonical" href="http://localhost:1313/post/llm3/">
    

    <meta property="og:url" content="http://localhost:1313/post/llm3/">
  <meta property="og:site_name" content="HomePage">
  <meta property="og:title" content="LLM - 3.指令理解阶段(核心) - 指令微调">
  <meta property="og:description" content="LLM 是 Large Language Model（大型语言模型）的缩写，是一种基于人工智能技术的自然语言处理模型。它通过大量的文本数据进行训练，能够理解和生成人类语言，广泛应用于文本生成、翻译、问答、摘要等任务。">
  <meta property="og:locale" content="en_US">
  <meta property="og:type" content="article">
    <meta property="article:section" content="post">
    <meta property="article:published_time" content="2025-05-01T11:00:59-04:00">
    <meta property="article:modified_time" content="2025-05-01T11:00:59-04:00">
    <meta property="article:tag" content="LLM">

  <meta itemprop="name" content="LLM - 3.指令理解阶段(核心) - 指令微调">
  <meta itemprop="description" content="LLM 是 Large Language Model（大型语言模型）的缩写，是一种基于人工智能技术的自然语言处理模型。它通过大量的文本数据进行训练，能够理解和生成人类语言，广泛应用于文本生成、翻译、问答、摘要等任务。">
  <meta itemprop="datePublished" content="2025-05-01T11:00:59-04:00">
  <meta itemprop="dateModified" content="2025-05-01T11:00:59-04:00">
  <meta itemprop="wordCount" content="155">
  <meta itemprop="keywords" content="LLM">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="LLM - 3.指令理解阶段(核心) - 指令微调">
  <meta name="twitter:description" content="LLM 是 Large Language Model（大型语言模型）的缩写，是一种基于人工智能技术的自然语言处理模型。它通过大量的文本数据进行训练，能够理解和生成人类语言，广泛应用于文本生成、翻译、问答、摘要等任务。">

	
  </head><body class="ma0 avenir bg-near-white development">

    
   
  

  
  
  
  <header class="cover bg-center" style="background-image: url('http://localhost:1313/images/LLM3/pia.jpg');">
    <div class="bg-black-60">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        HomePage
      
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/about/" title="About ME page">
              About ME
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/post/" title="Articles page">
              Articles
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/contact/" title="Contact page">
              Contact
            </a>
          </li>
          
        </ul>
      
      <div class="ananke-socials"><a href="https://www.facebook.com/patrick.kollitsch" target="_blank" rel="noopener"
        class="ananke-social-link link-transition facebook link dib z-999 pt3 pt0-l mr1"
        title="follow on Facebook - Opens in a new window"
        aria-label="follow on Facebook - Opens in a new window">
      <span class="icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M512 256C512 114.6 397.4 0 256 0S0 114.6 0 256C0 376 82.7 476.8 194.2 504.5V334.2H141.4V256h52.8V222.3c0-87.1 39.4-127.5 125-127.5c16.2 0 44.2 3.2 55.7 6.4V172c-6-.6-16.5-1-29.6-1c-42 0-58.2 15.9-58.2 57.2V256h83.6l-14.4 78.2H287V510.1C413.8 494.8 512 386.9 512 256h0z"/></svg>
            
          </span></a><a href="https://bsky.app/profile/kollitsch.dev" target="_blank" rel="noopener"
        class="ananke-social-link link-transition bluesky link dib z-999 pt3 pt0-l mr1"
        title="follow on Bluesky - Opens in a new window"
        aria-label="follow on Bluesky - Opens in a new window">
      <span class="icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M407.8 294.7c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3zM288 227.1C261.9 176.4 190.9 81.9 124.9 35.3C61.6-9.4 37.5-1.7 21.6 5.5C3.3 13.8 0 41.9 0 58.4S9.1 194 15 213.9c19.5 65.7 89.1 87.9 153.2 80.7c3.3-.5 6.6-.9 10-1.4c-3.3 .5-6.6 1-10 1.4C74.3 308.6-9.1 342.8 100.3 464.5C220.6 589.1 265.1 437.8 288 361.1c22.9 76.7 49.2 222.5 185.6 103.4c102.4-103.4 28.1-156-65.8-169.9c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3c64.1 7.1 133.6-15.1 153.2-80.7C566.9 194 576 75 576 58.4s-3.3-44.7-21.6-52.9c-15.8-7.1-40-14.9-103.2 29.8C385.1 81.9 314.1 176.4 288 227.1z"/></svg>
            
          </span></a><a href="http://linkedin.com/in/patrickkollitsch" target="_blank" rel="noopener"
        class="ananke-social-link link-transition linkedin link dib z-999 pt3 pt0-l mr1"
        title="follow on LinkedIn - Opens in a new window"
        aria-label="follow on LinkedIn - Opens in a new window">
      <span class="icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
            
          </span></a></div>

    </div>
  </div>
</nav>

      <div class="tc-l pv6 ph3 ph4-ns">
        
          <div class="f2 f1-l fw2 white-90 mb0 lh-title">LLM - 3.指令理解阶段(核心) - 指令微调</div>
          
            <div class="fw1 f5 f3-l white-80 measure-wide-l center lh-copy mt3 mb4">
              LLM 是 Large Language Model（大型语言模型）的缩写，是一种基于人工智能技术的自然语言处理模型。它通过大量的文本数据进行训练，能够理解和生成人类语言，广泛应用于文本生成、翻译、问答、摘要等任务。
            </div>
          
        
      </div>
    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked ttu">
          
        Articles
      </aside><div id="sharing" class="mt3 ananke-socials"><a href="mailto:?&amp;body=http%3A%2F%2Flocalhost%3A1313%2Fpost%2Fllm3%2F&amp;subject=LLM&#43;-&#43;3.%E6%8C%87%E4%BB%A4%E7%90%86%E8%A7%A3%E9%98%B6%E6%AE%B5%28%E6%A0%B8%E5%BF%83%29&#43;-&#43;%E6%8C%87%E4%BB%A4%E5%BE%AE%E8%B0%83"
        class="ananke-social-link email no-underline"
        title="Share on Email" aria-label="Share on Email"
        target="_blank" rel="nofollow noopener noreferrer">
        <span class="icon">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M64 112c-8.8 0-16 7.2-16 16l0 22.1L220.5 291.7c20.7 17 50.4 17 71.1 0L464 150.1l0-22.1c0-8.8-7.2-16-16-16L64 112zM48 212.2L48 384c0 8.8 7.2 16 16 16l384 0c8.8 0 16-7.2 16-16l0-171.8L322 328.8c-38.4 31.5-93.7 31.5-132 0L48 212.2zM0 128C0 92.7 28.7 64 64 64l384 0c35.3 0 64 28.7 64 64l0 256c0 35.3-28.7 64-64 64L64 448c-35.3 0-64-28.7-64-64L0 128z"/></svg>
                
              </span></a><a href="https://facebook.com/sharer/sharer.php?&amp;u=http%3A%2F%2Flocalhost%3A1313%2Fpost%2Fllm3%2F"
        class="ananke-social-link facebook no-underline"
        title="Share on Facebook" aria-label="Share on Facebook"
        target="_blank" rel="nofollow noopener noreferrer">
        <span class="icon">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M512 256C512 114.6 397.4 0 256 0S0 114.6 0 256C0 376 82.7 476.8 194.2 504.5V334.2H141.4V256h52.8V222.3c0-87.1 39.4-127.5 125-127.5c16.2 0 44.2 3.2 55.7 6.4V172c-6-.6-16.5-1-29.6-1c-42 0-58.2 15.9-58.2 57.2V256h83.6l-14.4 78.2H287V510.1C413.8 494.8 512 386.9 512 256h0z"/></svg>
                
              </span></a><a href="https://bsky.app/intent/compose?&amp;text=http%3A%2F%2Flocalhost%3A1313%2Fpost%2Fllm3%2F"
        class="ananke-social-link bluesky no-underline"
        title="Share on Bluesky" aria-label="Share on Bluesky"
        target="_blank" rel="nofollow noopener noreferrer">
        <span class="icon">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M407.8 294.7c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3zM288 227.1C261.9 176.4 190.9 81.9 124.9 35.3C61.6-9.4 37.5-1.7 21.6 5.5C3.3 13.8 0 41.9 0 58.4S9.1 194 15 213.9c19.5 65.7 89.1 87.9 153.2 80.7c3.3-.5 6.6-.9 10-1.4c-3.3 .5-6.6 1-10 1.4C74.3 308.6-9.1 342.8 100.3 464.5C220.6 589.1 265.1 437.8 288 361.1c22.9 76.7 49.2 222.5 185.6 103.4c102.4-103.4 28.1-156-65.8-169.9c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3c64.1 7.1 133.6-15.1 153.2-80.7C566.9 194 576 75 576 58.4s-3.3-44.7-21.6-52.9c-15.8-7.1-40-14.9-103.2 29.8C385.1 81.9 314.1 176.4 288 227.1z"/></svg>
                
              </span></a><a href="https://www.linkedin.com/shareArticle?&amp;mini=true&amp;source=http%3A%2F%2Flocalhost%3A1313%2Fpost%2Fllm3%2F&amp;summary=%0A&amp;title=LLM&#43;-&#43;3.%E6%8C%87%E4%BB%A4%E7%90%86%E8%A7%A3%E9%98%B6%E6%AE%B5%28%E6%A0%B8%E5%BF%83%29&#43;-&#43;%E6%8C%87%E4%BB%A4%E5%BE%AE%E8%B0%83&amp;url=http%3A%2F%2Flocalhost%3A1313%2Fpost%2Fllm3%2F"
        class="ananke-social-link linkedin no-underline"
        title="Share on LinkedIn" aria-label="Share on LinkedIn"
        target="_blank" rel="nofollow noopener noreferrer">
        <span class="icon">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
                
              </span></a></div>
<h1 class="f1 athelas mt3 mb1">LLM - 3.指令理解阶段(核心) - 指令微调</h1>
      
      
      
      <time class="f6 mv4 dib tracked" datetime="2025-05-01T11:00:59-04:00">May 1, 2025</time>
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p><img src="/images/LLM3/0.png" alt="0"></p>
<p>指令微调又称有监督微调，旨在使模型具备指令遵循 （Instruction Following）能力。</p>
<p>核心问题：<em>如何构造指令数据？如何高效低成本地进行指令微调训练？如何在语言模型基础上进一步扩大上下文？</em></p>
<h2 id="1-指令微调训练">1. 指令微调训练</h2>
<ul>
<li>
<p>步骤</p>
<ol>
<li>明确地<strong>定义</strong>相应的自然语言形式的指令或者提示</li>
<li>把训练数据<strong>调整</strong>成包含指令以及与之对应的响应的<strong>形式</strong></li>
<li>使用包含<strong>指令</strong>和<strong>响应</strong>的训练数据对预训练模型进行微调操作</li>
</ol>
<p>目标函数往往只是针对输出部分来计算损失</p>
</li>
</ul>
<h3 id="11-指令微调数据">1.1 指令微调数据</h3>
<p>组成：“指令输入”与“答案输出”</p>
<p>若需要模型理解多轮对话的能力，可以将<strong>对话历史</strong>都做为指令，让模型学习最后一轮的输出结果。（即，把最后一轮“Assistant”回答前的所有数据当做“输入”，最后一轮“Assistant”回答做为“输出）</p>
<p> </p>
<h3 id="12-数据构建方法">1.2 数据构建方法</h3>
<ol>
<li>
<h4 id="手动构建">手动构建</h4>
<ul>
<li><strong>优点</strong>：高质量、可解释性、灵活可控</li>
</ul>
</li>
<li>
<h4 id="现有数据集转换">现有数据集转换</h4>
<p>整合和修改多个开源数据集，最终将它们合并成一个新数据集用于大模型指令微调。</p>
<ul>
<li>
<p><strong>优点</strong>：多样性和全面性、规模大、节省时间</p>
</li>
<li>
<p>解决数据冗余的问题：多数据集合并</p>
<p><img src="/images/LLM3/1.png" alt="1"></p>
</li>
</ul>
</li>
<li>
<h4 id="自动构建指令">自动构建指令</h4>
<p>用大语言模型的生成能力自动构建指令。</p>
<p><img src="/images/LLM3/2.png" alt="2"></p>
<ol>
<li>
<p>生成任务指令</p>
</li>
<li>
<p>确定指令是否代表分类任务</p>
</li>
<li>
<p>生成任务输入和输出</p>
<ul>
<li>
<p>非分类任务：输入优先（即先产生输入，模型再根据任务指令和输入生成输出）</p>
</li>
<li>
<p>分类任务：输出优先（即先产生所有可能的输出标签，再根据任务指令和输出，补充相应的输入）</p>
<p>目的是避免模型过多地生成某些特定类别的输入，而忽略其他的类别。</p>
</li>
</ul>
</li>
<li>
<p>在将新生成的指令数据加入指令池之前，过滤低质量数据</p>
</li>
</ol>
</li>
</ol>
<p> </p>
<h3 id="13-数据评估与影响">1.3 数据评估与影响</h3>
<ol>
<li>
<h4 id="评估数据">评估数据</h4>
<p>数据质量、数据多样性&hellip;</p>
</li>
<li>
<h4 id="数据对结果影响">数据对结果影响</h4>
<ol>
<li>若能将数据质量与多样性标准有机结合，模型可以达到更好的效果</li>
<li>仅需 60 个训练样本的指令微调，就足以使大语言模型高效执行问答任务，并展现出强大的泛化能力。</li>
<li>增加训练数据并未带来显著的性能提升，反而可能损害模型表现。</li>
</ol>
</li>
</ol>
<p> </p>
<h3 id="14-指令微调训练策略">1.4 指令微调训练策略</h3>
<p><img src="/images/LLM3/3.png" alt="3"></p>
<p>混合各类训练数据在一起可以在一定程度上增强所有任务效果。</p>
<ul>
<li>开源数据集可分为 通用指令微调数据集 和 特定领域指令微调数据集</li>
</ul>
<p> </p>
<p> </p>
<h2 id="2-高效模型微调方法">2. 高效模型微调方法</h2>
<h3 id="21-传统微调方法">2.1 传统微调方法</h3>
<ol>
<li>
<h4 id="微调适配器adapter-layer">微调适配器（Adapter Layer）</h4>
<p>在 Transformer 层中的自注意力模块与 MLP 块之间、以及 MLP 模块与残差连接之间，添加适配器层（Adapter Layer）作为可训练参数。</p>
<p><strong>缺点</strong>：增加网络的深度，从而在模型推理时带来额外的时间开销。</p>
</li>
<li>
<h4 id="前缀微调">前缀微调</h4>
<p>在输入序列前缀添加连续可微的软提示作为可训练参数。</p>
<p><strong>缺点</strong>：模型可接受的最大输入长度有限，随着软提示的参数量增多，实际输入序列的最大长度也会相应减小，影响模型性能。</p>
</li>
</ol>
<h3 id="22-lora">2.2 LoRA</h3>
<ul>
<li>
<p><strong>背景</strong>：</p>
<p>语言模型针对特定任务微调后，权重矩阵通常具有<strong>很低的本征秩</strong>。也就是说，参数更新量即便投影到较小的子空间中，也不会影响学习的有效性。</p>
<p>LoRA 方法可以在缩减训练参数量和 GPU 显存占用的同时，使训练后的模型具有与全量微调相当的性能。</p>
</li>
<li>
<p><strong>方法</strong>：固定预训练模型参数不变，在原本权重矩阵旁路添加低秩矩阵的乘积作为可训练参数，用以模拟参数的变化量。</p>
</li>
</ul>
<p>对于输入 x 来说，输出如下：</p>
<p><img src="/images/LLM3/4.png" alt="4"></p>
<p>算法结构：</p>
<p><img src="/images/LLM3/5.png" alt="5"></p>
<p><strong>函数库</strong>：peft</p>
<p> </p>
<h3 id="23-lora-的变体">2.3 LoRA 的变体</h3>
<ol>
<li>
<h4 id="adalora">AdaLoRA</h4>
<p>微调时，根据<strong>各权重矩阵对下游任务的重要性</strong>动态调整秩的大小</p>
<ul>
<li>
<p>实现方法</p>
<ol>
<li>
<p><strong>直接令重要性分数等于奇异值</strong></p>
<p>对可训练参数 ∆W 进行奇异值分解，令 ∆W= P Γ Q</p>
<p>并在损失函数中添加以下正则化项：</p>
<p><img src="/images/LLM3/6.png" alt="6"></p>
<p>为每一组奇异值及其奇异向量计算重要性分数，根据所有组的重要性分数排序来裁剪权重矩阵，以达到降秩的目的。</p>
</li>
<li>
<p>计算参数敏感性</p>
<p><img src="/images/LLM3/7.png" alt="7"></p>
<p>该式估计了某个参数变为 0 后，损失函数值的变化。因此， I(wij) 越大，表示模型对该参数越敏感，这个参数也就越应该被保留。</p>
</li>
</ol>
</li>
</ul>
</li>
<li>
<h4 id="qlora">QLoRA</h4>
<p>没有对 LoRA 的逻辑做出修改，而是通过将预训练模型量化为 4-bit 节省计算开销，并保持原本 16-bit 微调的性能。</p>
<ul>
<li>实现方法
<ul>
<li><strong>分页优化器</strong>：指在训练过程中显存不足时自动将优化器状态移至内存，在需要更新优化器状态时再加载回来。</li>
<li>基于分位数量化构建新的数据类型 4-bit NormalFloat（<strong>NF4</strong>）：使原数据经量化后，每个量化区间中的值的数量相同。</li>
<li><strong>双重量化</strong>：分块量化减小离群点的影响范围；为了恢复量化后的数据，需要存储每一块数据的放缩系数，故进一步对这些放缩系数进行量化。</li>
</ul>
</li>
</ul>
</li>
</ol>
<p> </p>
<p> </p>
<h2 id="3-上下文窗口扩展方法">3. 上下文窗口扩展方法</h2>
<h3 id="31-增加上下文窗口的微调">3.1 增加上下文窗口的微调</h3>
<p>采用直接的方式，即通过使用一个更大的上下文窗口来微调现有的预训练 Transformer，以适应长文本建模需求。</p>
<p>缺点：在扩展到更大的上下文窗口时效率较低。</p>
<p> </p>
<h3 id="32-具有外推能力的位置编码">3.2 具有外推能力的位置编码</h3>
<p>外推能力源于位置编码中表征<strong>相对位置</strong>信息的部分</p>
<ul>
<li>
<p><strong>T5 Bias Position Embedding</strong></p>
<p>直接在 Attention Map 上操作，对于查询和键之间的不同距离，模型会学习一个偏置的标量值，将其加在注意力分数上，并在每一层都进行此操作，从而学习一个相对位置的编码信息。</p>
</li>
<li>
<p><strong>ALiBi</strong></p>
<p>一种预定义的相对位置编码。ALiBi 在 Softmax 的结果后添加一个静态的不可学习的偏置项：</p>
<p><img src="/images/LLM3/8.png" alt="8"></p>
<p>其中 m是对不同注意力头设置的斜率值。</p>
<p><img src="/images/LLM3/9.png" alt="9"></p>
</li>
</ul>
<p> </p>
<h3 id="33-插值法">3.3 插值法</h3>
<p>不改变模型架构而直接扩展大语言模型上下文窗口大小。</p>
<p><strong>位置插值法</strong>：直接缩小位置索引，使最大位置索引与预训练阶段的上下文窗口限制相匹配。</p>
<p><img src="/images/LLM3/10.png" alt="10"></p>
<p> </p>
<p> </p>
<h2 id="4-指令微调实践deepspeed-chat-sft">4. 指令微调实践：DeepSpeed-Chat SFT</h2>
<ol>
<li><strong>指令微调</strong>：使用精选的人类回答来微调预训练语言模型以应对各种查询。</li>
<li><strong>奖励模型微调</strong>：使用一个包含人类对同一查询的多个答案打分的数据集来训练一个独立的奖励模型。</li>
<li><strong>基于人类反馈的强化学习</strong>（RLHF）训练：利用近端策略优化 （PPO）算法，根据奖励模型的奖励反馈进一步微调 SFT 模型。</li>
</ol>
<p><img src="/images/LLM3/11.png" alt="11"></p><ul class="pa0">
  
   <li class="list di">
     <a href="/tags/llm/" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">LLM</a>
   </li>
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




  <div class="bg-light-gray pa3 nested-list-reset nested-copy-line-height nested-links">
    <p class="f5 b mb3">Related</p>
    <ul class="pa0 list">
	   
	     <li  class="mb2">
          <a href="/post/llm2/">LLM - 2.预训练阶段</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/post/llm1/">LLM - 1.基础理论</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/post/bert/">BERT</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/post/%E5%BE%AE%E8%B0%83/">微调</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/post/mllm/">MLLM</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/post/transformer/">transformer</a>
        </li>
	    
    </ul>
</div>

</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://localhost:1313/" >
    &copy;  HomePage 2025 
  </a>
    <div><div class="ananke-socials"><a href="https://www.facebook.com/patrick.kollitsch" target="_blank" rel="noopener"
        class="ananke-social-link link-transition facebook link dib z-999 pt3 pt0-l mr1"
        title="follow on Facebook - Opens in a new window"
        aria-label="follow on Facebook - Opens in a new window">
      <span class="icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M512 256C512 114.6 397.4 0 256 0S0 114.6 0 256C0 376 82.7 476.8 194.2 504.5V334.2H141.4V256h52.8V222.3c0-87.1 39.4-127.5 125-127.5c16.2 0 44.2 3.2 55.7 6.4V172c-6-.6-16.5-1-29.6-1c-42 0-58.2 15.9-58.2 57.2V256h83.6l-14.4 78.2H287V510.1C413.8 494.8 512 386.9 512 256h0z"/></svg>
            
          </span></a><a href="https://bsky.app/profile/kollitsch.dev" target="_blank" rel="noopener"
        class="ananke-social-link link-transition bluesky link dib z-999 pt3 pt0-l mr1"
        title="follow on Bluesky - Opens in a new window"
        aria-label="follow on Bluesky - Opens in a new window">
      <span class="icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M407.8 294.7c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3zM288 227.1C261.9 176.4 190.9 81.9 124.9 35.3C61.6-9.4 37.5-1.7 21.6 5.5C3.3 13.8 0 41.9 0 58.4S9.1 194 15 213.9c19.5 65.7 89.1 87.9 153.2 80.7c3.3-.5 6.6-.9 10-1.4c-3.3 .5-6.6 1-10 1.4C74.3 308.6-9.1 342.8 100.3 464.5C220.6 589.1 265.1 437.8 288 361.1c22.9 76.7 49.2 222.5 185.6 103.4c102.4-103.4 28.1-156-65.8-169.9c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3c64.1 7.1 133.6-15.1 153.2-80.7C566.9 194 576 75 576 58.4s-3.3-44.7-21.6-52.9c-15.8-7.1-40-14.9-103.2 29.8C385.1 81.9 314.1 176.4 288 227.1z"/></svg>
            
          </span></a><a href="http://linkedin.com/in/patrickkollitsch" target="_blank" rel="noopener"
        class="ananke-social-link link-transition linkedin link dib z-999 pt3 pt0-l mr1"
        title="follow on LinkedIn - Opens in a new window"
        aria-label="follow on LinkedIn - Opens in a new window">
      <span class="icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
            
          </span></a></div>
</div>
  </div>
</footer>

  </body>
</html>
