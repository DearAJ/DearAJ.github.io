<!DOCTYPE html>
<html lang="en-US">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>PPO | HomePage</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="1. 论文详读
Proximal Policy Optimization Algorithms（Proximal：近似）
2. PPO
回顾 TRPO


使用 KL 散度约束 policy 的更新幅度；使用重要性采样
缺点：近似会带来误差（重要性采样的通病）；解带约束的优化问题困难


PPO 的改进


TRPO 采用重要性采样 &mdash;-&gt; PPO 采用 clip 截断，限制新旧策略差异，避免更新过大。


优势函数 At 选用多步时序差分


自适应的 KL 惩罚项






Critic网络训练：
通过最小化critic_loss = MSE(critic(states), td_target)，让critic的价值估计更准确


Actor网络更新：

TD误差的广义形式(GAE)被用作优势函数，指导策略更新方向
优势函数越大，表示该动作比平均表现更好，应被加强



 
3. PPO-惩罚
PPO-惩罚（PPO-Penalty）：用拉格朗日乘数法将 KL 散度的限制放进了目标函数中，使其变成了一个无约束的优化问题，在迭代的过程中不断更新 KL 散度前的系数 beta。">
    <meta name="generator" content="Hugo 0.140.2">
    
    
    
      <meta name="robots" content="noindex, nofollow">
    
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



    

    
      

    

    

    
      <link rel="canonical" href="http://localhost:1313/post/ppo/">
    

    <meta property="og:url" content="http://localhost:1313/post/ppo/">
  <meta property="og:site_name" content="HomePage">
  <meta property="og:title" content="PPO">
  <meta property="og:description" content="1. 论文详读 Proximal Policy Optimization Algorithms（Proximal：近似）
2. PPO 回顾 TRPO 使用 KL 散度约束 policy 的更新幅度；使用重要性采样
缺点：近似会带来误差（重要性采样的通病）；解带约束的优化问题困难
PPO 的改进 TRPO 采用重要性采样 —-&gt; PPO 采用 clip 截断，限制新旧策略差异，避免更新过大。
优势函数 At 选用多步时序差分
自适应的 KL 惩罚项
Critic网络训练：
通过最小化critic_loss = MSE(critic(states), td_target)，让critic的价值估计更准确
Actor网络更新：
TD误差的广义形式(GAE)被用作优势函数，指导策略更新方向 优势函数越大，表示该动作比平均表现更好，应被加强 3. PPO-惩罚 PPO-惩罚（PPO-Penalty）：用拉格朗日乘数法将 KL 散度的限制放进了目标函数中，使其变成了一个无约束的优化问题，在迭代的过程中不断更新 KL 散度前的系数 beta。">
  <meta property="og:locale" content="en_US">
  <meta property="og:type" content="article">
    <meta property="article:section" content="post">
    <meta property="article:published_time" content="2025-04-03T04:00:59-07:00">
    <meta property="article:modified_time" content="2025-04-03T04:00:59-07:00">
    <meta property="article:tag" content="RL">

  <meta itemprop="name" content="PPO">
  <meta itemprop="description" content="1. 论文详读 Proximal Policy Optimization Algorithms（Proximal：近似）
2. PPO 回顾 TRPO 使用 KL 散度约束 policy 的更新幅度；使用重要性采样
缺点：近似会带来误差（重要性采样的通病）；解带约束的优化问题困难
PPO 的改进 TRPO 采用重要性采样 —-&gt; PPO 采用 clip 截断，限制新旧策略差异，避免更新过大。
优势函数 At 选用多步时序差分
自适应的 KL 惩罚项
Critic网络训练：
通过最小化critic_loss = MSE(critic(states), td_target)，让critic的价值估计更准确
Actor网络更新：
TD误差的广义形式(GAE)被用作优势函数，指导策略更新方向 优势函数越大，表示该动作比平均表现更好，应被加强 3. PPO-惩罚 PPO-惩罚（PPO-Penalty）：用拉格朗日乘数法将 KL 散度的限制放进了目标函数中，使其变成了一个无约束的优化问题，在迭代的过程中不断更新 KL 散度前的系数 beta。">
  <meta itemprop="datePublished" content="2025-04-03T04:00:59-07:00">
  <meta itemprop="dateModified" content="2025-04-03T04:00:59-07:00">
  <meta itemprop="wordCount" content="754">
  <meta itemprop="keywords" content="RL">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="PPO">
  <meta name="twitter:description" content="1. 论文详读 Proximal Policy Optimization Algorithms（Proximal：近似）
2. PPO 回顾 TRPO 使用 KL 散度约束 policy 的更新幅度；使用重要性采样
缺点：近似会带来误差（重要性采样的通病）；解带约束的优化问题困难
PPO 的改进 TRPO 采用重要性采样 —-&gt; PPO 采用 clip 截断，限制新旧策略差异，避免更新过大。
优势函数 At 选用多步时序差分
自适应的 KL 惩罚项
Critic网络训练：
通过最小化critic_loss = MSE(critic(states), td_target)，让critic的价值估计更准确
Actor网络更新：
TD误差的广义形式(GAE)被用作优势函数，指导策略更新方向 优势函数越大，表示该动作比平均表现更好，应被加强 3. PPO-惩罚 PPO-惩罚（PPO-Penalty）：用拉格朗日乘数法将 KL 散度的限制放进了目标函数中，使其变成了一个无约束的优化问题，在迭代的过程中不断更新 KL 散度前的系数 beta。">

	
  </head><body class="ma0 avenir bg-near-white development">

    
   
  

  
  
  
  <header class="cover bg-center" style="background-image: url('http://localhost:1313/images/PPO2/pia.jpg');">
    <div class="bg-black-60">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        HomePage
      
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/about/" title="About ME page">
              About ME
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/post/" title="Articles page">
              Articles
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/contact/" title="Contact page">
              Contact
            </a>
          </li>
          
        </ul>
      
      <div class="ananke-socials"><a href="https://www.facebook.com/patrick.kollitsch" target="_blank" rel="noopener"
        class="ananke-social-link link-transition facebook link dib z-999 pt3 pt0-l mr1"
        title="follow on Facebook - Opens in a new window"
        aria-label="follow on Facebook - Opens in a new window">
      <span class="icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M512 256C512 114.6 397.4 0 256 0S0 114.6 0 256C0 376 82.7 476.8 194.2 504.5V334.2H141.4V256h52.8V222.3c0-87.1 39.4-127.5 125-127.5c16.2 0 44.2 3.2 55.7 6.4V172c-6-.6-16.5-1-29.6-1c-42 0-58.2 15.9-58.2 57.2V256h83.6l-14.4 78.2H287V510.1C413.8 494.8 512 386.9 512 256h0z"/></svg>
            
          </span></a><a href="https://bsky.app/profile/kollitsch.dev" target="_blank" rel="noopener"
        class="ananke-social-link link-transition bluesky link dib z-999 pt3 pt0-l mr1"
        title="follow on Bluesky - Opens in a new window"
        aria-label="follow on Bluesky - Opens in a new window">
      <span class="icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M407.8 294.7c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3zM288 227.1C261.9 176.4 190.9 81.9 124.9 35.3C61.6-9.4 37.5-1.7 21.6 5.5C3.3 13.8 0 41.9 0 58.4S9.1 194 15 213.9c19.5 65.7 89.1 87.9 153.2 80.7c3.3-.5 6.6-.9 10-1.4c-3.3 .5-6.6 1-10 1.4C74.3 308.6-9.1 342.8 100.3 464.5C220.6 589.1 265.1 437.8 288 361.1c22.9 76.7 49.2 222.5 185.6 103.4c102.4-103.4 28.1-156-65.8-169.9c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3c64.1 7.1 133.6-15.1 153.2-80.7C566.9 194 576 75 576 58.4s-3.3-44.7-21.6-52.9c-15.8-7.1-40-14.9-103.2 29.8C385.1 81.9 314.1 176.4 288 227.1z"/></svg>
            
          </span></a><a href="http://linkedin.com/in/patrickkollitsch" target="_blank" rel="noopener"
        class="ananke-social-link link-transition linkedin link dib z-999 pt3 pt0-l mr1"
        title="follow on LinkedIn - Opens in a new window"
        aria-label="follow on LinkedIn - Opens in a new window">
      <span class="icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
            
          </span></a></div>

    </div>
  </div>
</nav>

      <div class="tc-l pv6 ph3 ph4-ns">
        
          <div class="f2 f1-l fw2 white-90 mb0 lh-title">PPO</div>
          
        
      </div>
    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked ttu">
          
        Articles
      </aside><div id="sharing" class="mt3 ananke-socials"><a href="mailto:?&amp;body=http%3A%2F%2Flocalhost%3A1313%2Fpost%2Fppo%2F&amp;subject=PPO"
        class="ananke-social-link email no-underline"
        title="Share on Email" aria-label="Share on Email"
        target="_blank" rel="nofollow noopener noreferrer">
        <span class="icon">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M64 112c-8.8 0-16 7.2-16 16l0 22.1L220.5 291.7c20.7 17 50.4 17 71.1 0L464 150.1l0-22.1c0-8.8-7.2-16-16-16L64 112zM48 212.2L48 384c0 8.8 7.2 16 16 16l384 0c8.8 0 16-7.2 16-16l0-171.8L322 328.8c-38.4 31.5-93.7 31.5-132 0L48 212.2zM0 128C0 92.7 28.7 64 64 64l384 0c35.3 0 64 28.7 64 64l0 256c0 35.3-28.7 64-64 64L64 448c-35.3 0-64-28.7-64-64L0 128z"/></svg>
                
              </span></a><a href="https://facebook.com/sharer/sharer.php?&amp;u=http%3A%2F%2Flocalhost%3A1313%2Fpost%2Fppo%2F"
        class="ananke-social-link facebook no-underline"
        title="Share on Facebook" aria-label="Share on Facebook"
        target="_blank" rel="nofollow noopener noreferrer">
        <span class="icon">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M512 256C512 114.6 397.4 0 256 0S0 114.6 0 256C0 376 82.7 476.8 194.2 504.5V334.2H141.4V256h52.8V222.3c0-87.1 39.4-127.5 125-127.5c16.2 0 44.2 3.2 55.7 6.4V172c-6-.6-16.5-1-29.6-1c-42 0-58.2 15.9-58.2 57.2V256h83.6l-14.4 78.2H287V510.1C413.8 494.8 512 386.9 512 256h0z"/></svg>
                
              </span></a><a href="https://bsky.app/intent/compose?&amp;text=http%3A%2F%2Flocalhost%3A1313%2Fpost%2Fppo%2F"
        class="ananke-social-link bluesky no-underline"
        title="Share on Bluesky" aria-label="Share on Bluesky"
        target="_blank" rel="nofollow noopener noreferrer">
        <span class="icon">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M407.8 294.7c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3zM288 227.1C261.9 176.4 190.9 81.9 124.9 35.3C61.6-9.4 37.5-1.7 21.6 5.5C3.3 13.8 0 41.9 0 58.4S9.1 194 15 213.9c19.5 65.7 89.1 87.9 153.2 80.7c3.3-.5 6.6-.9 10-1.4c-3.3 .5-6.6 1-10 1.4C74.3 308.6-9.1 342.8 100.3 464.5C220.6 589.1 265.1 437.8 288 361.1c22.9 76.7 49.2 222.5 185.6 103.4c102.4-103.4 28.1-156-65.8-169.9c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3c64.1 7.1 133.6-15.1 153.2-80.7C566.9 194 576 75 576 58.4s-3.3-44.7-21.6-52.9c-15.8-7.1-40-14.9-103.2 29.8C385.1 81.9 314.1 176.4 288 227.1z"/></svg>
                
              </span></a><a href="https://www.linkedin.com/shareArticle?&amp;mini=true&amp;source=http%3A%2F%2Flocalhost%3A1313%2Fpost%2Fppo%2F&amp;summary=1.&#43;%E8%AE%BA%E6%96%87%E8%AF%A6%E8%AF%BB&#43;Proximal&#43;Policy&#43;Optimization&#43;Algorithms%EF%BC%88Proximal%EF%BC%9A%E8%BF%91%E4%BC%BC%EF%BC%89%0A2.&#43;PPO&#43;%E5%9B%9E%E9%A1%BE&#43;TRPO&#43;%E4%BD%BF%E7%94%A8&#43;KL&#43;%E6%95%A3%E5%BA%A6%E7%BA%A6%E6%9D%9F&#43;policy&#43;%E7%9A%84%E6%9B%B4%E6%96%B0%E5%B9%85%E5%BA%A6%EF%BC%9B%E4%BD%BF%E7%94%A8%E9%87%8D%E8%A6%81%E6%80%A7%E9%87%87%E6%A0%B7%0A%E7%BC%BA%E7%82%B9%EF%BC%9A%E8%BF%91%E4%BC%BC%E4%BC%9A%E5%B8%A6%E6%9D%A5%E8%AF%AF%E5%B7%AE%EF%BC%88%E9%87%8D%E8%A6%81%E6%80%A7%E9%87%87%E6%A0%B7%E7%9A%84%E9%80%9A%E7%97%85%EF%BC%89%EF%BC%9B%E8%A7%A3%E5%B8%A6%E7%BA%A6%E6%9D%9F%E7%9A%84%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%E5%9B%B0%E9%9A%BE%0APPO&#43;%E7%9A%84%E6%94%B9%E8%BF%9B&#43;TRPO&#43;%E9%87%87%E7%94%A8%E9%87%8D%E8%A6%81%E6%80%A7%E9%87%87%E6%A0%B7&#43;%26amp%3Bmdash%3B-%26amp%3Bgt%3B&#43;PPO&#43;%E9%87%87%E7%94%A8&#43;clip&#43;%E6%88%AA%E6%96%AD%EF%BC%8C%E9%99%90%E5%88%B6%E6%96%B0%E6%97%A7%E7%AD%96%E7%95%A5%E5%B7%AE%E5%BC%82%EF%BC%8C%E9%81%BF%E5%85%8D%E6%9B%B4%E6%96%B0%E8%BF%87%E5%A4%A7%E3%80%82%0A%E4%BC%98%E5%8A%BF%E5%87%BD%E6%95%B0&#43;At&#43;%E9%80%89%E7%94%A8%E5%A4%9A%E6%AD%A5%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%0A%E8%87%AA%E9%80%82%E5%BA%94%E7%9A%84&#43;KL&#43;%E6%83%A9%E7%BD%9A%E9%A1%B9%0ACritic%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%EF%BC%9A%0A%E9%80%9A%E8%BF%87%E6%9C%80%E5%B0%8F%E5%8C%96critic_loss&#43;%3D&#43;MSE%28critic%28states%29%2C&#43;td_target%29%EF%BC%8C%E8%AE%A9critic%E7%9A%84%E4%BB%B7%E5%80%BC%E4%BC%B0%E8%AE%A1%E6%9B%B4%E5%87%86%E7%A1%AE%0AActor%E7%BD%91%E7%BB%9C%E6%9B%B4%E6%96%B0%EF%BC%9A%0ATD%E8%AF%AF%E5%B7%AE%E7%9A%84%E5%B9%BF%E4%B9%89%E5%BD%A2%E5%BC%8F%28GAE%29%E8%A2%AB%E7%94%A8%E4%BD%9C%E4%BC%98%E5%8A%BF%E5%87%BD%E6%95%B0%EF%BC%8C%E6%8C%87%E5%AF%BC%E7%AD%96%E7%95%A5%E6%9B%B4%E6%96%B0%E6%96%B9%E5%90%91&#43;%E4%BC%98%E5%8A%BF%E5%87%BD%E6%95%B0%E8%B6%8A%E5%A4%A7%EF%BC%8C%E8%A1%A8%E7%A4%BA%E8%AF%A5%E5%8A%A8%E4%BD%9C%E6%AF%94%E5%B9%B3%E5%9D%87%E8%A1%A8%E7%8E%B0%E6%9B%B4%E5%A5%BD%EF%BC%8C%E5%BA%94%E8%A2%AB%E5%8A%A0%E5%BC%BA&#43;3.&#43;PPO-%E6%83%A9%E7%BD%9A&#43;PPO-%E6%83%A9%E7%BD%9A%EF%BC%88PPO-Penalty%EF%BC%89%EF%BC%9A%E7%94%A8%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%95%B0%E6%B3%95%E5%B0%86&#43;KL&#43;%E6%95%A3%E5%BA%A6%E7%9A%84%E9%99%90%E5%88%B6%E6%94%BE%E8%BF%9B%E4%BA%86%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E4%B8%AD%EF%BC%8C%E4%BD%BF%E5%85%B6%E5%8F%98%E6%88%90%E4%BA%86%E4%B8%80%E4%B8%AA%E6%97%A0%E7%BA%A6%E6%9D%9F%E7%9A%84%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%EF%BC%8C%E5%9C%A8%E8%BF%AD%E4%BB%A3%E7%9A%84%E8%BF%87%E7%A8%8B%E4%B8%AD%E4%B8%8D%E6%96%AD%E6%9B%B4%E6%96%B0&#43;KL&#43;%E6%95%A3%E5%BA%A6%E5%89%8D%E7%9A%84%E7%B3%BB%E6%95%B0&#43;beta%E3%80%82%0A&amp;title=PPO&amp;url=http%3A%2F%2Flocalhost%3A1313%2Fpost%2Fppo%2F"
        class="ananke-social-link linkedin no-underline"
        title="Share on LinkedIn" aria-label="Share on LinkedIn"
        target="_blank" rel="nofollow noopener noreferrer">
        <span class="icon">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
                
              </span></a></div>
<h1 class="f1 athelas mt3 mb1">PPO</h1>
      
      
      
      <time class="f6 mv4 dib tracked" datetime="2025-04-03T04:00:59-07:00">April 3, 2025</time>
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><h2 id="1-论文详读">1. 论文详读</h2>
<p><a href="https://arxiv.org/abs/1707.06347">Proximal Policy Optimization Algorithms</a>（Proximal：近似）</p>
<h2 id="2-ppo">2. PPO</h2>
<h4 id="回顾-trpo">回顾 TRPO</h4>
<ul>
<li>
<p>使用 KL 散度约束 policy 的更新幅度；使用重要性采样</p>
<p><strong>缺点</strong>：近似会带来误差（重要性采样的通病）；解带约束的优化问题困难</p>
</li>
<li>
<h4 id="ppo-的改进">PPO 的改进</h4>
<ol>
<li>
<p>TRPO 采用重要性采样 &mdash;-&gt; PPO 采用 <strong>clip 截断</strong>，限制新旧策略差异，避免更新过大。</p>
</li>
<li>
<p>优势函数 At 选用多步时序差分</p>
</li>
<li>
<p>自适应的 KL 惩罚项</p>
</li>
</ol>
</li>
</ul>
<ol>
<li>
<p><strong>Critic网络训练</strong>：</p>
<p>通过最小化<code>critic_loss = MSE(critic(states), td_target)</code>，让critic的价值估计更准确</p>
</li>
<li>
<p><strong>Actor网络更新</strong>：</p>
<ul>
<li>TD误差的广义形式(GAE)被用作优势函数，指导策略更新方向</li>
<li>优势函数越大，表示该动作比平均表现更好，应被加强</li>
</ul>
</li>
</ol>
<p> </p>
<h2 id="3-ppo-惩罚">3. PPO-惩罚</h2>
<p>PPO-惩罚（PPO-Penalty）：用拉格朗日乘数法将 KL 散度的限制放进了目标函数中，使其变成了一个无约束的优化问题，在迭代的过程中不断更新 KL 散度前的系数 beta。</p>
<p><img src="/images/PPO2/2.png" alt="2"></p>
<ul>
<li>
<p>dk 即为 KL 散度值。</p>
<p>第一种情况 dk 小，说明安全，关注前项；第二种情况 dk 大，不安全，故乘 2 关注后一项。</p>
</li>
</ul>
<p> </p>
<h2 id="4-ppo-截断">4. PPO-截断</h2>
<p>在目标函数中进行限制，以保证新的参数和旧的参数的差距不会太大。</p>
<p><img src="/images/PPO2/3.png" alt="3"></p>
<p>前一项是原来的，后一项是要做截断的；</p>
<p>保证更新幅度不会过大或过小。</p>
<p><img src="/images/PPO2/4.png" alt="4"></p>
<p> </p>
<h2 id="5-ppo-代码实践">5. PPO 代码实践</h2>
<p>大量实验表明，PPO-截断总是比 PPO-惩罚表现得更好。因此下面我们使用 <strong>PPO-截断</strong> 的代码实现。</p>
<p><img src="/images/PPO2/5.png" alt="5"></p>
<ul>
<li>
<h4 id="td-误差">TD 误差</h4>
<ol>
<li>
<p>计算TD目标</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>td_target <span style="color:#f92672">=</span> rewards <span style="color:#f92672">+</span> γ <span style="color:#f92672">*</span> critic(next_states) <span style="color:#f92672">*</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> dones)
</span></span></code></pre></div><ul>
<li><code>rewards</code>：当前获得的即时奖励（r_t）</li>
<li><code>critic(next_states)</code>：critic网络对下一状态的估值（V(s_{t+1})）</li>
<li><code>(1 - dones)</code>：终止状态处理（如果done=True，则忽略后续状态价值）</li>
</ul>
</li>
<li>
<p>计算TD误差</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>td_delta <span style="color:#f92672">=</span> td_target <span style="color:#f92672">-</span> critic(states)
</span></span></code></pre></div><ul>
<li><code>critic(states)</code>：critic 网络对当前状态的估值（V(s_t)）</li>
<li>结果即为<code>δ_t = (r_t + γV(s_{t+1})) - V(s_t)</code></li>
</ul>
</li>
</ol>
<p><img src="/images/PPO2/7.png" alt="7"></p>
</li>
<li>
<h4 id="优势函数gae">优势函数（GAE）</h4>
<p>核心：<strong>对多个时间步的TD误差进行加权平均</strong></p>
<p><img src="/images/PPO2/8.png" alt="8"></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>advantage <span style="color:#f92672">=</span> rl_utils<span style="color:#f92672">.</span>compute_advantage(gamma, lmbda, td_delta)
</span></span></code></pre></div></li>
</ul>
<h3 id="离散环境">离散环境</h3>
<ol>
<li>
<h4 id="定义策略网络和价值网络">定义策略网络和价值网络</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> gym
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn.functional <span style="color:#66d9ef">as</span> F
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> rl_utils
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">PolicyNet</span>(torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, state_dim, hidden_dim, action_dim):
</span></span><span style="display:flex;"><span>        super(PolicyNet, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>fc1 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Linear(state_dim, hidden_dim)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>fc2 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Linear(hidden_dim, action_dim)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>fc1(x))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> F<span style="color:#f92672">.</span>softmax(self<span style="color:#f92672">.</span>fc2(x), dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)	 <span style="color:#75715e"># 输出动作概率分布</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">ValueNet</span>(torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, state_dim, hidden_dim):
</span></span><span style="display:flex;"><span>        super(ValueNet, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>fc1 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Linear(state_dim, hidden_dim)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>fc2 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Linear(hidden_dim, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>fc1(x))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>fc2(x)	  <span style="color:#75715e"># 不接激活函数，直接输出值</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">PPO</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;&#39;&#39; PPO算法,采用截断方式 &#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, state_dim, hidden_dim, action_dim, actor_lr, critic_lr,
</span></span><span style="display:flex;"><span>                 lmbda, epochs, eps, gamma, device):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>actor <span style="color:#f92672">=</span> PolicyNet(state_dim, hidden_dim, action_dim)<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>critic <span style="color:#f92672">=</span> ValueNet(state_dim, hidden_dim)<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>actor_optimizer <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>optim<span style="color:#f92672">.</span>Adam(self<span style="color:#f92672">.</span>actor<span style="color:#f92672">.</span>parameters(),
</span></span><span style="display:flex;"><span>                                                lr<span style="color:#f92672">=</span>actor_lr)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>critic_optimizer <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>optim<span style="color:#f92672">.</span>Adam(self<span style="color:#f92672">.</span>critic<span style="color:#f92672">.</span>parameters(),
</span></span><span style="display:flex;"><span>                                                 lr<span style="color:#f92672">=</span>critic_lr)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>gamma <span style="color:#f92672">=</span> gamma    <span style="color:#75715e"># 折扣因子</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>lmbda <span style="color:#f92672">=</span> lmbda    <span style="color:#75715e"># GAE参数</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>epochs <span style="color:#f92672">=</span> epochs	<span style="color:#75715e"># 一条序列的数据用来训练轮数</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>eps <span style="color:#f92672">=</span> eps  			<span style="color:#75715e"># PPO中截断范围的参数</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>device <span style="color:#f92672">=</span> device
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>     <span style="color:#e6db74">&#39;&#39;&#39;通过策略网络得到动作概率分布；使用分类分布进行动作采样&#39;&#39;&#39;</span>   
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">take_action</span>(self, state):
</span></span><span style="display:flex;"><span>        state <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([state], dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float)<span style="color:#f92672">.</span>to(self<span style="color:#f92672">.</span>device)
</span></span><span style="display:flex;"><span>        probs <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>actor(state)
</span></span><span style="display:flex;"><span>        action_dist <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>distributions<span style="color:#f92672">.</span>Categorical(probs)
</span></span><span style="display:flex;"><span>        action <span style="color:#f92672">=</span> action_dist<span style="color:#f92672">.</span>sample()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> action<span style="color:#f92672">.</span>item()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;&#39;&#39;关键部分，实现了PPO的更新逻辑。&#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">update</span>(self, transition_dict):
</span></span><span style="display:flex;"><span>        states <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(transition_dict[<span style="color:#e6db74">&#39;states&#39;</span>], dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float)<span style="color:#f92672">.</span>to(self<span style="color:#f92672">.</span>device)
</span></span><span style="display:flex;"><span>        actions <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(transition_dict[<span style="color:#e6db74">&#39;actions&#39;</span>])<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>to(self<span style="color:#f92672">.</span>device)
</span></span><span style="display:flex;"><span>        rewards <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(transition_dict[<span style="color:#e6db74">&#39;rewards&#39;</span>], dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float)<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>to(self<span style="color:#f92672">.</span>device)
</span></span><span style="display:flex;"><span>        next_states <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(transition_dict[<span style="color:#e6db74">&#39;next_states&#39;</span>], dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float)<span style="color:#f92672">.</span>to(self<span style="color:#f92672">.</span>device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        dones <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(transition_dict[<span style="color:#e6db74">&#39;dones&#39;</span>], dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float)<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>to(self<span style="color:#f92672">.</span>device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 计算TD目标</span>
</span></span><span style="display:flex;"><span>        td_target <span style="color:#f92672">=</span> rewards <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>gamma <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>critic(next_states) <span style="color:#f92672">*</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> dones)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 计算优势函数（使用GAE）</span>
</span></span><span style="display:flex;"><span>        td_delta <span style="color:#f92672">=</span> td_target <span style="color:#f92672">-</span> self<span style="color:#f92672">.</span>critic(states)
</span></span><span style="display:flex;"><span>        advantage <span style="color:#f92672">=</span> rl_utils<span style="color:#f92672">.</span>compute_advantage(self<span style="color:#f92672">.</span>gamma, self<span style="color:#f92672">.</span>lmbda, td_delta<span style="color:#f92672">.</span>cpu())<span style="color:#f92672">.</span>to(self<span style="color:#f92672">.</span>device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 旧策略的概率（固定不更新）</span>
</span></span><span style="display:flex;"><span>        old_log_probs <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>log(self<span style="color:#f92672">.</span>actor(states)<span style="color:#f92672">.</span>gather(<span style="color:#ae81ff">1</span>, actions))<span style="color:#f92672">.</span>detach()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(self<span style="color:#f92672">.</span>epochs):
</span></span><span style="display:flex;"><span>          	<span style="color:#75715e"># 计算新旧策略概率比</span>
</span></span><span style="display:flex;"><span>            log_probs <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>log(self<span style="color:#f92672">.</span>actor(states)<span style="color:#f92672">.</span>gather(<span style="color:#ae81ff">1</span>, actions))
</span></span><span style="display:flex;"><span>            ratio <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>exp(log_probs <span style="color:#f92672">-</span> old_log_probs)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># PPO核心损失计算</span>
</span></span><span style="display:flex;"><span>            surr1 <span style="color:#f92672">=</span> ratio <span style="color:#f92672">*</span> advantage
</span></span><span style="display:flex;"><span>            surr2 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>clamp(ratio, <span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> self<span style="color:#f92672">.</span>eps, <span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>eps) <span style="color:#f92672">*</span> advantage  <span style="color:#75715e"># 截断</span>
</span></span><span style="display:flex;"><span>            actor_loss <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>mean(<span style="color:#f92672">-</span>torch<span style="color:#f92672">.</span>min(surr1, surr2))  									<span style="color:#75715e"># PPO损失函数</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># 价值网络损失</span>
</span></span><span style="display:flex;"><span>            critic_loss <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>mean(F<span style="color:#f92672">.</span>mse_loss(self<span style="color:#f92672">.</span>critic(states), td_target<span style="color:#f92672">.</span>detach()))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># 反向传播更新</span>
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>actor_optimizer<span style="color:#f92672">.</span>zero_grad()
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>critic_optimizer<span style="color:#f92672">.</span>zero_grad()
</span></span><span style="display:flex;"><span>            actor_loss<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>            critic_loss<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>actor_optimizer<span style="color:#f92672">.</span>step()
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>critic_optimizer<span style="color:#f92672">.</span>step()
</span></span></code></pre></div><p> </p>
</li>
<li>
<h4 id="在车杆环境中训练-ppo-算法">在车杆环境中训练 PPO 算法</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>actor_lr <span style="color:#f92672">=</span> <span style="color:#ae81ff">1e-3</span>
</span></span><span style="display:flex;"><span>critic_lr <span style="color:#f92672">=</span> <span style="color:#ae81ff">1e-2</span>
</span></span><span style="display:flex;"><span>num_episodes <span style="color:#f92672">=</span> <span style="color:#ae81ff">500</span>
</span></span><span style="display:flex;"><span>hidden_dim <span style="color:#f92672">=</span> <span style="color:#ae81ff">128</span>
</span></span><span style="display:flex;"><span>gamma <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.98</span>
</span></span><span style="display:flex;"><span>lmbda <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.95</span>
</span></span><span style="display:flex;"><span>epochs <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span>eps <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.2</span>
</span></span><span style="display:flex;"><span>device <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>device(<span style="color:#e6db74">&#34;cuda&#34;</span>) <span style="color:#66d9ef">if</span> torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>is_available() <span style="color:#66d9ef">else</span> torch<span style="color:#f92672">.</span>device(
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;cpu&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>env_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;CartPole-v0&#39;</span>
</span></span><span style="display:flex;"><span>env <span style="color:#f92672">=</span> gym<span style="color:#f92672">.</span>make(env_name)
</span></span><span style="display:flex;"><span>env<span style="color:#f92672">.</span>seed(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>torch<span style="color:#f92672">.</span>manual_seed(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>state_dim <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>observation_space<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>action_dim <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>action_space<span style="color:#f92672">.</span>n
</span></span><span style="display:flex;"><span>agent <span style="color:#f92672">=</span> PPO(state_dim, hidden_dim, action_dim, actor_lr, critic_lr, lmbda,
</span></span><span style="display:flex;"><span>            epochs, eps, gamma, device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>return_list <span style="color:#f92672">=</span> rl_utils<span style="color:#f92672">.</span>train_on_policy_agent(env, agent, num_episodes)
</span></span></code></pre></div></li>
</ol>
<p> </p>
<h3 id="连续环境">连续环境</h3>
<p>让策略网络输出连续动作<strong>高斯分布</strong>的均值和标准差。后续的连续动作则在该高斯分布中采样得到。</p>
<ol>
<li>
<h4 id="定义网络">定义网络</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">PolicyNetContinuous</span>(torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, state_dim, hidden_dim, action_dim):
</span></span><span style="display:flex;"><span>        super(PolicyNetContinuous, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>fc1 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Linear(state_dim, hidden_dim)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>fc_mu <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Linear(hidden_dim, action_dim)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>fc_std <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Linear(hidden_dim, action_dim)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>fc1(x))
</span></span><span style="display:flex;"><span>        mu <span style="color:#f92672">=</span> <span style="color:#ae81ff">2.0</span> <span style="color:#f92672">*</span> torch<span style="color:#f92672">.</span>tanh(self<span style="color:#f92672">.</span>fc_mu(x))
</span></span><span style="display:flex;"><span>        std <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>softplus(self<span style="color:#f92672">.</span>fc_std(x))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> mu, std
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">PPOContinuous</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;&#39;&#39; 处理连续动作的PPO算法 &#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, state_dim, hidden_dim, action_dim, actor_lr, critic_lr,
</span></span><span style="display:flex;"><span>                 lmbda, epochs, eps, gamma, device):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>actor <span style="color:#f92672">=</span> PolicyNetContinuous(state_dim, hidden_dim,
</span></span><span style="display:flex;"><span>                                         action_dim)<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>critic <span style="color:#f92672">=</span> ValueNet(state_dim, hidden_dim)<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>actor_optimizer <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>optim<span style="color:#f92672">.</span>Adam(self<span style="color:#f92672">.</span>actor<span style="color:#f92672">.</span>parameters(),
</span></span><span style="display:flex;"><span>                                                lr<span style="color:#f92672">=</span>actor_lr)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>critic_optimizer <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>optim<span style="color:#f92672">.</span>Adam(self<span style="color:#f92672">.</span>critic<span style="color:#f92672">.</span>parameters(),
</span></span><span style="display:flex;"><span>                                                 lr<span style="color:#f92672">=</span>critic_lr)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>gamma <span style="color:#f92672">=</span> gamma
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>lmbda <span style="color:#f92672">=</span> lmbda
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>epochs <span style="color:#f92672">=</span> epochs
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>eps <span style="color:#f92672">=</span> eps
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>device <span style="color:#f92672">=</span> device
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">take_action</span>(self, state):
</span></span><span style="display:flex;"><span>        state <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([state], dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float)<span style="color:#f92672">.</span>to(self<span style="color:#f92672">.</span>device)
</span></span><span style="display:flex;"><span>        mu, sigma <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>actor(state)
</span></span><span style="display:flex;"><span>        action_dist <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>distributions<span style="color:#f92672">.</span>Normal(mu, sigma)
</span></span><span style="display:flex;"><span>        action <span style="color:#f92672">=</span> action_dist<span style="color:#f92672">.</span>sample()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> [action<span style="color:#f92672">.</span>item()]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">update</span>(self, transition_dict):
</span></span><span style="display:flex;"><span>        states <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(transition_dict[<span style="color:#e6db74">&#39;states&#39;</span>],
</span></span><span style="display:flex;"><span>                              dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float)<span style="color:#f92672">.</span>to(self<span style="color:#f92672">.</span>device)
</span></span><span style="display:flex;"><span>        actions <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(transition_dict[<span style="color:#e6db74">&#39;actions&#39;</span>],
</span></span><span style="display:flex;"><span>                               dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float)<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>to(self<span style="color:#f92672">.</span>device)
</span></span><span style="display:flex;"><span>        rewards <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(transition_dict[<span style="color:#e6db74">&#39;rewards&#39;</span>],
</span></span><span style="display:flex;"><span>                               dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float)<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>to(self<span style="color:#f92672">.</span>device)
</span></span><span style="display:flex;"><span>        next_states <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(transition_dict[<span style="color:#e6db74">&#39;next_states&#39;</span>],
</span></span><span style="display:flex;"><span>                                   dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float)<span style="color:#f92672">.</span>to(self<span style="color:#f92672">.</span>device)
</span></span><span style="display:flex;"><span>        dones <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(transition_dict[<span style="color:#e6db74">&#39;dones&#39;</span>],
</span></span><span style="display:flex;"><span>                             dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float)<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>to(self<span style="color:#f92672">.</span>device)
</span></span><span style="display:flex;"><span>        rewards <span style="color:#f92672">=</span> (rewards <span style="color:#f92672">+</span> <span style="color:#ae81ff">8.0</span>) <span style="color:#f92672">/</span> <span style="color:#ae81ff">8.0</span>  <span style="color:#75715e"># 和TRPO一样,对奖励进行修改,方便训练</span>
</span></span><span style="display:flex;"><span>        td_target <span style="color:#f92672">=</span> rewards <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>gamma <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>critic(next_states) <span style="color:#f92672">*</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span>
</span></span><span style="display:flex;"><span>                                                                       dones)
</span></span><span style="display:flex;"><span>        td_delta <span style="color:#f92672">=</span> td_target <span style="color:#f92672">-</span> self<span style="color:#f92672">.</span>critic(states)
</span></span><span style="display:flex;"><span>        advantage <span style="color:#f92672">=</span> rl_utils<span style="color:#f92672">.</span>compute_advantage(self<span style="color:#f92672">.</span>gamma, self<span style="color:#f92672">.</span>lmbda,
</span></span><span style="display:flex;"><span>                                               td_delta<span style="color:#f92672">.</span>cpu())<span style="color:#f92672">.</span>to(self<span style="color:#f92672">.</span>device)
</span></span><span style="display:flex;"><span>        mu, std <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>actor(states)
</span></span><span style="display:flex;"><span>        action_dists <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>distributions<span style="color:#f92672">.</span>Normal(mu<span style="color:#f92672">.</span>detach(), std<span style="color:#f92672">.</span>detach())
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 动作是正态分布</span>
</span></span><span style="display:flex;"><span>        old_log_probs <span style="color:#f92672">=</span> action_dists<span style="color:#f92672">.</span>log_prob(actions)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(self<span style="color:#f92672">.</span>epochs):
</span></span><span style="display:flex;"><span>            mu, std <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>actor(states)
</span></span><span style="display:flex;"><span>            action_dists <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>distributions<span style="color:#f92672">.</span>Normal(mu, std)
</span></span><span style="display:flex;"><span>            log_probs <span style="color:#f92672">=</span> action_dists<span style="color:#f92672">.</span>log_prob(actions)
</span></span><span style="display:flex;"><span>            ratio <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>exp(log_probs <span style="color:#f92672">-</span> old_log_probs)
</span></span><span style="display:flex;"><span>            surr1 <span style="color:#f92672">=</span> ratio <span style="color:#f92672">*</span> advantage
</span></span><span style="display:flex;"><span>            surr2 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>clamp(ratio, <span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> self<span style="color:#f92672">.</span>eps, <span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>eps) <span style="color:#f92672">*</span> advantage
</span></span><span style="display:flex;"><span>            actor_loss <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>mean(<span style="color:#f92672">-</span>torch<span style="color:#f92672">.</span>min(surr1, surr2))
</span></span><span style="display:flex;"><span>            critic_loss <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>mean(
</span></span><span style="display:flex;"><span>                F<span style="color:#f92672">.</span>mse_loss(self<span style="color:#f92672">.</span>critic(states), td_target<span style="color:#f92672">.</span>detach()))
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>actor_optimizer<span style="color:#f92672">.</span>zero_grad()
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>critic_optimizer<span style="color:#f92672">.</span>zero_grad()
</span></span><span style="display:flex;"><span>            actor_loss<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>            critic_loss<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>actor_optimizer<span style="color:#f92672">.</span>step()
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>critic_optimizer<span style="color:#f92672">.</span>step()
</span></span></code></pre></div><p> </p>
</li>
<li>
<h4 id="在倒立摆环境中训练">在倒立摆环境中训练</h4>
<p>创建环境<code>Pendulum-v0</code>，并设定随机数种子以便重复实现。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>actor_lr <span style="color:#f92672">=</span> <span style="color:#ae81ff">1e-4</span>
</span></span><span style="display:flex;"><span>critic_lr <span style="color:#f92672">=</span> <span style="color:#ae81ff">5e-3</span>
</span></span><span style="display:flex;"><span>num_episodes <span style="color:#f92672">=</span> <span style="color:#ae81ff">2000</span>
</span></span><span style="display:flex;"><span>hidden_dim <span style="color:#f92672">=</span> <span style="color:#ae81ff">128</span>
</span></span><span style="display:flex;"><span>gamma <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.9</span>
</span></span><span style="display:flex;"><span>lmbda <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.9</span>
</span></span><span style="display:flex;"><span>epochs <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span>eps <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.2</span>
</span></span><span style="display:flex;"><span>device <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>device(<span style="color:#e6db74">&#34;cuda&#34;</span>) <span style="color:#66d9ef">if</span> torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>is_available() <span style="color:#66d9ef">else</span> torch<span style="color:#f92672">.</span>device(
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;cpu&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>env_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;Pendulum-v0&#39;</span>
</span></span><span style="display:flex;"><span>env <span style="color:#f92672">=</span> gym<span style="color:#f92672">.</span>make(env_name)
</span></span><span style="display:flex;"><span>env<span style="color:#f92672">.</span>seed(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>torch<span style="color:#f92672">.</span>manual_seed(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>state_dim <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>observation_space<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>action_dim <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>action_space<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]  <span style="color:#75715e"># 连续动作空间</span>
</span></span><span style="display:flex;"><span>agent <span style="color:#f92672">=</span> PPOContinuous(state_dim, hidden_dim, action_dim, actor_lr, critic_lr,
</span></span><span style="display:flex;"><span>                      lmbda, epochs, eps, gamma, device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>return_list <span style="color:#f92672">=</span> rl_utils<span style="color:#f92672">.</span>train_on_policy_agent(env, agent, num_episodes)
</span></span></code></pre></div></li>
</ol>
<p> </p>
<h2 id="6-ppo-在-chatgpt-中的使用">6. PPO 在 ChatGPT 中的使用</h2>
<p><img src="/images/PPO2/1.png" alt="1"></p>
<p> </p>
<h2 id="总结">总结</h2>
<p>PPO 是 TRPO 的一种改进算法，它在实现上简化了 TRPO 中的复杂计算，并且它在实验中的性能大多数情况下会比 TRPO 更好，因此目前常被用作一种常用的基准算法。</p>
<p>TRPO 和 PPO 都属于<strong>在线策略</strong>学习算法，即使优化目标中包含重要性采样的过程，但其只是用到了上一轮策略的数据，而不是过去所有策略的数据。</p><ul class="pa0">
  
   <li class="list di">
     <a href="/tags/rl/" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">RL</a>
   </li>
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




  <div class="bg-light-gray pa3 nested-list-reset nested-copy-line-height nested-links">
    <p class="f5 b mb3">Related</p>
    <ul class="pa0 list">
	   
	     <li  class="mb2">
          <a href="/post/trpo/">TRPO</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/post/actor-critic/">Actor-Critic</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/post/reinforce/">REINFORCE</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/post/dqn/">DQN (deep Q network)</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/post/qlearing/">Q-learing</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/post/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E6%98%93%E6%B7%B7%E6%B7%86%E7%82%B9/">强化学习-易混淆点</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/post/ppo-%E5%8E%9F%E7%90%86/">PPO-直观理解</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/post/%E5%BE%AE%E8%B0%83/">微调</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/post/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/">强化学习-数学基础</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/post/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3/">强化学习-直观理解</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/post/e2e/">End2End</a>
        </li>
	    
    </ul>
</div>

</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://localhost:1313/" >
    &copy;  HomePage 2025 
  </a>
    <div><div class="ananke-socials"><a href="https://www.facebook.com/patrick.kollitsch" target="_blank" rel="noopener"
        class="ananke-social-link link-transition facebook link dib z-999 pt3 pt0-l mr1"
        title="follow on Facebook - Opens in a new window"
        aria-label="follow on Facebook - Opens in a new window">
      <span class="icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M512 256C512 114.6 397.4 0 256 0S0 114.6 0 256C0 376 82.7 476.8 194.2 504.5V334.2H141.4V256h52.8V222.3c0-87.1 39.4-127.5 125-127.5c16.2 0 44.2 3.2 55.7 6.4V172c-6-.6-16.5-1-29.6-1c-42 0-58.2 15.9-58.2 57.2V256h83.6l-14.4 78.2H287V510.1C413.8 494.8 512 386.9 512 256h0z"/></svg>
            
          </span></a><a href="https://bsky.app/profile/kollitsch.dev" target="_blank" rel="noopener"
        class="ananke-social-link link-transition bluesky link dib z-999 pt3 pt0-l mr1"
        title="follow on Bluesky - Opens in a new window"
        aria-label="follow on Bluesky - Opens in a new window">
      <span class="icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M407.8 294.7c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3zM288 227.1C261.9 176.4 190.9 81.9 124.9 35.3C61.6-9.4 37.5-1.7 21.6 5.5C3.3 13.8 0 41.9 0 58.4S9.1 194 15 213.9c19.5 65.7 89.1 87.9 153.2 80.7c3.3-.5 6.6-.9 10-1.4c-3.3 .5-6.6 1-10 1.4C74.3 308.6-9.1 342.8 100.3 464.5C220.6 589.1 265.1 437.8 288 361.1c22.9 76.7 49.2 222.5 185.6 103.4c102.4-103.4 28.1-156-65.8-169.9c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3c64.1 7.1 133.6-15.1 153.2-80.7C566.9 194 576 75 576 58.4s-3.3-44.7-21.6-52.9c-15.8-7.1-40-14.9-103.2 29.8C385.1 81.9 314.1 176.4 288 227.1z"/></svg>
            
          </span></a><a href="http://linkedin.com/in/patrickkollitsch" target="_blank" rel="noopener"
        class="ananke-social-link link-transition linkedin link dib z-999 pt3 pt0-l mr1"
        title="follow on LinkedIn - Opens in a new window"
        aria-label="follow on LinkedIn - Opens in a new window">
      <span class="icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
            
          </span></a></div>
</div>
  </div>
</footer>

  </body>
</html>
