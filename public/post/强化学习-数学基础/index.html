<!DOCTYPE html>
<html lang="en-US">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>强化学习-数学基础 | HomePage</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="强化学习专注于如何让智能体（Agent）通过与环境的交互来学习最优策略，以最大化累积奖励。它的核心思想是试错学习，智能体通过尝试不同的行动，观察结果并获得奖励或惩罚，从而逐步改进自己的行为策略。">
    <meta name="generator" content="Hugo 0.140.2">
    
    
    
      <meta name="robots" content="noindex, nofollow">
    
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



    

    
      

    

    

    
      <link rel="canonical" href="http://localhost:1313/post/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/">
    

    <meta property="og:url" content="http://localhost:1313/post/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/">
  <meta property="og:site_name" content="HomePage">
  <meta property="og:title" content="强化学习-数学基础">
  <meta property="og:description" content="强化学习专注于如何让智能体（Agent）通过与环境的交互来学习最优策略，以最大化累积奖励。它的核心思想是试错学习，智能体通过尝试不同的行动，观察结果并获得奖励或惩罚，从而逐步改进自己的行为策略。">
  <meta property="og:locale" content="en_US">
  <meta property="og:type" content="article">
    <meta property="article:section" content="post">
    <meta property="article:published_time" content="2025-02-19T11:00:59-04:00">
    <meta property="article:modified_time" content="2025-02-19T11:00:59-04:00">
    <meta property="article:tag" content="RL">

  <meta itemprop="name" content="强化学习-数学基础">
  <meta itemprop="description" content="强化学习专注于如何让智能体（Agent）通过与环境的交互来学习最优策略，以最大化累积奖励。它的核心思想是试错学习，智能体通过尝试不同的行动，观察结果并获得奖励或惩罚，从而逐步改进自己的行为策略。">
  <meta itemprop="datePublished" content="2025-02-19T11:00:59-04:00">
  <meta itemprop="dateModified" content="2025-02-19T11:00:59-04:00">
  <meta itemprop="wordCount" content="381">
  <meta itemprop="keywords" content="RL">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="强化学习-数学基础">
  <meta name="twitter:description" content="强化学习专注于如何让智能体（Agent）通过与环境的交互来学习最优策略，以最大化累积奖励。它的核心思想是试错学习，智能体通过尝试不同的行动，观察结果并获得奖励或惩罚，从而逐步改进自己的行为策略。">

	
  </head><body class="ma0 avenir bg-near-white development">

    
   
  

  
  
  
  <header class="cover bg-center" style="background-image: url('http://localhost:1313/images/RL2/lucky.jpg');">
    <div class="bg-black-60">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        HomePage
      
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/about/" title="About ME page">
              About ME
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/post/" title="Articles page">
              Articles
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/contact/" title="Contact page">
              Contact
            </a>
          </li>
          
        </ul>
      
      <div class="ananke-socials"><a href="https://www.facebook.com/patrick.kollitsch" target="_blank" rel="noopener"
        class="ananke-social-link link-transition facebook link dib z-999 pt3 pt0-l mr1"
        title="follow on Facebook - Opens in a new window"
        aria-label="follow on Facebook - Opens in a new window">
      <span class="icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M512 256C512 114.6 397.4 0 256 0S0 114.6 0 256C0 376 82.7 476.8 194.2 504.5V334.2H141.4V256h52.8V222.3c0-87.1 39.4-127.5 125-127.5c16.2 0 44.2 3.2 55.7 6.4V172c-6-.6-16.5-1-29.6-1c-42 0-58.2 15.9-58.2 57.2V256h83.6l-14.4 78.2H287V510.1C413.8 494.8 512 386.9 512 256h0z"/></svg>
            
          </span></a><a href="https://bsky.app/profile/kollitsch.dev" target="_blank" rel="noopener"
        class="ananke-social-link link-transition bluesky link dib z-999 pt3 pt0-l mr1"
        title="follow on Bluesky - Opens in a new window"
        aria-label="follow on Bluesky - Opens in a new window">
      <span class="icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M407.8 294.7c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3zM288 227.1C261.9 176.4 190.9 81.9 124.9 35.3C61.6-9.4 37.5-1.7 21.6 5.5C3.3 13.8 0 41.9 0 58.4S9.1 194 15 213.9c19.5 65.7 89.1 87.9 153.2 80.7c3.3-.5 6.6-.9 10-1.4c-3.3 .5-6.6 1-10 1.4C74.3 308.6-9.1 342.8 100.3 464.5C220.6 589.1 265.1 437.8 288 361.1c22.9 76.7 49.2 222.5 185.6 103.4c102.4-103.4 28.1-156-65.8-169.9c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3c64.1 7.1 133.6-15.1 153.2-80.7C566.9 194 576 75 576 58.4s-3.3-44.7-21.6-52.9c-15.8-7.1-40-14.9-103.2 29.8C385.1 81.9 314.1 176.4 288 227.1z"/></svg>
            
          </span></a><a href="http://linkedin.com/in/patrickkollitsch" target="_blank" rel="noopener"
        class="ananke-social-link link-transition linkedin link dib z-999 pt3 pt0-l mr1"
        title="follow on LinkedIn - Opens in a new window"
        aria-label="follow on LinkedIn - Opens in a new window">
      <span class="icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
            
          </span></a></div>

    </div>
  </div>
</nav>

      <div class="tc-l pv6 ph3 ph4-ns">
        
          <div class="f2 f1-l fw2 white-90 mb0 lh-title">强化学习-数学基础</div>
          
            <div class="fw1 f5 f3-l white-80 measure-wide-l center lh-copy mt3 mb4">
              强化学习专注于如何让智能体（Agent）通过与环境的交互来学习最优策略，以最大化累积奖励。它的核心思想是试错学习，智能体通过尝试不同的行动，观察结果并获得奖励或惩罚，从而逐步改进自己的行为策略。
            </div>
          
        
      </div>
    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked ttu">
          
        Articles
      </aside><div id="sharing" class="mt3 ananke-socials"><a href="mailto:?&amp;body=http%3A%2F%2Flocalhost%3A1313%2Fpost%2F%25E5%25BC%25BA%25E5%258C%2596%25E5%25AD%25A6%25E4%25B9%25A0-%25E6%2595%25B0%25E5%25AD%25A6%25E5%259F%25BA%25E7%25A1%2580%2F&amp;subject=%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80"
        class="ananke-social-link email no-underline"
        title="Share on Email" aria-label="Share on Email"
        target="_blank" rel="nofollow noopener noreferrer">
        <span class="icon">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M64 112c-8.8 0-16 7.2-16 16l0 22.1L220.5 291.7c20.7 17 50.4 17 71.1 0L464 150.1l0-22.1c0-8.8-7.2-16-16-16L64 112zM48 212.2L48 384c0 8.8 7.2 16 16 16l384 0c8.8 0 16-7.2 16-16l0-171.8L322 328.8c-38.4 31.5-93.7 31.5-132 0L48 212.2zM0 128C0 92.7 28.7 64 64 64l384 0c35.3 0 64 28.7 64 64l0 256c0 35.3-28.7 64-64 64L64 448c-35.3 0-64-28.7-64-64L0 128z"/></svg>
                
              </span></a><a href="https://facebook.com/sharer/sharer.php?&amp;u=http%3A%2F%2Flocalhost%3A1313%2Fpost%2F%25E5%25BC%25BA%25E5%258C%2596%25E5%25AD%25A6%25E4%25B9%25A0-%25E6%2595%25B0%25E5%25AD%25A6%25E5%259F%25BA%25E7%25A1%2580%2F"
        class="ananke-social-link facebook no-underline"
        title="Share on Facebook" aria-label="Share on Facebook"
        target="_blank" rel="nofollow noopener noreferrer">
        <span class="icon">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M512 256C512 114.6 397.4 0 256 0S0 114.6 0 256C0 376 82.7 476.8 194.2 504.5V334.2H141.4V256h52.8V222.3c0-87.1 39.4-127.5 125-127.5c16.2 0 44.2 3.2 55.7 6.4V172c-6-.6-16.5-1-29.6-1c-42 0-58.2 15.9-58.2 57.2V256h83.6l-14.4 78.2H287V510.1C413.8 494.8 512 386.9 512 256h0z"/></svg>
                
              </span></a><a href="https://bsky.app/intent/compose?&amp;text=http%3A%2F%2Flocalhost%3A1313%2Fpost%2F%25E5%25BC%25BA%25E5%258C%2596%25E5%25AD%25A6%25E4%25B9%25A0-%25E6%2595%25B0%25E5%25AD%25A6%25E5%259F%25BA%25E7%25A1%2580%2F"
        class="ananke-social-link bluesky no-underline"
        title="Share on Bluesky" aria-label="Share on Bluesky"
        target="_blank" rel="nofollow noopener noreferrer">
        <span class="icon">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M407.8 294.7c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3zM288 227.1C261.9 176.4 190.9 81.9 124.9 35.3C61.6-9.4 37.5-1.7 21.6 5.5C3.3 13.8 0 41.9 0 58.4S9.1 194 15 213.9c19.5 65.7 89.1 87.9 153.2 80.7c3.3-.5 6.6-.9 10-1.4c-3.3 .5-6.6 1-10 1.4C74.3 308.6-9.1 342.8 100.3 464.5C220.6 589.1 265.1 437.8 288 361.1c22.9 76.7 49.2 222.5 185.6 103.4c102.4-103.4 28.1-156-65.8-169.9c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3c64.1 7.1 133.6-15.1 153.2-80.7C566.9 194 576 75 576 58.4s-3.3-44.7-21.6-52.9c-15.8-7.1-40-14.9-103.2 29.8C385.1 81.9 314.1 176.4 288 227.1z"/></svg>
                
              </span></a><a href="https://www.linkedin.com/shareArticle?&amp;mini=true&amp;source=http%3A%2F%2Flocalhost%3A1313%2Fpost%2F%25E5%25BC%25BA%25E5%258C%2596%25E5%25AD%25A6%25E4%25B9%25A0-%25E6%2595%25B0%25E5%25AD%25A6%25E5%259F%25BA%25E7%25A1%2580%2F&amp;summary=%E6%80%BB%E8%BF%B0&#43;%E5%9F%BA%E7%A1%80%E5%B7%A5%E5%85%B7&#43;%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%EF%BC%9Astate%2C&#43;action%2C&#43;reward%2C&#43;return%2C&#43;episode%2C&#43;policy%2C&#43;mdp%26amp%3Bhellip%3B&#43;%E8%B4%9D%E5%B0%94%E6%9B%BC%E5%85%AC%E5%BC%8F%EF%BC%9A%E7%94%A8%E4%BA%8E%E8%AF%84%E4%BB%B7%E7%AD%96%E7%95%A5&#43;%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%80%E4%BC%98%E5%85%AC%E5%BC%8F%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%9C%80%E7%BB%88%E7%9B%AE%E6%A0%87%E6%98%AF%E6%B1%82%E8%A7%A3%E6%9C%80%E4%BC%98%E7%AD%96%E7%95%A5&#43;%E7%AE%97%E6%B3%95%2F%E6%96%B9%E6%B3%95&#43;%E5%80%BC%E8%BF%AD%E4%BB%A3%E3%80%81%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3%E2%80%94%E2%80%94&#43;truncated&#43;policy&#43;iteration%EF%BC%9A%E5%80%BC%E5%92%8C%E7%AD%96%E7%95%A5update%E4%B8%8D%E6%96%AD%E8%BF%AD%E4%BB%A3&#43;Monte&#43;Carlo&#43;Learning%EF%BC%9A%E6%97%A0%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0&#43;%E9%9A%8F%E5%8D%B3%E8%BF%91%E4%BC%BC%E7%90%86%E8%AE%BA%EF%BC%9Afrom&#43;non-incremental&#43;to&#43;incremental&#43;%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E6%96%B9%E6%B3%95%28TD%29&#43;%E5%80%BC%E5%87%BD%E6%95%B0%E4%BC%B0%E8%AE%A1%EF%BC%9Atabular&#43;representation&#43;to&#43;function&#43;representation%EF%BC%8C%E5%BC%95%E5%85%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&#43;Policy&#43;Gradient&#43;Methods%EF%BC%9Afrom&#43;value-based&#43;to&#43;policy-based&#43;Actor-Critic&#43;Methods%EF%BC%9Apolicy-based&#43;%2B&#43;value-based&#43;&amp;title=%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80&amp;url=http%3A%2F%2Flocalhost%3A1313%2Fpost%2F%25E5%25BC%25BA%25E5%258C%2596%25E5%25AD%25A6%25E4%25B9%25A0-%25E6%2595%25B0%25E5%25AD%25A6%25E5%259F%25BA%25E7%25A1%2580%2F"
        class="ananke-social-link linkedin no-underline"
        title="Share on LinkedIn" aria-label="Share on LinkedIn"
        target="_blank" rel="nofollow noopener noreferrer">
        <span class="icon">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
                
              </span></a></div>
<h1 class="f1 athelas mt3 mb1">强化学习-数学基础</h1>
      
      
      
      <time class="f6 mv4 dib tracked" datetime="2025-02-19T11:00:59-04:00">February 19, 2025</time>
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><h2 id="总述">总述</h2>
<p><img src="/images/RL2/1.png" alt="1"></p>
<ul>
<li>
<h4 id="基础工具"><strong>基础工具</strong></h4>
<ol>
<li><strong>基本概念</strong>：state, action, reward, return, episode, policy, mdp&hellip;</li>
<li><strong>贝尔曼公式</strong>：用于评价策略</li>
<li><strong>贝尔曼最优公式</strong>：强化学习的最终目标是求解最优策略</li>
</ol>
</li>
<li>
<h4 id="算法方法">算法/方法</h4>
<ol start="4">
<li><strong>值迭代、策略迭代—— truncated policy iteration</strong>：值和策略update不断迭代</li>
<li><strong>Monte Carlo Learning</strong>：无模型学习</li>
<li><strong>随即近似理论</strong>：from non-incremental to incremental</li>
<li><strong>时序差分方法(TD)</strong></li>
<li><strong>值函数估计</strong>：tabular representation to function representation，引入神经网络</li>
<li><strong>Policy Gradient Methods</strong>：from value-based to policy-based</li>
<li><strong>Actor-Critic Methods</strong>：policy-based + value-based</li>
</ol>
</li>
</ul>
<p> </p>
<h2 id="1-基本概念">1 基本概念</h2>
<h4 id="1-专有名词">1. 专有名词</h4>
<ol>
<li>
<p><strong>grid-world</strong>：小机器人在网格里走路</p>
</li>
<li>
<p><strong>state</strong>：agent在环境中的状态，用s1、s2&hellip;表示；s是列向量，可表示速度、加速度等</p>
</li>
<li>
<p><strong>state space</strong>：即把所有的state放在一起的集合</p>
</li>
<li>
<p><strong>action</strong>：可采取的行动，如往上走、往右走&hellip;</p>
</li>
<li>
<p><strong>action-space</strong>：所有的action放在一起的集合，用A表示</p>
</li>
<li>
<p><strong>state transition</strong>：采取一个action后，从一个state转到另一个state的过程；定义了agent与环境的一种交互行为</p>
</li>
<li>
<p><strong>forbidden area</strong>：进去后受到惩罚/不可进入</p>
</li>
<li>
<p><strong>tabular representation</strong>：使用表格描述state transition</p>
</li>
<li>
<p><strong>state transition probability</strong>：使用条件概率来表述tate transition，用于描述随机性</p>
</li>
<li>
<p><strong>policy</strong>：使用箭头表示，告诉agent在某state时应该采取哪个action。基于policy，可以得到path</p>
</li>
<li>
<p><strong>mathematical representation</strong>：条件概率，用π表示某状态对应的策略</p>
</li>
<li>
<p><strong>stochastic policies</strong>：某状态对应多个不同概率的action</p>
</li>
<li>
<p><strong>reward</strong>：一个数(标量)；正奖励负惩罚</p>
<p><img src="/images/RL2/2.png" alt="2"></p>
<p>可视为 <em>human-machine interface</em>，即人类与机器交互的一种手段，引导机器该怎么做。</p>
<p>同时也可以用 tabular representation 来表示reward，但只能表示唯一的reward；</p>
<p>还可以使用 mathematical representation，用条件概率来表示。</p>
</li>
<li>
<p><strong>trajectory</strong>：一个 state-action-reward 链</p>
</li>
<li>
<p><strong>return</strong>：沿着 trajectory 所得到的 reward 总和，用于评估策略</p>
</li>
<li>
<p><strong>discounted rate</strong>、<strong>discounted return</strong>：</p>
<p><img src="/images/RL2/3.png" alt="3"></p>
<p>gamma 较小，比较近视，更加注重最近的一些 reward；反之 gamma 较大，比较远视。</p>
</li>
<li>
<p><strong>terminal states</strong>：终止状态</p>
</li>
<li>
<p><strong>episode/trail</strong>：通常被定义为一个会终止的 trajectory，这些任务被称为 episodic tasks</p>
</li>
<li>
<p><strong>continuing task</strong>：有些任务是不会结束，永远持续的/或时间比较长</p>
<ul>
<li>统一方法：把 episodic tasks 转为continuing task
<ol>
<li>把 target state 视为 absorbing state，不论采取什么 action 都会再回到这个状态，并且 reward 为0；</li>
<li>将 target state 视为一个普通的状态，可离开可留下。</li>
</ol>
</li>
</ul>
</li>
</ol>
<p> </p>
<h4 id="2-马尔可夫决策过程-mdp-markov-decision-process">2. 马尔可夫决策过程 (MDP markov decision process)</h4>
<ul>
<li>
<p>MDP 的关键组成：</p>
<p><img src="/images/RL2/4.png" alt="4"></p>
</li>
<li>
<p>用圆和边来表示 markov process：</p>
<p><img src="/images/RL2/5.png" alt="5"></p>
</li>
</ul>
<p>当 policy 确定后，markov process 就成了 markov decision process</p>
<p> </p>
<p> </p>
<h2 id="2-贝尔曼公式">2 贝尔曼公式</h2>
<p>核心概念 state value、基础工具 bellman equation</p>
<h4 id="1-计算return">1. 计算return</h4>
<ol>
<li>
<p><strong>方法一：用定义</strong></p>
<p><img src="/images/RL2/6.png" alt="6"></p>
</li>
<li>
<p><strong>方法二</strong>：Bootstrapping!</p>
<p><img src="/images/RL2/7.png" alt="7"></p>
<p>从某状态出发的return，依赖于从其他状态出发的 return</p>
<p>简易贝尔曼公式：<img src="/images/RL2/8.png" alt="8"></p>
</li>
</ol>
<p> </p>
<h4 id="2-state-value">2. state value</h4>
<ul>
<li>
<p>对于单步过程：</p>
<p><img src="/images/RL2/9.png" alt="9"></p>
</li>
</ul>
<p>state value 就是 Gt(discounted return) 的<strong>期望值</strong></p>
<p> </p>
<h4 id="3-贝尔曼公式">3. 贝尔曼公式</h4>
<p>描述了不同状态的 state value 之间的关系，可用于计算 state value</p>
<ol>
<li>
<p><strong>推导</strong></p>
<p><img src="/images/RL2/10.png" alt="10"></p>
<blockquote>
<p>[!IMPORTANT]</p>
<ol>
<li>如上所示，即为 Bellman equation，表示了不同状态的 state-value functions（左边是s的，右边是s&rsquo;的）</li>
<li>有两项组成：当前 reward 和未来的 reward</li>
<li>该式子对所有的状态都成立！！！（若有 n 个状态，则会有 n 个这样的式子）</li>
<li>通过这 n 个式子，联立可求出 state value —— Bootstrapping!</li>
<li>π 是给定的 policy，解决这个问题的过程就是 policy evaluation</li>
<li>p 代表 dynamic model，可能已知或未知</li>
</ol>
</blockquote>
<ul>
<li>
<p>以下图 s1 为例：</p>
<p><img src="/images/RL2/11.png" alt="11"></p>
<p>联立所有的式子，并代入具体的gamma，即可求出所有的 state value；state value 越高，代表策略越好</p>
</li>
</ul>
</li>
<li>
<p><strong>Matrix-vector form</strong></p>
<p>将所有状态的贝尔曼公式放在一起，并写成矩阵形式：</p>
<p><img src="/images/RL2/12.png" alt="12"></p>
<ul>
<li>
<p>例如：</p>
<p><img src="/images/RL2/13.png" alt="13"></p>
</li>
</ul>
</li>
<li>
<p><strong>求解 state value</strong></p>
<ol>
<li>
<p>为什么要求解 state value？</p>
<p>给定 policy，求解 state value 的过程叫 <strong>policy evaluation</strong>，即评估策略好不好，它是 RL 的基础问题。</p>
</li>
<li>
<p>解析方法</p>
<ol>
<li>
<p><strong>closed-form solution</strong></p>
<p><img src="/images/RL2/14.png" alt="14"></p>
<p>逆难求，实际很少用</p>
</li>
<li>
<p><strong>iterative solution</strong></p>
<p><img src="/images/RL2/15.png" alt="15"></p>
<p><img src="/images/RL2/16.png" alt="16"></p>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<p> </p>
<h4 id="4-action-value">4. action value</h4>
<p><img src="/images/RL2/17.png" alt="17"></p>
<ul>
<li>
<p>区别：</p>
<ul>
<li><strong>state value</strong>：从一个状态出发，可以得到的平均 return</li>
<li><strong>action value</strong>：从一个状态出发，并且选择了一个 action 后得到的平均 return</li>
</ul>
<p><img src="/images/RL2/18.png" alt="18"></p>
<p>求 action value 的平均，可得 state value ；同理已知 state value 可求 action value</p>
</li>
<li>
<p>例子：</p>
<p><img src="/images/RL2/19.png" alt="19"></p>
<p>虽然往右走，但所有的 action 都是可以计算的，<strong>非零</strong>！未来可变，详见后面的课程。</p>
</li>
</ul>
<p>最后选择 action value 最大的那个 policy。</p>
<p> </p>
<h4 id="5-小结">5. 小结</h4>
<p><img src="/images/RL2/20.png" alt="20"></p>
<p> </p>
<p> </p>
<p> </p>
<h2 id="3-最优策略-与-贝尔曼最优公式">3 最优策略 与 贝尔曼最优公式</h2>
<p>贝尔曼最优公式是贝尔曼公式的一种特殊情况 —— 强化学习的目的就是寻找最优策略</p>
<ul>
<li>
<p>重点关注：</p>
<ul>
<li>核心概念：optimal state value 和 optimal policy</li>
<li>基础工具：the Bellman optimality equation (BOE)</li>
</ul>
</li>
<li>
<p>例子</p>
<ol>
<li>
<p>求解 state value</p>
<p><img src="/images/RL2/21.png" alt="21"></p>
</li>
<li>
<p>求解 action value</p>
<p><img src="/images/RL2/22.png" alt="22"></p>
<p>注：1、2、3、4、5 分别代表上右下左原地</p>
</li>
<li>
<p>如何改进策略？—— 使用 action value</p>
<p><img src="/images/RL2/23.png" alt="23"></p>
<p>观察到 a3 时，action value最大；若选择 a3 作为策略：</p>
<p><img src="/images/RL2/24.png" alt="24"></p>
</li>
</ol>
<p>直观上来说，选择最大的 action value 可以得到最优的策略。</p>
</li>
</ul>
<p> </p>
<h4 id="1-最优策略optimal-policy-定义">1. 最优策略(optimal policy) 定义</h4>
<p><strong>定义：某策略相比于其他的策略，能得到最大的 state value</strong></p>
<ul>
<li>贝尔曼公式回答了如下问题：
<ul>
<li>是否存在？</li>
<li>是否唯一？</li>
<li>确定性or非确定性？</li>
<li>如何得到？</li>
</ul>
</li>
</ul>
<p> </p>
<h4 id="2-贝尔曼最优公式boe">2. 贝尔曼最优公式(BOE)</h4>
<p><img src="/images/RL2/25.png" alt="25"></p>
<ul>
<li>
<p>可向量化：</p>
<p><img src="/images/RL2/26.png" alt="26"></p>
</li>
</ul>
<ol>
<li>
<p><strong>如何理解左边 maxπ</strong></p>
<p><img src="/images/RL2/27.png" alt="27"></p>
<p>化成了 v = f(v):</p>
<p><img src="/images/RL2/28.png" alt="28"></p>
</li>
<li>
<p><strong>压缩映射定理（Contraction Mapping Theotrm）</strong></p>
<ul>
<li>
<p><strong>不动点(fixed point)</strong>：集合上的一个点x，在函数f上满足f(x)=x，则该点称为不动点 —— 通过f又映射到了自己。</p>
</li>
<li>
<p><strong>Contraction Mapping</strong>：若f满足不等式</p>
<p><img src="/images/RL2/29.png" alt="29"></p>
</li>
<li>
<p><strong>Contraction Mapping Theotrm</strong>:</p>
<p><img src="/images/RL2/30.png" alt="30"></p>
</li>
</ul>
</li>
<li>
<p><strong>利用 Contraction Mapping Theotrm 求解 BOE</strong></p>
<p>v=f(v)满足 Contraction Property，可以使用定理解出。</p>
<p>贝尔曼最优公式的解，必是最优的 state value，相对应的policy也是最优的。</p>
<p><img src="/images/RL2/31.png" alt="31"></p>
</li>
</ol>
<p> </p>
<h4 id="3-分析最优策略">3. 分析最优策略</h4>
<ol>
<li>
<p>什么因素决定了它的最优策略：下式中红色元素</p>
<p><img src="/images/RL2/32.png" alt="32"></p>
<ul>
<li>gamma 大，更加远视</li>
<li>gamma 小，更加近视</li>
<li>gamma 为0，选择 immediate reward</li>
</ul>
</li>
<li>
<p>将 r 改变为 ar+b 后，v的变化：</p>
<p><img src="/images/RL2/33.png" alt="33"></p>
</li>
<li>
<p>除了 r，gamma 也可以约束智能体不要绕远路</p>
<p><img src="/images/RL2/34.png" alt="34"></p>
<p>绕远路意味着到达目标的奖励晚，对应的 gamma 次方大，打折厉害。</p>
</li>
</ol>
<p> </p>
<h4 id="4-总结">4. 总结</h4>
<p><img src="/images/RL2/35.png" alt="35"></p>
<p><img src="/images/RL2/36.png" alt="36"></p>
<p> </p>
<p> </p><ul class="pa0">
  
   <li class="list di">
     <a href="/tags/rl/" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">RL</a>
   </li>
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




  <div class="bg-light-gray pa3 nested-list-reset nested-copy-line-height nested-links">
    <p class="f5 b mb3">Related</p>
    <ul class="pa0 list">
	   
	     <li  class="mb2">
          <a href="/post/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3/">强化学习-直观理解</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/post/e2e/">End2End</a>
        </li>
	    
    </ul>
</div>

</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://localhost:1313/" >
    &copy;  HomePage 2025 
  </a>
    <div><div class="ananke-socials"><a href="https://www.facebook.com/patrick.kollitsch" target="_blank" rel="noopener"
        class="ananke-social-link link-transition facebook link dib z-999 pt3 pt0-l mr1"
        title="follow on Facebook - Opens in a new window"
        aria-label="follow on Facebook - Opens in a new window">
      <span class="icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M512 256C512 114.6 397.4 0 256 0S0 114.6 0 256C0 376 82.7 476.8 194.2 504.5V334.2H141.4V256h52.8V222.3c0-87.1 39.4-127.5 125-127.5c16.2 0 44.2 3.2 55.7 6.4V172c-6-.6-16.5-1-29.6-1c-42 0-58.2 15.9-58.2 57.2V256h83.6l-14.4 78.2H287V510.1C413.8 494.8 512 386.9 512 256h0z"/></svg>
            
          </span></a><a href="https://bsky.app/profile/kollitsch.dev" target="_blank" rel="noopener"
        class="ananke-social-link link-transition bluesky link dib z-999 pt3 pt0-l mr1"
        title="follow on Bluesky - Opens in a new window"
        aria-label="follow on Bluesky - Opens in a new window">
      <span class="icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M407.8 294.7c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3zM288 227.1C261.9 176.4 190.9 81.9 124.9 35.3C61.6-9.4 37.5-1.7 21.6 5.5C3.3 13.8 0 41.9 0 58.4S9.1 194 15 213.9c19.5 65.7 89.1 87.9 153.2 80.7c3.3-.5 6.6-.9 10-1.4c-3.3 .5-6.6 1-10 1.4C74.3 308.6-9.1 342.8 100.3 464.5C220.6 589.1 265.1 437.8 288 361.1c22.9 76.7 49.2 222.5 185.6 103.4c102.4-103.4 28.1-156-65.8-169.9c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3c64.1 7.1 133.6-15.1 153.2-80.7C566.9 194 576 75 576 58.4s-3.3-44.7-21.6-52.9c-15.8-7.1-40-14.9-103.2 29.8C385.1 81.9 314.1 176.4 288 227.1z"/></svg>
            
          </span></a><a href="http://linkedin.com/in/patrickkollitsch" target="_blank" rel="noopener"
        class="ananke-social-link link-transition linkedin link dib z-999 pt3 pt0-l mr1"
        title="follow on LinkedIn - Opens in a new window"
        aria-label="follow on LinkedIn - Opens in a new window">
      <span class="icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
            
          </span></a></div>
</div>
  </div>
</footer>

  </body>
</html>
