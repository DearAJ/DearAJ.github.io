<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AIInfra on HomePage</title>
    <link>http://localhost:1313/tags/aiinfra/</link>
    <description>Recent content in AIInfra on HomePage</description>
    <generator>Hugo</generator>
    <language>en-US</language>
    <lastBuildDate>Fri, 11 Jul 2025 11:00:59 -0400</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/aiinfra/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>分布式并行训练</title>
      <link>http://localhost:1313/post/%E5%88%86%E5%B8%83%E5%BC%8F%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83/</link>
      <pubDate>Fri, 11 Jul 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/%E5%88%86%E5%B8%83%E5%BC%8F%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83/</guid>
      <description>&lt;p&gt;分布式训练将训练工作负载分散到多个工作节点，因此可以显著提高训练速度和模型准确性。&lt;/p&gt;&#xA;&lt;h2 id=&#34;ddpdistributed-data-parallel&#34;&gt;DDP（Distributed Data Parallel）&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;为什么用 Distributed Training？&lt;/strong&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;节约时间&lt;/li&gt;&#xA;&lt;li&gt;增加计算量&lt;/li&gt;&#xA;&lt;li&gt;模型更快&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;如何实现？&lt;/strong&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;在同一机器上使用多个 GPUs&lt;/li&gt;&#xA;&lt;li&gt;在集群上使用多个机器&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;什么是ddp&#34;&gt;什么是DDP？&lt;/h3&gt;&#xA;&lt;p&gt;DDP：在训练过程中内部保持同步。每个 GPU 进程都有相同模型，仅数据不同。&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;DistributedSampler&lt;/strong&gt;：确保每个设备获得不重叠的输入批次，从而处理 n  倍数据。&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/Users/aijunyang/DearAJ.github.io/static/images/DPT/4.png&#34; alt=&#34;4&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;模型在所有设备上复制。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;模型接受不同输入的数据后，在本地运行&lt;strong&gt;前向传播和后向传播&lt;/strong&gt;。&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/Users/aijunyang/DearAJ.github.io/static/images/DPT/1.png&#34; alt=&#34;1&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;每个副本模型累计的梯度不同，&lt;strong&gt;DDP&lt;/strong&gt; 启动&lt;strong&gt;同步&lt;/strong&gt;：使用&lt;a href=&#34;https://tech.preferred.jp/en/blog/technologies-behind-distributed-deep-learning-allreduce/&#34;&gt;环状 AllReduce 算法&lt;/a&gt;聚合所有副本的梯度，将梯度与通信重叠&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/Users/aijunyang/DearAJ.github.io/static/images/DPT/2.png&#34; alt=&#34;截屏2025-07-12 14.45.22&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;同步&lt;/strong&gt; 不会等待所有的梯度计算完成，它在反向传播进行的同时沿环进行通信，确保 GPU 不会空闲&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;运行&lt;strong&gt;优化器&lt;/strong&gt;，将所有副本模型的参数更新为相同的值&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/Users/aijunyang/DearAJ.github.io/static/images/DPT/3.png&#34; alt=&#34;3&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;对比 &lt;code&gt;DataParallel&lt;/code&gt; (DP)：DP 非常简单（只需额外一行代码），但性能要差得多。&lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;单节点多-gpu-训练&#34;&gt;单节点多 GPU 训练&lt;/h3&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;容错分布式训练&#34;&gt;容错分布式训练&lt;/h3&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;fsdp&#34;&gt;FSDP&lt;/h2&gt;&#xA;&lt;h2 id=&#34;tp&#34;&gt;TP&lt;/h2&gt;&#xA;&lt;h2 id=&#34;rpc&#34;&gt;RPC&lt;/h2&gt;</description>
    </item>
  </channel>
</rss>
