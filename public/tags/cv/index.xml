<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>CV on HomePage</title>
    <link>http://localhost:1313/tags/cv/</link>
    <description>Recent content in CV on HomePage</description>
    <generator>Hugo</generator>
    <language>en-US</language>
    <lastBuildDate>Mon, 17 Feb 2025 11:00:59 -0400</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/cv/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>BEV 论文学习</title>
      <link>http://localhost:1313/post/bev/</link>
      <pubDate>Mon, 17 Feb 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/bev/</guid>
      <description>&lt;h2 id=&#34;vision-centric-bev-perception-a-survey&#34;&gt;Vision-Centric BEV Perception: A Survey&lt;/h2&gt;&#xA;&lt;p&gt;BEV感知在自动驾驶领域具有重要应用，因为它能够提供直观的环境表示，并且便于多模态数据的融合。随着深度学习的快速发展，许多方法被提出以解决从透视视图（Perspective View, PV）到BEV的转换问题。&lt;/p&gt;&#xA;&lt;p&gt;本文系统地回顾了最新的BEV感知方法，并将其分为基于几何、基于深度、基于MLP和基于Transformer的四类方法，并详细分析了它们的优缺点。此外，本文还探讨了BEV感知的扩展应用，如多任务学习、多模态融合和语义占据预测等。&lt;/p&gt;&#xA;&lt;h4 id=&#34;1-背景介绍&#34;&gt;1. 背景介绍&lt;/h4&gt;&#xA;&lt;p&gt;BEV感知的核心任务是将透视视图（PV）中的图像序列转换为BEV特征，并在BEV空间中进行感知任务，如3D目标检测和语义地图生成。BEV表示的优势在于它能够提供精确的定位和绝对尺度信息，便于多视图、多模态和时间序列数据的融合。然而，由于摄像头通常安装在车辆上，捕捉到的图像是透视视图，如何将PV转换为BEV仍然是一个具有挑战性的问题。&lt;/p&gt;&#xA;&lt;h4 id=&#34;3-主要方法分类&#34;&gt;3. 主要方法分类&lt;/h4&gt;&#xA;&lt;p&gt;本文根据PV到BEV的转换方法，将现有方法分为四类：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;基于几何的方法&lt;/strong&gt;：这类方法主要依赖于逆透视映射（Inverse Perspective Mapping, IPM），通过几何变换将PV图像转换为BEV图像。IPM假设地面是平坦的，因此在复杂场景中（如存在高度变化的物体）会产生失真。为了减少失真，一些方法引入了语义信息或使用生成对抗网络（GAN）来增强BEV图像的质量。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;基于深度的方法&lt;/strong&gt;：这类方法通过深度估计将2D特征提升到3D空间，然后通过降维得到BEV表示。深度估计可以是显式的（如通过深度图）或隐式的（如通过任务监督）。基于深度的方法可以进一步分为点云方法和体素方法。点云方法将深度图转换为伪LiDAR点云，然后使用LiDAR检测器进行3D检测；体素方法则将2D特征映射到3D体素空间，并通过体素特征进行BEV感知。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;基于MLP的方法&lt;/strong&gt;：这类方法使用多层感知器（MLP）作为通用的映射函数，将PV特征映射到BEV空间。MLP方法不依赖于摄像头的几何参数，而是通过学习隐式表示来完成视图转换。尽管MLP具有通用逼近能力，但由于缺乏深度信息和遮挡问题，视图转换仍然具有挑战性。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;基于Transformer的方法&lt;/strong&gt;：这类方法利用Transformer的交叉注意力机制，直接构建BEV查询并在透视图像中搜索对应的特征。Transformer方法具有强大的关系建模能力和数据依赖性，能够处理复杂的多视图输入。根据查询的粒度，Transformer方法可以分为稀疏查询、密集查询和混合查询三类。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h4 id=&#34;4-扩展应用&#34;&gt;4. 扩展应用&lt;/h4&gt;&#xA;&lt;p&gt;本文还探讨了BEV感知的扩展应用，包括：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;多任务学习&lt;/strong&gt;：BEV表示可以同时支持多个下游任务，如3D目标检测、语义地图分割和运动预测。一些方法提出了统一的框架，通过共享的BEV特征同时完成多个任务。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;多模态融合&lt;/strong&gt;：BEV表示为多模态数据（如摄像头、LiDAR和雷达）的融合提供了便利。通过将不同模态的数据转换到BEV空间，可以在物理对应关系的基础上进行特征融合。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;语义占据预测&lt;/strong&gt;：基于BEV感知，语义占据预测任务旨在为3D空间中的每个占据区域分配语义标签。这类任务能够提供更细粒度的几何和语义信息，适用于复杂的自动驾驶场景。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h4 id=&#34;5-实验与结果&#34;&gt;5. 实验与结果&lt;/h4&gt;&#xA;&lt;p&gt;本文对各类方法在KITTI、nuScenes和Waymo等数据集上的性能进行了详细比较。实验结果表明，基于Transformer的方法在3D目标检测和语义分割任务中表现优异，尤其是在处理多视图输入时。此外，引入时间信息的模型（如BEVDet4D和BEVFormer）在速度和方向预测方面也有显著提升。&lt;/p&gt;&#xA;&lt;h4 id=&#34;6-总结与未来方向&#34;&gt;6. 总结与未来方向&lt;/h4&gt;&#xA;&lt;p&gt;本文系统地回顾了视觉为中心的BEV感知方法，并对其进行了分类和比较。未来的研究方向包括：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;进一步提高BEV感知的分辨率和计算效率。&lt;/li&gt;&#xA;&lt;li&gt;探索更高效的多模态融合方法。&lt;/li&gt;&#xA;&lt;li&gt;利用长时历史信息进行更精确的时间融合。&lt;/li&gt;&#xA;&lt;li&gt;开发更强大的语义占据预测模型。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;7-个人思考&#34;&gt;7. 个人思考&lt;/h4&gt;&#xA;&lt;p&gt;本文对BEV感知领域的现有方法进行了全面的总结，为未来的研究提供了清晰的方向。特别是基于Transformer的方法在复杂场景中的表现令人印象深刻，但其计算复杂度较高，如何在实际应用中实现高效部署仍然是一个挑战。此外，多模态融合和语义占据预测是未来自动驾驶感知系统的重要组成部分，值得进一步深入研究。&lt;/p&gt;&#xA;&lt;h4 id=&#34;参考文献&#34;&gt;参考文献&lt;/h4&gt;&#xA;&lt;p&gt;本文引用了大量相关文献，涵盖了从经典的几何方法到最新的深度学习和Transformer方法。这些文献为读者提供了进一步研究的参考。&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2409.17659&#34;&gt;Hierarchical End-to-End Autonomous Driving: Integrating BEV Perception with Deep Reinforcement Learning&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://blog.csdn.net/soaring_casia/article/details/142756723&#34;&gt;https://blog.csdn.net/soaring_casia/article/details/142756723&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>MLLM</title>
      <link>http://localhost:1313/post/mllm/</link>
      <pubDate>Sun, 09 Feb 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/mllm/</guid>
      <description>&lt;h1 id=&#34;1基础&#34;&gt;1基础&lt;/h1&gt;&#xA;&lt;h2 id=&#34;1-特征提取&#34;&gt;1. 特征提取&lt;/h2&gt;&#xA;&lt;h3 id=&#34;一cv中的特征提取&#34;&gt;&lt;strong&gt;一、CV中的特征提取&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;h4 id=&#34;1-传统方法手工设计特征&#34;&gt;&lt;strong&gt;1. 传统方法（手工设计特征）&lt;/strong&gt;&lt;/h4&gt;&#xA;&lt;p&gt;&lt;strong&gt;(1) 低级视觉特征&lt;/strong&gt;：颜色、纹理、 边缘与形状&amp;hellip;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;(2) 中级语义特征&lt;/strong&gt;：SIFT（尺度不变特征变换）、SURF（加速鲁棒特征）、LBP（局部二值模式）&amp;hellip;&lt;/p&gt;&#xA;&lt;h4 id=&#34;2-深度学习方法自动学习特征&#34;&gt;&lt;strong&gt;2. 深度学习方法（自动学习特征）&lt;/strong&gt;&lt;/h4&gt;&#xA;&lt;h5 id=&#34;1-卷积神经网络cnn&#34;&gt;&lt;strong&gt;(1) 卷积神经网络（CNN）&lt;/strong&gt;&lt;/h5&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;核心思想&lt;/strong&gt;：通过卷积层提取局部特征，池化层降低维度，全连接层进行分类。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;经典模型&lt;/strong&gt;：LeNet-5、AlexNet、VGGNet、ResNet(使用残差可以训练更深的网络)&amp;hellip;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/MLLM/1.png&#34; alt=&#34;1&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h5 id=&#34;2-视觉transformervit&#34;&gt;&lt;strong&gt;(2) 视觉Transformer（ViT）&lt;/strong&gt;&lt;/h5&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;核心思想&lt;/strong&gt;：将图像分割为小块（patches），通过自注意力机制建模全局关系。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;优势&lt;/strong&gt;：无需局部卷积先验，直接建模长距离依赖; 在ImageNet等任务上超越传统CNN。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;二nlp中的特征提取&#34;&gt;&lt;strong&gt;二、NLP中的特征提取&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;h4 id=&#34;1-传统方法基于统计与规则&#34;&gt;&lt;strong&gt;1. 传统方法（基于统计与规则）&lt;/strong&gt;&lt;/h4&gt;&#xA;&lt;p&gt;&lt;strong&gt;(1) 词袋模型（Bag of Words, BoW）&lt;/strong&gt;: 将文本表示为词汇表中单词的出现频率。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;(2) TF-IDF（词频-逆文档频率）&lt;/strong&gt;: 衡量单词在文档中的重要性（TF-IDF值 = 词频 × 逆文档频率）。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;(3) N-gram模型&lt;/strong&gt;: 统计连续N个词的组合频率（如Bi-gram、Tri-gram）。&lt;/p&gt;&#xA;&lt;h5 id=&#34;4-词嵌入预训练词向量&#34;&gt;&lt;strong&gt;(4) 词嵌入（预训练词向量）&lt;/strong&gt;&lt;/h5&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Word2Vec&lt;/strong&gt;（2013）：&#xA;&lt;ul&gt;&#xA;&lt;li&gt;通过Skip-Gram或CBOW模型，将词映射为低维稠密向量。&lt;/li&gt;&#xA;&lt;li&gt;相似词在向量空间中距离相近（如“国王-王后≈男人-女人”）。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;GloVe&lt;/strong&gt;（2014）：&#xA;&lt;ul&gt;&#xA;&lt;li&gt;基于全局词共现矩阵，结合统计信息和词向量学习。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;strong&gt;(5) 局限性&lt;/strong&gt;：无法建模长距离上下文依赖; 词向量静态，无法处理一词多义。&lt;/p&gt;</description>
    </item>
    <item>
      <title>目标检测</title>
      <link>http://localhost:1313/post/object-detection/</link>
      <pubDate>Fri, 31 Jan 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/object-detection/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/CV/19.png&#34; alt=&#34;19&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;技巧&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Ensembling&lt;/strong&gt;：Train several networks independently and average their outputs &lt;strong&gt;Multi-crop at test time&lt;/strong&gt;：Run classifier on multiple versions of test images and average results&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h3 id=&#34;定位&#34;&gt;定位&lt;/h3&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/CV/20.png&#34; alt=&#34;20&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;Need to output bx, by, bn, bw, class label (1-4)&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/CV/21.png&#34; alt=&#34;21&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;需人工标注&lt;strong&gt;特征点的坐标&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h3 id=&#34;基于滑动窗口的目标检测算法&#34;&gt;基于滑动窗口的目标检测算法&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;先训练卷积网络识别物体&lt;/li&gt;&#xA;&lt;li&gt;滑动+放大窗口+再次滑动&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;&lt;strong&gt;问题&lt;/strong&gt;：计算效率大，慢&lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;</description>
    </item>
  </channel>
</rss>
