<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Inference on HomePage</title>
    <link>http://localhost:1313/tags/inference/</link>
    <description>Recent content in Inference on HomePage</description>
    <generator>Hugo</generator>
    <language>en-US</language>
    <lastBuildDate>Sat, 19 Jul 2025 11:00:59 -0400</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/inference/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>SGLang</title>
      <link>http://localhost:1313/post/sglang/</link>
      <pubDate>Sat, 19 Jul 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/sglang/</guid>
      <description>&lt;p&gt;优势：&lt;strong&gt;后端运行快速&lt;/strong&gt;、&lt;strong&gt;前端语言灵活&lt;/strong&gt;、&lt;strong&gt;模型支持广泛&lt;/strong&gt;、&lt;strong&gt;社区活跃&lt;/strong&gt;。&lt;/p&gt;&#xA;&lt;p&gt;SGLang 的核心是通过其 Python API 构建和执行 Language Model Program。&lt;/p&gt;&#xA;&lt;h4 id=&#34;启动服务器&#34;&gt;&lt;strong&gt;启动服务器&lt;/strong&gt;&lt;/h4&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; sglang &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; sgl&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 配置 SGLang 运行时&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 如果在本地运行服务，这里指定服务地址和端口&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 如果直接在 Python 进程中加载模型 (需要安装 sglang[srt])，可以使用 sgl.init(&amp;#34;model_path&amp;#34;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 假设此时服务已在本地 30000 端口启动，并加载了模型：&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sgl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;init(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;http://127.0.0.1:30000&amp;#34;&lt;/span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 或者，如果在 Python 进程中直接加载模型（需要足够的显存）&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# sgl.init(&amp;#34;meta-llama/Llama-3.1-8B-Instruct&amp;#34;) # 使用 Hugging Face ID&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 或者&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# sgl.init(&amp;#34;/path/to/your/model_dir&amp;#34;) # 使用本地模型路径&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;h4 id=&#34;定义和运行一个简单的生成任务&#34;&gt;定义和运行一个简单的生成任务&lt;/h4&gt;&#xA;&lt;p&gt;&lt;code&gt;sgl.Runtime()&lt;/code&gt; 是 LM 程序的入口。&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; sglang &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; sgl&#x9;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;# 假设已经通过 sgl.init(...) 初始化了运行时&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 定义一个 LM Program&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;@sgl.function&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;simple_gen&lt;/span&gt;(s, query):&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    s &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;用户问：&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;query&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# 使用 sgl.gen() 进行文本生成&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    s &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;回答：&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; sgl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;gen(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;answer&amp;#34;&lt;/span&gt;, max_tokens&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt;)&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;# 使用 sgl.gen() 进行文本生成&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>vLLM</title>
      <link>http://localhost:1313/post/vllm/</link>
      <pubDate>Sat, 19 Jul 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/vllm/</guid>
      <description>&lt;p&gt;vLLM 是一个专门用于高效运行大语言模型的 Python 库。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;KV Cache&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;大模型推理时 huggingface 按照可生成最长序列长度分配显存。但这造成三种类型的浪费：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;﻿﻿预分配最大的 token 数，但不会用到。&lt;/li&gt;&#xA;&lt;li&gt;﻿﻿剩下的 token 还尚未用到，但现存已被预分配占用。&lt;/li&gt;&#xA;&lt;li&gt;﻿﻿显存之间的间隔碎片，因为 prompt 之间不同，显存不足以预分配给下一个文本生成。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/vllm/1.png&#34; alt=&#34;1&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;新 token 只用到前面 token 的 kv 向量，事实上只需保存之前的 kv 向量。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h4 id=&#34;page-attention&#34;&gt;&lt;strong&gt;Page Attention&lt;/strong&gt;&lt;/h4&gt;&#xA;&lt;p&gt;借鉴 OS 中的虚拟内存和页管理技术，把显存划分为 KVBlock，显存按照 KVBlock 来管理 KVCache，不用提前分配。&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/vllm/2.png&#34; alt=&#34;2&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
  </channel>
</rss>
