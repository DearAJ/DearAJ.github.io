<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LLM on HomePage</title>
    <link>http://localhost:1313/tags/llm/</link>
    <description>Recent content in LLM on HomePage</description>
    <generator>Hugo</generator>
    <language>en-US</language>
    <lastBuildDate>Mon, 17 Feb 2025 11:00:59 -0400</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/llm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>BEV 论文学习</title>
      <link>http://localhost:1313/post/bev%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/</link>
      <pubDate>Mon, 17 Feb 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/bev%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/</guid>
      <description>&lt;h2 id=&#34;vision-centric-bev-perception-a-survey&#34;&gt;Vision-Centric BEV Perception: A Survey&lt;/h2&gt;&#xA;&lt;p&gt;许多方法被提出以&lt;strong&gt;解决从透视视图（Perspective View, PV）到 BEV 的转换问题&lt;/strong&gt;，本文将它们分为基于几何、基于深度、基于 MLP 和基于 Transformer 的四类方法。&lt;/p&gt;&#xA;&lt;p&gt;此外，本文还探讨了 BEV 感知的扩展应用，如多任务学习、多模态融合和语义占据预测等。&lt;/p&gt;&#xA;&lt;h4 id=&#34;1-背景介绍&#34;&gt;1. 背景介绍&lt;/h4&gt;&#xA;&lt;p&gt;BEV 感知的核心任务是将 PV 中的&lt;strong&gt;图像序列转换为BEV特征，并在BEV空间中进行感知任务&lt;/strong&gt;（如3D目标检测和语义地图生成），能够提供精确的定位和绝对尺度信息，便于多视图、多模态和时间序列数据的融合。&lt;/p&gt;&#xA;&lt;p&gt;但由于摄像头通常安装在车辆上，捕捉到的图像是透视视图，如何将 PV 转换为 BEV 仍然是一个具有挑战性的问题。&lt;/p&gt;&#xA;&lt;h4 id=&#34;3-主要方法分类&#34;&gt;3. 主要方法分类&lt;/h4&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;基于几何的方法&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;优势&lt;/strong&gt;：这类方法主要依赖于&lt;strong&gt;逆透视映射（IPM）&lt;/strong&gt;，通过几何变换将 PV 图像转换为 BEV 图像。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;缺陷&lt;/strong&gt;：但 IPM 假设地面是平坦的，因此在复杂场景中（如存在高度变化的物体）会产生失真。为了减少失真，一些方法引入了&lt;strong&gt;语义信息&lt;/strong&gt;或使用 &lt;strong&gt;GAN&lt;/strong&gt; 。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;基于深度的方法&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;通过深度估计将 2D 特征提升到 3D 空间，然后通过降维得到 BEV 表示。深度估计可以是显式的（如通过深度图）或隐式的（如通过任务监督）。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;点云方法&lt;/strong&gt;：将深度图转换为伪 LiDAR 点云，然后使用 LiDAR 检测器进行 3D 检测&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;体素方法&lt;/strong&gt;：将 2D 特征映射到 3D 体素空间，并通过体素特征进行 BEV 感知&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;基于MLP的方法&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;优势&lt;/strong&gt;：MLP 方法不依赖于摄像头的几何参数，而是通过学习隐式表示来完成视图转换。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;缺陷&lt;/strong&gt;：尽管 MLP 具有通用逼近能力，但由于缺乏深度信息和遮挡问题，视图转换仍然具有挑战性。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;基于Transformer的方法&lt;/strong&gt;：&lt;/p&gt;</description>
    </item>
    <item>
      <title>MLLM</title>
      <link>http://localhost:1313/post/mllm/</link>
      <pubDate>Sun, 09 Feb 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/mllm/</guid>
      <description>&lt;h1 id=&#34;1基础&#34;&gt;1基础&lt;/h1&gt;&#xA;&lt;h2 id=&#34;1-特征提取&#34;&gt;1. 特征提取&lt;/h2&gt;&#xA;&lt;h3 id=&#34;一cv中的特征提取&#34;&gt;&lt;strong&gt;一、CV中的特征提取&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;h4 id=&#34;1-传统方法手工设计特征&#34;&gt;&lt;strong&gt;1. 传统方法（手工设计特征）&lt;/strong&gt;&lt;/h4&gt;&#xA;&lt;p&gt;&lt;strong&gt;(1) 低级视觉特征&lt;/strong&gt;：颜色、纹理、 边缘与形状&amp;hellip;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;(2) 中级语义特征&lt;/strong&gt;：SIFT（尺度不变特征变换）、SURF（加速鲁棒特征）、LBP（局部二值模式）&amp;hellip;&lt;/p&gt;&#xA;&lt;h4 id=&#34;2-深度学习方法自动学习特征&#34;&gt;&lt;strong&gt;2. 深度学习方法（自动学习特征）&lt;/strong&gt;&lt;/h4&gt;&#xA;&lt;h5 id=&#34;1-卷积神经网络cnn&#34;&gt;&lt;strong&gt;(1) 卷积神经网络（CNN）&lt;/strong&gt;&lt;/h5&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;核心思想&lt;/strong&gt;：通过卷积层提取局部特征，池化层降低维度，全连接层进行分类。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;经典模型&lt;/strong&gt;：LeNet-5、AlexNet、VGGNet、ResNet(使用残差可以训练更深的网络)&amp;hellip;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/MLLM/1.png&#34; alt=&#34;1&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h5 id=&#34;2-视觉transformervit&#34;&gt;&lt;strong&gt;(2) 视觉Transformer（ViT）&lt;/strong&gt;&lt;/h5&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;核心思想&lt;/strong&gt;：将图像分割为小块（patches），通过自注意力机制建模全局关系。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;优势&lt;/strong&gt;：无需局部卷积先验，直接建模长距离依赖; 在ImageNet等任务上超越传统CNN。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;二nlp中的特征提取&#34;&gt;&lt;strong&gt;二、NLP中的特征提取&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;h4 id=&#34;1-传统方法基于统计与规则&#34;&gt;&lt;strong&gt;1. 传统方法（基于统计与规则）&lt;/strong&gt;&lt;/h4&gt;&#xA;&lt;p&gt;&lt;strong&gt;(1) 词袋模型（Bag of Words, BoW）&lt;/strong&gt;: 将文本表示为词汇表中单词的出现频率。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;(2) TF-IDF（词频-逆文档频率）&lt;/strong&gt;: 衡量单词在文档中的重要性（TF-IDF值 = 词频 × 逆文档频率）。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;(3) N-gram模型&lt;/strong&gt;: 统计连续N个词的组合频率（如Bi-gram、Tri-gram）。&lt;/p&gt;&#xA;&lt;h5 id=&#34;4-词嵌入预训练词向量&#34;&gt;&lt;strong&gt;(4) 词嵌入（预训练词向量）&lt;/strong&gt;&lt;/h5&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Word2Vec&lt;/strong&gt;（2013）：&#xA;&lt;ul&gt;&#xA;&lt;li&gt;通过Skip-Gram或CBOW模型，将词映射为低维稠密向量。&lt;/li&gt;&#xA;&lt;li&gt;相似词在向量空间中距离相近（如“国王-王后≈男人-女人”）。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;GloVe&lt;/strong&gt;（2014）：&#xA;&lt;ul&gt;&#xA;&lt;li&gt;基于全局词共现矩阵，结合统计信息和词向量学习。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;strong&gt;(5) 局限性&lt;/strong&gt;：无法建模长距离上下文依赖; 词向量静态，无法处理一词多义。&lt;/p&gt;</description>
    </item>
    <item>
      <title>transformer</title>
      <link>http://localhost:1313/post/transformer/</link>
      <pubDate>Sat, 08 Feb 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/transformer/</guid>
      <description>&lt;h2 id=&#34;一transformer架构&#34;&gt;一、Transformer架构&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;﻿基于编码器-解码器架构来处理序列对&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;﻿跟使用注意力的seq2seq不同，Transformer是纯基于注意力&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;seq2seq&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/trans/1.png&#34; alt=&#34;1&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;transformer&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/trans/2.jpg&#34; alt=&#34;1&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h3 id=&#34;1-多头注意力muti-head-attention&#34;&gt;1. 多头注意力(Muti-head attention)&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;对同一key，value，query，希望抽取不同的信息*（类似卷积的多通道）*&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;例如短距离关系和长距离关系&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;多头注意力使用h个独立的注意力池化&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;合并各个头（head） 输出得到最终输出&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/trans/3.png&#34; alt=&#34;3&#34;&gt;&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;通过全连阶层，映射到一个较低的维度&lt;/li&gt;&#xA;&lt;li&gt;进行多个attention&lt;/li&gt;&#xA;&lt;li&gt;对每一个attention的输出，进行concat&lt;/li&gt;&#xA;&lt;li&gt;再通过一个全连接，得到输出的维度&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h4 id=&#34;数学原理&#34;&gt;数学原理&lt;/h4&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/trans/4.jpg&#34; alt=&#34;4&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>LLM</title>
      <link>http://localhost:1313/post/llm/</link>
      <pubDate>Mon, 20 Jan 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/llm/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1uNk1YxEJQ?spm_id_from=333.788.videopod.episodes&amp;amp;vd_source=80aea28698fb0235b45699fc7e6fcdac&amp;amp;p=2&#34;&gt;https://www.bilibili.com/video/BV1uNk1YxEJQ?spm_id_from=333.788.videopod.episodes&amp;vd_source=80aea28698fb0235b45699fc7e6fcdac&amp;p=2&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;概述&#34;&gt;概述&lt;/h1&gt;&#xA;&lt;h2 id=&#34;大模型的演变&#34;&gt;大模型的演变&lt;/h2&gt;&#xA;&lt;p&gt;大模型的训练整体上分为三个阶段：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;预训练&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;在这个阶段它会学习各种不同种类的语料，学习到语言的统计规律和一般知识。&lt;/p&gt;&#xA;&lt;p&gt;但是大模型在这个阶段只是学会了补全句子，却没有学会怎么样去领会人类的意图（类似成语接龙）。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;SFT（监督微调）&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;在这个阶段大模型可以学习各种人类的对话语料，甚至是非常专业的垂直领域知识。&lt;/p&gt;&#xA;&lt;p&gt;但是模型的回答有时候可能并不符合人类的偏好，它可能会输出一些涉黄、涉政、涉暴或者种族歧视等言论。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;RLHF（基于人类反馈的强化学习）&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;在这个阶段大模型会针对同一问题进行多次回答，人类会对这些回答打分。&lt;/p&gt;&#xA;&lt;p&gt;大模型会在此阶段学习到如何输出分数最高的回答，使得回答更符合人类的偏好。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;分类&#34;&gt;分类&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;大语言模型（LLM）&lt;/p&gt;&#xA;&lt;p&gt;专注于自然语言处理（NLP），旨在处理语言、文章、对话等自然语言文本。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;多模态模型&lt;/p&gt;&#xA;&lt;p&gt;多模态大模型能够同时处理和理解来自不同感知通道（如文本、图像、音频、视频等）的数据，在这些模态之间建立关联和交互。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;工作流程&#34;&gt;工作流程&lt;/h2&gt;&#xA;&lt;h3 id=&#34;分词化与词表映射&#34;&gt;分词化与词表映射&lt;/h3&gt;&#xA;&lt;p&gt;分词化（Tokenization）是指将段落和句子分割成更小的分词（token）的过程。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Diffusion Model</title>
      <link>http://localhost:1313/post/diffusion-model/</link>
      <pubDate>Fri, 17 Jan 2025 10:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/diffusion-model/</guid>
      <description>&lt;h1 id=&#34;概述&#34;&gt;概述&lt;/h1&gt;&#xA;&lt;h2 id=&#34;影像生成模型本质上的共同目标&#34;&gt;影像生成模型本质上的共同目标&lt;/h2&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/DM/19.png&#34; alt=&#34;19&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;进一步：输入加入了文字表述&lt;/p&gt;&#xA;&lt;p&gt;目标：产生的图片与真实图片越接近越好&lt;/p&gt;&#xA;&lt;h2 id=&#34;原理&#34;&gt;原理&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Reverse Process&lt;/strong&gt;（多次Denoise）&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;reconstructing meaningful data from noise&lt;/strong&gt; by iteratively removing noise that was added during the &lt;strong&gt;forward process&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Forward Process&lt;/strong&gt;:&lt;/p&gt;&#xA;&lt;p&gt;Gradually adds noise to data over multiple steps until the data becomes pure noise.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/DM/1.png&#34; alt=&#34;1&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Denoise输入：图片 + 噪音程度&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/DM/2.png&#34; alt=&#34;2&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;denoise-model内部&#34;&gt;Denoise Model内部&lt;/h3&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/DM/3.png&#34; alt=&#34;3&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Generative AI</title>
      <link>http://localhost:1313/post/generativeai/</link>
      <pubDate>Sat, 04 Jan 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/generativeai/</guid>
      <description>&lt;h1 id=&#34;chatgpt&#34;&gt;ChatGPT&lt;/h1&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/GA/1.png&#34; alt=&#34;1&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;G：generative&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;P：pre-trained&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;T：transformer&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;ChatGPT 真正做的事：文字接龙&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Autoregressive Generation&lt;/strong&gt;：逐个生成&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/GA/3.png&#34; alt=&#34;3&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/GA/4.png&#34; alt=&#34;4&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;token&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;文字接龙时可以选择的符号&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/GA/5.png&#34; alt=&#34;5&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;每次回答都随机（掷骰子）&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/GA/6.png&#34; alt=&#34;6&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>VAE</title>
      <link>http://localhost:1313/post/vae/</link>
      <pubDate>Sun, 29 Dec 2024 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/vae/</guid>
      <description>&lt;h2 id=&#34;普通自动编码器&#34;&gt;普通自动编码器&lt;/h2&gt;&#xA;&lt;p&gt;目标：将高维度数据压缩成较小的表示&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://prod-files-secure.s3.us-west-2.amazonaws.com/2da5ebdb-aecd-4b19-910d-af09587de5f1/ad01491f-ff96-44a7-9a2f-606c66461df8/%E6%88%AA%E5%B1%8F2024-12-30_19.27.19.png&#34; alt=&#34;截屏2024-12-30 19.27.19.png&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://prod-files-secure.s3.us-west-2.amazonaws.com/2da5ebdb-aecd-4b19-910d-af09587de5f1/40f99add-baee-40ad-ab9b-45dc5f312af9/%E6%88%AA%E5%B1%8F2024-12-30_19.28.31.png&#34; alt=&#34;截屏2024-12-30 19.28.31.png&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://prod-files-secure.s3.us-west-2.amazonaws.com/2da5ebdb-aecd-4b19-910d-af09587de5f1/99bd3fd9-fabb-421d-bcdb-0e6d0c5be310/%E6%88%AA%E5%B1%8F2024-12-30_19.29.08.png&#34; alt=&#34;截屏2024-12-30 19.29.08.png&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;应用：&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://prod-files-secure.s3.us-west-2.amazonaws.com/2da5ebdb-aecd-4b19-910d-af09587de5f1/1b594f9d-d71f-4af3-b7c9-285e1dfffec6/%E6%88%AA%E5%B1%8F2024-12-30_19.29.47.png&#34; alt=&#34;截屏2024-12-30 19.29.47.png&#34;&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;去噪自动编码器&#34;&gt;去噪自动编码器&lt;/h2&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://prod-files-secure.s3.us-west-2.amazonaws.com/2da5ebdb-aecd-4b19-910d-af09587de5f1/a8e8568e-18b2-4281-a7af-05a627e6f5f0/%E6%88%AA%E5%B1%8F2024-12-30_19.35.42.png&#34; alt=&#34;截屏2024-12-30 19.35.42.png&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://prod-files-secure.s3.us-west-2.amazonaws.com/2da5ebdb-aecd-4b19-910d-af09587de5f1/ce2c3930-ae83-40a9-b9d3-aa3f4f0246bd/%E6%88%AA%E5%B1%8F2024-12-30_19.36.58.png&#34; alt=&#34;截屏2024-12-30 19.36.58.png&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://prod-files-secure.s3.us-west-2.amazonaws.com/2da5ebdb-aecd-4b19-910d-af09587de5f1/fdd726fa-7eb9-4b24-af63-f917ee593b1c/%E6%88%AA%E5%B1%8F2024-12-30_19.37.09.png&#34; alt=&#34;截屏2024-12-30 19.37.09.png&#34;&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;变分自动编码器&#34;&gt;变分自动编码器&lt;/h2&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://prod-files-secure.s3.us-west-2.amazonaws.com/2da5ebdb-aecd-4b19-910d-af09587de5f1/f4509f09-0920-4a7e-b5ba-74cbec294241/%E6%88%AA%E5%B1%8F2024-12-30_19.39.12.png&#34; alt=&#34;截屏2024-12-30 19.39.12.png&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;损失函数&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://prod-files-secure.s3.us-west-2.amazonaws.com/2da5ebdb-aecd-4b19-910d-af09587de5f1/add2cf71-1094-4639-b2d5-699f551beb48/%E6%88%AA%E5%B1%8F2024-12-30_19.40.36.png&#34; alt=&#34;截屏2024-12-30 19.40.36.png&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;reparameterization-trick重参数化&#34;&gt;Reparameterization Trick(重参数化)&lt;/h3&gt;&#xA;&lt;p&gt;如何实现反向传播？&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://prod-files-secure.s3.us-west-2.amazonaws.com/2da5ebdb-aecd-4b19-910d-af09587de5f1/db9e763f-81d3-460f-81dc-b81242f41cdb/%E6%88%AA%E5%B1%8F2024-12-30_19.42.30.png&#34; alt=&#34;截屏2024-12-30 19.42.30.png&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://prod-files-secure.s3.us-west-2.amazonaws.com/2da5ebdb-aecd-4b19-910d-af09587de5f1/e3fb22c3-4a30-4511-9457-4dc08d40b210/%E6%88%AA%E5%B1%8F2024-12-30_19.44.43.png&#34; alt=&#34;截屏2024-12-30 19.44.43.png&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;核心代码&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://prod-files-secure.s3.us-west-2.amazonaws.com/2da5ebdb-aecd-4b19-910d-af09587de5f1/9c033e5a-8e28-4e82-ab6e-80d29dd08a20/%E6%88%AA%E5%B1%8F2024-12-30_19.45.33.png&#34; alt=&#34;截屏2024-12-30 19.45.33.png&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://prod-files-secure.s3.us-west-2.amazonaws.com/2da5ebdb-aecd-4b19-910d-af09587de5f1/64564c69-b355-4559-95b2-2186d3cf9e08/%E6%88%AA%E5%B1%8F2024-12-30_19.45.42.png&#34; alt=&#34;截屏2024-12-30 19.45.42.png&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://prod-files-secure.s3.us-west-2.amazonaws.com/2da5ebdb-aecd-4b19-910d-af09587de5f1/df9a2aa4-f700-4975-a5ca-f315eab2bcab/%E6%88%AA%E5%B1%8F2024-12-30_19.46.02.png&#34; alt=&#34;截屏2024-12-30 19.46.02.png&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;解耦变分自编码器&#34;&gt;&lt;strong&gt;解耦变分自编码器&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;p&gt;目的：确保潜在分布中的不同神经元互不相干，都在尝试学习输入数据中的不同内容。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;解决方式：增加超参数$\beta$，衡量损失函数中的KL散度。&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://prod-files-secure.s3.us-west-2.amazonaws.com/2da5ebdb-aecd-4b19-910d-af09587de5f1/9501fd11-c948-42ff-ba18-807ca26a11eb/%E6%88%AA%E5%B1%8F2024-12-30_22.11.49.png&#34; alt=&#34;截屏2024-12-30 22.11.49.png&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://prod-files-secure.s3.us-west-2.amazonaws.com/2da5ebdb-aecd-4b19-910d-af09587de5f1/d64275f8-60ea-4cd6-862a-d8d7821c8b3d/%E6%88%AA%E5%B1%8F2024-12-30_22.18.10.png&#34; alt=&#34;截屏2024-12-30 22.18.10.png&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;$\beta$过小：过拟合&lt;/p&gt;&#xA;&lt;p&gt;$\beta$过大：失去输入中的大量细节&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;在强化学习中的作用&lt;/strong&gt;：使Agent可以在压缩后的输入T空间上运行。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
