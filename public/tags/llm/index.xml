<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LLM on HomePage</title>
    <link>http://localhost:1313/tags/llm/</link>
    <description>Recent content in LLM on HomePage</description>
    <generator>Hugo</generator>
    <language>en-US</language>
    <lastBuildDate>Sun, 20 Apr 2025 11:00:59 -0400</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/llm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>LLM - 1.基础理论</title>
      <link>http://localhost:1313/post/llm1/</link>
      <pubDate>Sun, 20 Apr 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/llm1/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/LLM1/0.png&#34; alt=&#34;0&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h2 id=&#34;1-绪论&#34;&gt;1. 绪论&lt;/h2&gt;&#xA;&lt;p&gt;核心目标：对自然语言的概率分布建模&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;n元语法/n元文法模型&lt;/strong&gt;：假设任意单词 wi 出现的概率只与过去 n−1 个词相关。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;统计语言模型(SLM)&lt;/strong&gt;：使用平滑处理，提高低概率事件，降低高概率事件，使整体的概率分布趋于均匀。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;缺点&lt;/strong&gt;：长度限制；依赖人工设计的平滑技术；参数多。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;基于分布式表示和神经网络的语言模型&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;词向量&lt;/strong&gt;：词的独热编码被映射为一个低维稠密的实数向量&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;自监督学习&lt;/strong&gt;：深度神经网络需要采用有监督方法，使用标注数据进行训练。&lt;/p&gt;&#xA;&lt;p&gt;但由于训练目标可以通过无标注文本直接获得，因此模型的训练仅需要大规模无标注文本。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;预训练语言模型(PLM)&lt;/strong&gt;：将预训练模型应用于下游任务时，只需要“微调”预训练模型，使用具体任务的标注数据在预训练语言模型上进行监督训练，就可以取得显著的性能提升。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;语境学习 (ICL)&lt;/strong&gt;：直接使用大语言模型，就可以在很多任务的少样本场景中取得很好的效果。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;缩放法则(Scaling Laws)&lt;/strong&gt;：模型的性能依赖于模型的规模，包括参数量、数据集大小和计算量，模型的效果会随着三者的指数增加而平稳提升。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>BERT</title>
      <link>http://localhost:1313/post/bert/</link>
      <pubDate>Wed, 16 Apr 2025 04:00:59 -0700</pubDate>
      <guid>http://localhost:1313/post/bert/</guid>
      <description>&lt;p&gt;名字来源：美国的一个动画片芝麻街里的主人公&lt;/p&gt;&#xA;&lt;p&gt;论文：https://arxiv.org/abs/1810.04805&lt;/p&gt;&#xA;&lt;h3 id=&#34;nlp-里的迁移学习&#34;&gt;NLP 里的迁移学习&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;在 bert 之前：使用预训练好的模型来抽取词、句子的特征&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;如用 word2vec 或 语言模型（当作embedding层）&lt;/li&gt;&#xA;&lt;li&gt;﻿不更新预训练好的模型&lt;/li&gt;&#xA;&lt;li&gt;﻿缺点&#xA;&lt;ul&gt;&#xA;&lt;li&gt;需要构建新的网络来抓取新任务需要的信息&lt;/li&gt;&#xA;&lt;li&gt;﻿Word2vec 忽略了时序信息，语言模型只看了一个方向&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;bert 的动机&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;基于微调的 NLP 模型&lt;/p&gt;&#xA;&lt;p&gt;前面的层不用动，改最后一层的 output layer 即可&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;预训练的模型抽取了足够多的信息，新的任务只需要增加一个简单的输出层&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h2 id=&#34;bert-架构&#34;&gt;BERT 架构&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;本质&lt;/strong&gt;：一个砍掉解码器、只有编码器的 transformer&lt;/p&gt;&#xA;&lt;p&gt;bert 的工作：证明了效果非常好&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;两个版本：&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Base&lt;/strong&gt;: #blocks=12, hidden size=768, #heads=12,  #parameters=110M&lt;/p&gt;&#xA;&lt;p&gt;﻿﻿&lt;strong&gt;Large&lt;/strong&gt;: #blocks=24, hidden size=1024, #heads=1, #parameter=340M&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;﻿在大规模数据上训练&amp;gt;3B词&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>微调</title>
      <link>http://localhost:1313/post/%E5%BE%AE%E8%B0%83/</link>
      <pubDate>Wed, 19 Mar 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/%E5%BE%AE%E8%B0%83/</guid>
      <description>&lt;h2 id=&#34;大模型预训练&#34;&gt;大模型预训练&lt;/h2&gt;&#xA;&lt;h4 id=&#34;1-从零开始的预训练&#34;&gt;1 从零开始的预训练&lt;/h4&gt;&#xA;&lt;h4 id=&#34;2-在已有开源模型基础上针对特定任务进行训练&#34;&gt;2 在已有开源模型基础上针对特定任务进行训练&lt;/h4&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h3 id=&#34;lora&#34;&gt;LoRa&lt;/h3&gt;&#xA;&lt;p&gt;通过化简权重矩阵，实现高效微调&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/preTrain/1.png&#34; alt=&#34;1&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;将loraA与loraB相乘得到一个lora权重矩阵，将lora权重矩阵加在原始权重矩阵上，就得到了对原始网络的更新。&lt;/p&gt;&#xA;&lt;p&gt;训练参数量减少，但微调效果基本不变。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;两个重要参数：&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/preTrain/2.png&#34; alt=&#34;2&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>MLLM</title>
      <link>http://localhost:1313/post/mllm/</link>
      <pubDate>Sun, 09 Feb 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/mllm/</guid>
      <description>&lt;h1 id=&#34;1基础&#34;&gt;1基础&lt;/h1&gt;&#xA;&lt;h2 id=&#34;1-特征提取&#34;&gt;1. 特征提取&lt;/h2&gt;&#xA;&lt;h3 id=&#34;一cv中的特征提取&#34;&gt;&lt;strong&gt;一、CV中的特征提取&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;h4 id=&#34;1-传统方法手工设计特征&#34;&gt;&lt;strong&gt;1. 传统方法（手工设计特征）&lt;/strong&gt;&lt;/h4&gt;&#xA;&lt;p&gt;&lt;strong&gt;(1) 低级视觉特征&lt;/strong&gt;：颜色、纹理、 边缘与形状&amp;hellip;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;(2) 中级语义特征&lt;/strong&gt;：SIFT（尺度不变特征变换）、SURF（加速鲁棒特征）、LBP（局部二值模式）&amp;hellip;&lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h4 id=&#34;2-深度学习方法自动学习特征&#34;&gt;&lt;strong&gt;2. 深度学习方法（自动学习特征）&lt;/strong&gt;&lt;/h4&gt;&#xA;&lt;h5 id=&#34;1-卷积神经网络cnn&#34;&gt;&lt;strong&gt;(1) 卷积神经网络（CNN）&lt;/strong&gt;&lt;/h5&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;核心思想&lt;/strong&gt;：通过卷积层提取局部特征，池化层降低维度，全连接层进行分类。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;经典模型&lt;/strong&gt;：LeNet-5、AlexNet、VGGNet、ResNet(使用残差可以训练更深的网络)&amp;hellip;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/MLLM/1.png&#34; alt=&#34;1&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h5 id=&#34;2-视觉transformervit&#34;&gt;&lt;strong&gt;(2) 视觉Transformer（ViT）&lt;/strong&gt;&lt;/h5&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;核心思想&lt;/strong&gt;：将图像分割为小块（patches），通过自注意力机制建模全局关系。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;优势&lt;/strong&gt;：无需局部卷积先验，直接建模长距离依赖; 在ImageNet等任务上超越传统CNN。&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>transformer</title>
      <link>http://localhost:1313/post/transformer/</link>
      <pubDate>Sat, 08 Feb 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/transformer/</guid>
      <description>&lt;h2 id=&#34;一transformer架构&#34;&gt;一、Transformer架构&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;﻿基于编码器-解码器架构来处理序列对&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;﻿跟使用注意力的seq2seq不同，Transformer是纯基于注意力&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;seq2seq&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/trans/1.png&#34; alt=&#34;1&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;transformer&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/trans/2.jpg&#34; alt=&#34;1&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
  </channel>
</rss>
