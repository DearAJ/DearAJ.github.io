<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LLM on HomePage</title>
    <link>http://localhost:1313/tags/llm/</link>
    <description>Recent content in LLM on HomePage</description>
    <generator>Hugo</generator>
    <language>en-US</language>
    <lastBuildDate>Thu, 12 Jun 2025 10:00:59 -0400</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/llm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Prompting Guide</title>
      <link>http://localhost:1313/post/prompt/</link>
      <pubDate>Thu, 12 Jun 2025 10:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/prompt/</guid>
      <description>&lt;h2 id=&#34;1-agentic-workflows&#34;&gt;1. Agentic Workflows&lt;/h2&gt;&#xA;&lt;h3 id=&#34;system-prompt-reminders&#34;&gt;System Prompt Reminders&lt;/h3&gt;&#xA;&lt;p&gt;在提示中包含三种关键类型的提醒：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;h4 id=&#34;持久性&#34;&gt;持久性&lt;/h4&gt;&#xA;&lt;p&gt;确保模型理解它正在进入多消息轮次，并防止它过早地将控制权交还给用户。示例如下：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;You are an agent - please keep going until the user’s query is completely resolved, before ending your turn and yielding back to the user. Only terminate your turn when you are sure that the problem is solved.&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h4 id=&#34;工具调用&#34;&gt;工具调用&lt;/h4&gt;&#xA;&lt;p&gt;鼓励模型充分利用其工具，并降低其产生幻觉或猜测答案的可能性。示例如下：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;If you are not sure about file content or codebase structure pertaining to the user’s request, use your tools to read files and gather the relevant information: do NOT guess or make up an answer.&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h4 id=&#34;规划-可选&#34;&gt;规划 [可选]&lt;/h4&gt;&#xA;&lt;p&gt;可确保模型在文本中明确规划和反映每个工具调用，而不是通过将一系列单独的工具调用链接在一起来完成任务。示例如下：&lt;/p&gt;</description>
    </item>
    <item>
      <title>LLM - 4.大模型增强 - RAG</title>
      <link>http://localhost:1313/post/llm4/</link>
      <pubDate>Sun, 11 May 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/llm4/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/RAG/0-1.png&#34; alt=&#34;0-1&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>LLM - 3.指令理解阶段(核心) - 强化学习</title>
      <link>http://localhost:1313/post/llm3.5/</link>
      <pubDate>Fri, 02 May 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/llm3.5/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/LLM3.5/0.png&#34; alt=&#34;0&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>LLM - 3.指令理解阶段(核心) - 指令微调</title>
      <link>http://localhost:1313/post/llm3/</link>
      <pubDate>Thu, 01 May 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/llm3/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/LLM3/0.png&#34; alt=&#34;0&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;指令微调又称有监督微调，旨在使模型具备指令遵循 （Instruction Following）能力。&lt;/p&gt;&#xA;&lt;p&gt;核心问题：&lt;em&gt;如何构造指令数据？如何高效低成本地进行指令微调训练？如何在语言模型基础上进一步扩大上下文？&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>LLM - 2.预训练阶段</title>
      <link>http://localhost:1313/post/llm2/</link>
      <pubDate>Sun, 27 Apr 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/llm2/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/LLM2/0.png&#34; alt=&#34;0&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>LLM - 1.基础理论</title>
      <link>http://localhost:1313/post/llm1/</link>
      <pubDate>Sun, 20 Apr 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/llm1/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/LLM1/0.png&#34; alt=&#34;0&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>BERT</title>
      <link>http://localhost:1313/post/bert/</link>
      <pubDate>Wed, 16 Apr 2025 04:00:59 -0700</pubDate>
      <guid>http://localhost:1313/post/bert/</guid>
      <description>&lt;p&gt;名字来源：美国的一个动画片芝麻街里的主人公&lt;/p&gt;&#xA;&lt;p&gt;论文：https://arxiv.org/abs/1810.04805&lt;/p&gt;&#xA;&lt;h3 id=&#34;nlp-里的迁移学习&#34;&gt;NLP 里的迁移学习&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;在 bert 之前：使用预训练好的模型来抽取词、句子的特征&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;如用 word2vec 或 语言模型（当作embedding层）&lt;/li&gt;&#xA;&lt;li&gt;﻿不更新预训练好的模型&lt;/li&gt;&#xA;&lt;li&gt;﻿缺点&#xA;&lt;ul&gt;&#xA;&lt;li&gt;需要构建新的网络来抓取新任务需要的信息&lt;/li&gt;&#xA;&lt;li&gt;﻿Word2vec 忽略了时序信息，语言模型只看了一个方向&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;bert 的动机&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;基于微调的 NLP 模型&lt;/p&gt;&#xA;&lt;p&gt;前面的层不用动，改最后一层的 output layer 即可&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;预训练的模型抽取了足够多的信息，新的任务只需要增加一个简单的输出层&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h2 id=&#34;bert-架构&#34;&gt;BERT 架构&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;本质&lt;/strong&gt;：一个砍掉解码器、只有编码器的 transformer&lt;/p&gt;&#xA;&lt;p&gt;bert 的工作：证明了效果非常好&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;两个版本：&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Base&lt;/strong&gt;: #blocks=12, hidden size=768, #heads=12,  #parameters=110M&lt;/p&gt;&#xA;&lt;p&gt;﻿﻿&lt;strong&gt;Large&lt;/strong&gt;: #blocks=24, hidden size=1024, #heads=1, #parameter=340M&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;﻿在大规模数据上训练&amp;gt;3B词&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>微调</title>
      <link>http://localhost:1313/post/%E5%BE%AE%E8%B0%83/</link>
      <pubDate>Wed, 19 Mar 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/%E5%BE%AE%E8%B0%83/</guid>
      <description>&lt;h2 id=&#34;大模型预训练&#34;&gt;大模型预训练&lt;/h2&gt;&#xA;&lt;h4 id=&#34;1-从零开始的预训练&#34;&gt;1 从零开始的预训练&lt;/h4&gt;&#xA;&lt;h4 id=&#34;2-在已有开源模型基础上针对特定任务进行训练&#34;&gt;2 在已有开源模型基础上针对特定任务进行训练&lt;/h4&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h3 id=&#34;lora&#34;&gt;LoRa&lt;/h3&gt;&#xA;&lt;p&gt;通过化简权重矩阵，实现高效微调&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/preTrain/1.png&#34; alt=&#34;1&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;将loraA与loraB相乘得到一个lora权重矩阵，将lora权重矩阵加在原始权重矩阵上，就得到了对原始网络的更新。&lt;/p&gt;&#xA;&lt;p&gt;训练参数量减少，但微调效果基本不变。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;两个重要参数：&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/preTrain/2.png&#34; alt=&#34;2&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>MLLM</title>
      <link>http://localhost:1313/post/mllm/</link>
      <pubDate>Sun, 09 Feb 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/mllm/</guid>
      <description>&lt;h1 id=&#34;1基础&#34;&gt;1基础&lt;/h1&gt;&#xA;&lt;h2 id=&#34;1-特征提取&#34;&gt;1. 特征提取&lt;/h2&gt;&#xA;&lt;h3 id=&#34;一cv中的特征提取&#34;&gt;&lt;strong&gt;一、CV中的特征提取&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;h4 id=&#34;1-传统方法手工设计特征&#34;&gt;&lt;strong&gt;1. 传统方法（手工设计特征）&lt;/strong&gt;&lt;/h4&gt;&#xA;&lt;p&gt;&lt;strong&gt;(1) 低级视觉特征&lt;/strong&gt;：颜色、纹理、 边缘与形状&amp;hellip;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;(2) 中级语义特征&lt;/strong&gt;：SIFT（尺度不变特征变换）、SURF（加速鲁棒特征）、LBP（局部二值模式）&amp;hellip;&lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h4 id=&#34;2-深度学习方法自动学习特征&#34;&gt;&lt;strong&gt;2. 深度学习方法（自动学习特征）&lt;/strong&gt;&lt;/h4&gt;&#xA;&lt;h5 id=&#34;1-卷积神经网络cnn&#34;&gt;&lt;strong&gt;(1) 卷积神经网络（CNN）&lt;/strong&gt;&lt;/h5&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;核心思想&lt;/strong&gt;：通过卷积层提取局部特征，池化层降低维度，全连接层进行分类。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;经典模型&lt;/strong&gt;：LeNet-5、AlexNet、VGGNet、ResNet(使用残差可以训练更深的网络)&amp;hellip;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/MLLM/1.png&#34; alt=&#34;1&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h5 id=&#34;2-视觉transformervit&#34;&gt;&lt;strong&gt;(2) 视觉Transformer（ViT）&lt;/strong&gt;&lt;/h5&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;核心思想&lt;/strong&gt;：将图像分割为小块（patches），通过自注意力机制建模全局关系。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;优势&lt;/strong&gt;：无需局部卷积先验，直接建模长距离依赖; 在ImageNet等任务上超越传统CNN。&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>transformer</title>
      <link>http://localhost:1313/post/transformer/</link>
      <pubDate>Sat, 08 Feb 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/transformer/</guid>
      <description>&lt;h2 id=&#34;一transformer架构&#34;&gt;一、Transformer架构&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;﻿基于编码器-解码器架构来处理序列对&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;﻿跟使用注意力的seq2seq不同，Transformer是纯基于注意力&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;seq2seq&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/trans/1.png&#34; alt=&#34;1&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;transformer&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/trans/2.jpg&#34; alt=&#34;1&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
  </channel>
</rss>
