<!DOCTYPE html>
<html lang="en-US">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>HomePage</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="">
    <meta name="generator" content="Hugo 0.140.2">
    
    
    
      <meta name="robots" content="noindex, nofollow">
    
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



    

    
      

    

    
    
      <link href="/tags/rl/index.xml" rel="alternate" type="application/rss+xml" title="HomePage" />
      <link href="/tags/rl/index.xml" rel="feed" type="application/rss+xml" title="HomePage" />
      
    

    
      <link rel="canonical" href="http://localhost:1313/tags/rl/">
    

    <meta property="og:url" content="http://localhost:1313/tags/rl/">
  <meta property="og:site_name" content="HomePage">
  <meta property="og:title" content="RL">
  <meta property="og:locale" content="en_US">
  <meta property="og:type" content="website">

  <meta itemprop="name" content="RL">
  <meta itemprop="datePublished" content="2025-04-11T04:00:59-07:00">
  <meta itemprop="dateModified" content="2025-04-11T04:00:59-07:00">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="RL">

	
  </head><body class="ma0 avenir bg-near-white development">

    

  
  
  <header class="cover bg-top" style="background-image: url('http://localhost:1313/images/taytay.jpg');">
    
    <div class="bg-black-60">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        HomePage
      
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/about/" title="About ME page">
              About ME
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/post/" title="Articles page">
              Articles
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/contact/" title="Contact page">
              Contact
            </a>
          </li>
          
        </ul>
      
      <div class="ananke-socials"><a href="https://www.facebook.com/patrick.kollitsch" target="_blank" rel="noopener"
        class="ananke-social-link link-transition facebook link dib z-999 pt3 pt0-l mr1"
        title="follow on Facebook - Opens in a new window"
        aria-label="follow on Facebook - Opens in a new window">
      <span class="icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M512 256C512 114.6 397.4 0 256 0S0 114.6 0 256C0 376 82.7 476.8 194.2 504.5V334.2H141.4V256h52.8V222.3c0-87.1 39.4-127.5 125-127.5c16.2 0 44.2 3.2 55.7 6.4V172c-6-.6-16.5-1-29.6-1c-42 0-58.2 15.9-58.2 57.2V256h83.6l-14.4 78.2H287V510.1C413.8 494.8 512 386.9 512 256h0z"/></svg>
            
          </span></a><a href="https://bsky.app/profile/kollitsch.dev" target="_blank" rel="noopener"
        class="ananke-social-link link-transition bluesky link dib z-999 pt3 pt0-l mr1"
        title="follow on Bluesky - Opens in a new window"
        aria-label="follow on Bluesky - Opens in a new window">
      <span class="icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M407.8 294.7c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3zM288 227.1C261.9 176.4 190.9 81.9 124.9 35.3C61.6-9.4 37.5-1.7 21.6 5.5C3.3 13.8 0 41.9 0 58.4S9.1 194 15 213.9c19.5 65.7 89.1 87.9 153.2 80.7c3.3-.5 6.6-.9 10-1.4c-3.3 .5-6.6 1-10 1.4C74.3 308.6-9.1 342.8 100.3 464.5C220.6 589.1 265.1 437.8 288 361.1c22.9 76.7 49.2 222.5 185.6 103.4c102.4-103.4 28.1-156-65.8-169.9c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3c64.1 7.1 133.6-15.1 153.2-80.7C566.9 194 576 75 576 58.4s-3.3-44.7-21.6-52.9c-15.8-7.1-40-14.9-103.2 29.8C385.1 81.9 314.1 176.4 288 227.1z"/></svg>
            
          </span></a><a href="http://linkedin.com/in/patrickkollitsch" target="_blank" rel="noopener"
        class="ananke-social-link link-transition linkedin link dib z-999 pt3 pt0-l mr1"
        title="follow on LinkedIn - Opens in a new window"
        aria-label="follow on LinkedIn - Opens in a new window">
      <span class="icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
            
          </span></a></div>

    </div>
  </div>
</nav>

      <div class="tc-l pv4 pv6-l ph3 ph4-ns">
        <h1 class="f2 f-subheadline-l fw2 white-90 mb0 lh-title">
          RL
        </h1>
        
      </div>
    </div>
  </header>


    <main class="pb7" role="main">
      
  <article class="cf pa3 pa4-m pa4-l">
    <div class="measure-wide-l center f4 lh-copy nested-copy-line-height nested-links mid-gray">
      <p>Below you will find pages that utilize the taxonomy term “RL”</p>
    </div>
  </article>
  <div class="mw8 center">
    <section class="flex-ns flex-wrap justify-around mt5">
      
        <div class="relative w-100  mb4 bg-white">
            <div class="mb3 pa4 mid-gray overflow-hidden">
    
      <div class="f6">
        April 11, 2025
      </div>
    
    <h1 class="f3 near-black">
      <a href="/post/%E6%95%B0%E6%8D%AE%E9%9B%86-nusences/" class="link black dim">
        数据集-NuSences
      </a>
    </h1>
    <div class="nested-links f5 lh-copy nested-copy-line-height">
      <ol>
<li>
<h4 id="内容">内容</h4>
<p>nuScenes 包含 1000 个场景，大约 1.4M 的相机图像、390k LIDAR 扫描、1.4M 雷达扫描和 40k 关键帧中的 1.4M 对象边界框。</p>
<p>nuScenes-lidarseg 包含 40000 个点云和 1000 个场景（850 个用于训练和验证的场景，以及 150 个用于测试的场景）中的 14 亿个注释点。</p>
<p> </p>
</li>
<li>
<h4 id="数据采集">数据采集</h4>
<ol>
<li>
<p><strong>车辆设置</strong></p>
<p><img src="/images/nuSences/1.png" alt="1"></p>
<ol>
<li>1 个旋转激光雷达 （Velodyne HDL32E）</li>
<li>5 个远程雷达传感器 （Continental ARS 408-21）</li>
<li>6 个相机 （Basler acA1600-60gc）</li>
<li>1个 IMU &amp; GPS （高级导航空间版）</li>
</ol>
</li>
<li>
<p><strong>Sensor(传感器)校准 - 内外参</strong></p>
<ol>
<li>LIDAR extrinsics</li>
<li>相机 extrinsics</li>
<li>RADAR extrinsics</li>
<li>相机 intrinsic 校准</li>
</ol>
</li>
<li>
<p><strong>Sensor(传感器)同步</strong></p>
<p>实现跨模态数据对齐：当顶部 LIDAR 扫描相机 FOV 的中心时，会触发相机的曝光</p>
</li>
</ol>
</li>
</ol>
    </div>
  <a href="/post/%E6%95%B0%E6%8D%AE%E9%9B%86-nusences/" class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a>
  </div>

        </div>
      
        <div class="relative w-100  mb4 bg-white">
            <div class="mb3 pa4 mid-gray overflow-hidden">
    
      <div class="f6">
        April 6, 2025
      </div>
    
    <h1 class="f3 near-black">
      <a href="/post/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E5%8F%91%E5%B1%95%E8%B6%8B%E5%8A%BF/" class="link black dim">
        强化学习-发展趋势
      </a>
    </h1>
    <div class="nested-links f5 lh-copy nested-copy-line-height">
      <h4 id="1阶段">1阶段</h4>
<p><strong>RL</strong>：一种解决马尔可夫决策过程的方法。</p>
<p><strong>分类</strong>：value-based算法（DQN）和 policy-based算法（PPO)。</p>
<p><strong>应用方向</strong>：多智能体强化学习， 安全强化学习等等。</p>
<p> </p>
<h4 id="2-阶段">2 阶段</h4>
<p>强化学习应用的论文描述严格（必须有以下内容）：</p>
<ol>
<li>非常准确的状态空间和动作空间定义</li>
<li>必须存在状态转移函数，不允许单步决策，也就是一个动作就gameover</li>
<li>必须有过程奖励，且需要存在牺牲短期的过程奖励而获取最大累计回报的case案例</li>
</ol>
<p><strong>应用方向</strong>：游戏AI（份额不大）</p>
<p> </p>
<h4 id="3-阶段">3 阶段</h4>
<p>RL 落地的真正难点在于<strong>问题的真实构建</strong>，而非近似构建或策略求解等等方面的问题。从原先任务只有求解策略的过程是强化学习，变成了 <strong>构建问题+求解策略</strong> 统称为强化学习。</p>
<p>典型如 offline model-based RL 和 RLHF，其中核心的模块变成了<strong>通过神经网络模拟状态转移函数和奖励函数</strong>，策略求解反而在方法论中被一句带过。</p>
<p>这个过程可以被解耦，变成跟强化学习毫无相关的名词概念，例如世界模型概念等。对于RL，没有有效的交互环境下的就没法达到目标，有这种有效交互环境的实际应用场景却非常少。</p>
    </div>
  <a href="/post/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E5%8F%91%E5%B1%95%E8%B6%8B%E5%8A%BF/" class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a>
  </div>

        </div>
      
        <div class="relative w-100  mb4 bg-white">
            <div class="mb3 pa4 mid-gray overflow-hidden">
    
      <div class="f6">
        April 4, 2025
      </div>
    
    <h1 class="f3 near-black">
      <a href="/post/%E5%B7%A5%E5%85%B7%E9%93%BE-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" class="link black dim">
        工具链-强化学习
      </a>
    </h1>
    <div class="nested-links f5 lh-copy nested-copy-line-height">
      <h2 id="1-gym">1. gym</h2>
<p>官方文档：https://www.gymlibrary.dev</p>
<ul>
<li>
<p>最小例子 <code>CartPole-v0</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python3" data-lang="python3"><span style="display:flex;"><span><span style="color:#f92672">import</span> gymenv <span style="color:#f92672">=</span> gym<span style="color:#f92672">.</span>make(<span style="color:#e6db74">&#39;CartPole-v0&#39;</span>)
</span></span><span style="display:flex;"><span>env<span style="color:#f92672">.</span>reset()
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">1000</span>):
</span></span><span style="display:flex;"><span>    env<span style="color:#f92672">.</span>render()
</span></span><span style="display:flex;"><span>    env<span style="color:#f92672">.</span>step(env<span style="color:#f92672">.</span>action_space<span style="color:#f92672">.</span>sample()) <span style="color:#75715e"># take a random action</span>
</span></span></code></pre></div></li>
</ul>
<p> </p>
<h3 id="观测-observations">观测 (Observations)</h3>
<p>在 Gym 仿真中，每一次回合开始，需要先执行 <code>reset()</code> 函数，返回<strong>初始观测信息，然后根据标志位 <code>done</code> 的状态，来决定是否进行下一次回合</strong>。代码表示：</p>
<p> </p>
<p><code>env.step()</code> 函数对每一步进行仿真，返回 4 个参数：</p>
<ul>
<li>
<p><strong>观测</strong> Observation (Object)：当前 step 执行后，环境的观测(类型为对象)。例如，从相机获取的像素点，机器人各个关节的角度或棋盘游戏当前的状态等；</p>
</li>
<li>
<p><strong>奖励</strong> Reward (Float): 执行上一步动作(action)后，智体(agent)获得的奖励，不同的环境中奖励值变化范围也不相同，但是强化学习的目标就是使得总奖励值最大；</p>
</li>
<li>
<p><strong>完成</strong> Done (Boolen): 表示是否需要将环境重置 <code>env.reset</code>。</p>
</li>
</ul>
    </div>
  <a href="/post/%E5%B7%A5%E5%85%B7%E9%93%BE-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a>
  </div>

        </div>
      
        <div class="relative w-100  mb4 bg-white">
            <div class="mb3 pa4 mid-gray overflow-hidden">
    
      <div class="f6">
        April 3, 2025
      </div>
    
    <h1 class="f3 near-black">
      <a href="/post/ppo/" class="link black dim">
        PPO
      </a>
    </h1>
    <div class="nested-links f5 lh-copy nested-copy-line-height">
      <h2 id="1-论文详读">1. 论文详读</h2>
<p><a href="https://arxiv.org/abs/1707.06347">Proximal Policy Optimization Algorithms</a>（Proximal：近似）</p>
<h2 id="2-ppo">2. PPO</h2>
<h4 id="回顾-trpo">回顾 TRPO</h4>
<ul>
<li>
<p>使用 KL 散度约束 policy 的更新幅度；使用重要性采样</p>
<p><strong>缺点</strong>：近似会带来误差（重要性采样的通病）；解带约束的优化问题困难</p>
</li>
<li>
<h4 id="ppo-的改进">PPO 的改进</h4>
<ol>
<li>
<p>TRPO 采用重要性采样 &mdash;-&gt; PPO 采用 <strong>clip 截断</strong>，限制新旧策略差异，避免更新过大。</p>
</li>
<li>
<p>优势函数 At 选用多步时序差分</p>
</li>
<li>
<p>自适应的 KL 惩罚项</p>
</li>
</ol>
</li>
</ul>
<ol>
<li><strong>Critic网络训练</strong>：通过最小化<code>critic_loss = MSE(critic(states), td_target)</code>，让critic的价值估计更准确</li>
<li><strong>Actor网络更新</strong>：
<ul>
<li>TD误差的广义形式（GAE）被用作优势函数，指导策略更新方向</li>
<li>优势函数越大，表示该动作比平均表现更好，应被加强</li>
</ul>
</li>
</ol>
<p> </p>
<h2 id="3-ppo-惩罚">3. PPO-惩罚</h2>
<p>PPO-惩罚（PPO-Penalty）：用拉格朗日乘数法将 KL 散度的限制放进了目标函数中，使其变成了一个无约束的优化问题，在迭代的过程中不断更新 KL 散度前的系数 beta。</p>
    </div>
  <a href="/post/ppo/" class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a>
  </div>

        </div>
      
        <div class="relative w-100  mb4 bg-white">
            <div class="mb3 pa4 mid-gray overflow-hidden">
    
      <div class="f6">
        April 2, 2025
      </div>
    
    <h1 class="f3 near-black">
      <a href="/post/trpo/" class="link black dim">
        TRPO
      </a>
    </h1>
    <div class="nested-links f5 lh-copy nested-copy-line-height">
      <p>基于策略的方法的缺点：当策略网络是深度模型时沿着策略梯度更新参数，很有可能由于步长太长，策略突然显著变差，进而影响训练效果。</p>
<ul>
<li>
<p><strong>信任区域策略优化</strong>（TRPO）算法的核心思想：</p>
<p><strong>信任区域</strong>（trust region）：在这个区域上更新策略时能够得到某种策略性能的安全性保证。</p>
</li>
</ul>
<p> </p>
<h2 id="1-策略目标">1. 策略目标</h2>
<p><img src="/images/trpo/1.png" alt="1"></p>
<p><img src="/images/trpo/2.png" alt="2"></p>
    </div>
  <a href="/post/trpo/" class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a>
  </div>

        </div>
      
        <div class="relative w-100  mb4 bg-white">
            <div class="mb3 pa4 mid-gray overflow-hidden">
    
      <div class="f6">
        April 2, 2025
      </div>
    
    <h1 class="f3 near-black">
      <a href="/post/actor-critic/" class="link black dim">
        Actor-Critic
      </a>
    </h1>
    <div class="nested-links f5 lh-copy nested-copy-line-height">
      <p>Actor-Critic 算法本质上是基于策略的算法，因为这一系列算法的<strong>目标都是优化一个带参数的策略，只是会额外学习价值函数</strong>，从而帮助策略函数更好地学习。</p>
<p>Actor-Critic 算法则可以在每一步之后都进行更新，并且不对任务的步数做限制。</p>
<ul>
<li>
<p>更一般形式的策略梯度</p>
<p><img src="/images/ac/1.png" alt="1"></p>
</li>
</ul>
<p> </p>
<h3 id="1-actor策略网络">1. Actor（策略网络）</h3>
<p>Actor 要做的是与环境交互，并在 Critic 价值函数的指导下用策略梯度学习一个更好的策略。</p>
<p>Actor 的更新采用策略梯度的原则。</p>
<h3 id="2--critic价值网络">2.  Critic（价值网络）</h3>
<p>Critic 要做的是通过 Actor 与环境交互收集的数据学习一个价值函数，这个价值函数会用于判断在当前状态什么动作是好的，什么动作不是好的，进而帮助 Actor 进行策略更新。</p>
    </div>
  <a href="/post/actor-critic/" class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a>
  </div>

        </div>
      
        <div class="relative w-100  mb4 bg-white">
            <div class="mb3 pa4 mid-gray overflow-hidden">
    
      <div class="f6">
        April 1, 2025
      </div>
    
    <h1 class="f3 near-black">
      <a href="/post/reinforce/" class="link black dim">
        REINFORCE
      </a>
    </h1>
    <div class="nested-links f5 lh-copy nested-copy-line-height">
      <p>Q-learning、DQN 算法都是<strong>基于价值</strong>（value-based）的方法</p>
<ul>
<li>Q-learning 是处理有限状态的算法</li>
<li>DQN 可以用来解决连续状态的问题</li>
</ul>
<p>在强化学习中，除了基于值函数的方法，还有一支非常经典的方法，那就是<strong>基于策略</strong>（policy-based）的方法。</p>
<p> </p>
<p>对比 value-based 和 policy-based</p>
<ul>
<li>基于值函数：主要是学习<strong>值函数</strong>，然后根据值函数导出一个策略，学习过程中并不存在一个显式的策略；</li>
<li>基于策略：直接显式地学习一个<strong>目标策略</strong>。策略梯度是基于策略的方法的基础。</li>
</ul>
<p> </p>
<p> </p>
<h2 id="1-策略梯度">1. 策略梯度</h2>
<ul>
<li>
<p>将策略参数化：寻找一个最优策略并最大化这个策略在环境中的期望回报，即调整策略参数使平均回报最大化。</p>
</li>
<li>
<p>策略学习的目标函数</p>
<p><img src="/images/REINFORCE/1.png" alt="1"></p>
<ul>
<li>J(θ) 是策略的目标函数（想要最大化的量）；</li>
<li>πθ 是参数为θ的随机性策略，并且处处可微（可以理解为AI的决策规则）；</li>
<li>Vπθ(s0) 指从初始状态s₀开始<strong>遵循策略π能获得的预期总回报</strong>；</li>
<li>Es0 是对所有可能的初始状态求期望。</li>
</ul>
</li>
</ul>
    </div>
  <a href="/post/reinforce/" class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a>
  </div>

        </div>
      
        <div class="relative w-100  mb4 bg-white">
            <div class="mb3 pa4 mid-gray overflow-hidden">
    
      <div class="f6">
        April 1, 2025
      </div>
    
    <h1 class="f3 near-black">
      <a href="/post/dqn/" class="link black dim">
        DQN (deep Q network)
      </a>
    </h1>
    <div class="nested-links f5 lh-copy nested-copy-line-height">
      <p>Q-learning 算法用表格存储动作价值的做法只在 环境的状态和动作都是离散的，并且空间都比较小 的情况下适用.</p>
<p><strong>DQN</strong>：用来解决连续状态下离散动作的问题，是离线策略算法，可以使用ε-贪婪策略来平衡探索与利用。</p>
<p><strong>Q 网络</strong>：用于拟合函数Q函数的神经网络</p>
<p><img src="/images/DQN/1.png" alt="1"></p>
<p><strong>Q 网络的损失函数</strong>（均方误差形式）</p>
<p><img src="/images/DQN/2.png" alt="2"></p>
    </div>
  <a href="/post/dqn/" class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a>
  </div>

        </div>
      
        <div class="relative w-100  mb4 bg-white">
            <div class="mb3 pa4 mid-gray overflow-hidden">
    
      <div class="f6">
        March 31, 2025
      </div>
    
    <h1 class="f3 near-black">
      <a href="/post/qlearing/" class="link black dim">
        Q-learing
      </a>
    </h1>
    <div class="nested-links f5 lh-copy nested-copy-line-height">
      <p><strong>无模型的强化学习</strong>：不需要事先知道环境的奖励函数和状态转移函数，而是直接使用和环境交互的过程中采样到的数据来学习。</p>
<h2 id="1-时序差分方法">1. 时序差分方法</h2>
<p>时序差分方法核心：对未来动作选择的价值估计来更新对当前动作选择的价值估计。</p>
<h3 id="蒙特卡洛方法monte-carlo-methods">蒙特卡洛方法（Monte-Carlo methods）</h3>
<p>使用重复随机抽样，然后运用概率统计方法来从抽样结果中归纳出我们想求的目标的数值估计。</p>
<ul>
<li>
<p>用蒙特卡洛方法来估计一个策略在一个马尔可夫决策过程中的状态价值函数：<strong>用样本均值作为期望值的估计</strong></p>
<ol>
<li>在 MDP 上采样很多条序列，计算从这个状态出发的回报再求其期望</li>
<li>一条序列只计算一次回报，也就是这条序列第一次出现该状态时计算后面的累积奖励，而后面再次出现该状态时，该状态就被忽略了。</li>
</ol>
</li>
<li>
<p>蒙特卡洛方法对价值函数的增量更新方式</p>
<p><img src="/images/QLearning/1.png" alt="1"></p>
</li>
<li>
<p>时序差分方法只需要当前步结束即可进行计算</p>
</li>
</ul>
    </div>
  <a href="/post/qlearing/" class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a>
  </div>

        </div>
      
        <div class="relative w-100  mb4 bg-white">
            <div class="mb3 pa4 mid-gray overflow-hidden">
    
      <div class="f6">
        March 31, 2025
      </div>
    
    <h1 class="f3 near-black">
      <a href="/post/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E6%98%93%E6%B7%B7%E6%B7%86%E7%82%B9/" class="link black dim">
        强化学习-易混淆点
      </a>
    </h1>
    <div class="nested-links f5 lh-copy nested-copy-line-height">
      <ol>
<li>
<h3 id="状态价值函数-vs-动作价值函数">状态价值函数 vs 动作价值函数</h3>
<ol>
<li>
<p><strong>状态价值函数</strong></p>
<p><img src="/images/RLconfused/1.png" alt="1"></p>
</li>
<li>
<p><strong>动作价值函数</strong></p>
<p><img src="/images/RLconfused/2.png" alt="2"></p>
</li>
<li>
<p><strong>优势函数</strong></p>
<p><strong>在状态 s 下选择动作 a 比平均情况（即遵循当前策略）好多少</strong></p>
<p>A(s,a)=Q(s,a)−V(s)</p>
<ul>
<li>求解优势函数：<strong>广义优势估计</strong>(GAE)</li>
</ul>
</li>
<li>
<p><strong>广义优势估计</strong>(GAE)</p>
<p>通过指数加权平均不同步长的优势估计（从1步到无穷步），结合γ<em>γ</em>和λ<em>λ</em>的幂次衰减，实现平滑的回报估计。</p>
<p><img src="/images/RLconfused/5.png" alt="5"></p>
</li>
</ol>
</li>
</ol>
    </div>
  <a href="/post/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E6%98%93%E6%B7%B7%E6%B7%86%E7%82%B9/" class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a>
  </div>

        </div>
      
        <div class="relative w-100  mb4 bg-white">
            <div class="mb3 pa4 mid-gray overflow-hidden">
    
      <div class="f6">
        March 29, 2025
      </div>
    
    <h1 class="f3 near-black">
      <a href="/post/ppo-%E5%8E%9F%E7%90%86/" class="link black dim">
        PPO-直观理解
      </a>
    </h1>
    <div class="nested-links f5 lh-copy nested-copy-line-height">
      <h2 id="1-基础概念">1. 基础概念</h2>
<p><img src="/images/PPO/1.png" alt="1"></p>
<ol>
<li>
<p><strong>enviroment</strong>：看到的画面+看不到的后台画面，不了解细节</p>
</li>
<li>
<p><strong>agent(智能体)</strong>：根据策略得到尽可能多的奖励</p>
</li>
<li>
<p><strong>state</strong>：当前状态</p>
</li>
<li>
<p><strong>observation</strong>：state的一部分（有时候agent无法看全）</p>
</li>
<li>
<p><strong>action</strong>：agent做出的动作</p>
</li>
<li>
<p><strong>reward</strong>：agent做出一个动作后环境给予的奖励</p>
</li>
<li>
<p><strong>action space</strong>：可以选择的动作，如上下左右</p>
</li>
<li>
<p><strong>policy</strong>：策略函数，输入state，输出Action的<strong>概率分布</strong>。一般用π表示。</p>
<ol>
<li>训练时应尝试各种action</li>
<li>输出应具有多样性</li>
</ol>
</li>
<li>
<p><strong>Trajectory/Episode/Rollout</strong>：轨迹，用 t 表示一连串状态和动作的序列。有的状态转移是确定的，也有的是不确定的。</p>
</li>
<li>
<p><strong>Return</strong>：回报，从当前时间点到游戏结束的 Reward 的累积和。</p>
</li>
</ol>
<p>强化学习目标：训练一个Policy神经网络π，在所有状态S下，给出相应的Action，得到Return的期望最大。</p>
    </div>
  <a href="/post/ppo-%E5%8E%9F%E7%90%86/" class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a>
  </div>

        </div>
      
        <div class="relative w-100  mb4 bg-white">
            <div class="mb3 pa4 mid-gray overflow-hidden">
    
      <div class="f6">
        March 19, 2025
      </div>
    
    <h1 class="f3 near-black">
      <a href="/post/%E5%BE%AE%E8%B0%83/" class="link black dim">
        微调
      </a>
    </h1>
    <div class="nested-links f5 lh-copy nested-copy-line-height">
      <h2 id="大模型预训练">大模型预训练</h2>
<h4 id="1-从零开始的预训练">1 从零开始的预训练</h4>
<h4 id="2-在已有开源模型基础上针对特定任务进行训练">2 在已有开源模型基础上针对特定任务进行训练</h4>
<p> </p>
<h3 id="lora">LoRa</h3>
<p>通过化简权重矩阵，实现高效微调</p>
<p><img src="/images/preTrain/1.png" alt="1"></p>
<p>将loraA与loraB相乘得到一个lora权重矩阵，将lora权重矩阵加在原始权重矩阵上，就得到了对原始网络的更新。</p>
<p>训练参数量减少，但微调效果基本不变。</p>
<ul>
<li>
<p>两个重要参数：</p>
<p><img src="/images/preTrain/2.png" alt="2"></p>
</li>
</ul>
    </div>
  <a href="/post/%E5%BE%AE%E8%B0%83/" class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a>
  </div>

        </div>
      
        <div class="relative w-100  mb4 bg-white">
            <div class="mb3 pa4 mid-gray overflow-hidden">
    
      <div class="f6">
        February 19, 2025
      </div>
    
    <h1 class="f3 near-black">
      <a href="/post/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/" class="link black dim">
        强化学习-数学基础
      </a>
    </h1>
    <div class="nested-links f5 lh-copy nested-copy-line-height">
      <h2 id="总述">总述</h2>
<p><img src="/images/RL2/1.png" alt="1"></p>
<ul>
<li>
<h4 id="基础工具"><strong>基础工具</strong></h4>
<ol>
<li><strong>基本概念</strong>：state, action, reward, return, episode, policy, mdp&hellip;</li>
<li><strong>贝尔曼公式</strong>：用于评价策略</li>
<li><strong>贝尔曼最优公式</strong>：强化学习的最终目标是求解最优策略</li>
</ol>
</li>
<li>
<h4 id="算法方法">算法/方法</h4>
<ol start="4">
<li><strong>值迭代、策略迭代—— truncated policy iteration</strong>：值和策略update不断迭代</li>
<li><strong>Monte Carlo Learning</strong>：无模型学习</li>
<li><strong>随即近似理论</strong>：from non-incremental to incremental</li>
<li><strong>时序差分方法(TD)</strong></li>
<li><strong>值函数估计</strong>：tabular representation to function representation，引入神经网络</li>
<li><strong>Policy Gradient Methods</strong>：from value-based to policy-based</li>
<li><strong>Actor-Critic Methods</strong>：policy-based + value-based</li>
</ol>
</li>
</ul>
    </div>
  <a href="/post/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/" class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a>
  </div>

        </div>
      
        <div class="relative w-100  mb4 bg-white">
            <div class="mb3 pa4 mid-gray overflow-hidden">
    
      <div class="f6">
        January 2, 2025
      </div>
    
    <h1 class="f3 near-black">
      <a href="/post/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3/" class="link black dim">
        强化学习-直观理解
      </a>
    </h1>
    <div class="nested-links f5 lh-copy nested-copy-line-height">
      <p>不用告诉该怎么做，而是给定奖励函数，什么时候做好。</p>
<h3 id="回归">回归</h3>
<p>增加折现因子</p>
<p><img src="/images/RL/1.png" alt="1"></p>
<p><img src="/images/RL/2.png" alt="2"></p>
<h3 id="强化学习的形式化">强化学习的形式化</h3>
<p>A policy is a function $\pi(s) = a$ mapping from states to actions, that tells you what $action \space a$ to take in a given $state \space s$.</p>
<p><strong>goal</strong>: Find a $policy \space \pi$ that tells you what $action (a = (s))$ to take in every $state (s)$ so as to maximize the return.</p>
<p><img src="/images/RL/3.png" alt="3"></p>
<ul>
<li>
<p><strong>状态动作值函数</strong>（Q-Function）</p>
<p><strong>Q(s,a)</strong> = Return if you:</p>
<ul>
<li>start in state <em>s</em>.</li>
<li>take action <em>a</em> (once).</li>
<li>then behave optimally after that.</li>
</ul>
<p>The best possible return from state s is max$Q(s, a)$. The best possible action in state s is the action a that gives max$Q(s, a)$.</p>
</li>
</ul>
    </div>
  <a href="/post/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3/" class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a>
  </div>

        </div>
      
        <div class="relative w-100  mb4 bg-white">
            <div class="mb3 pa4 mid-gray overflow-hidden">
    
      <div class="f6">
        December 26, 2024
      </div>
    
    <h1 class="f3 near-black">
      <a href="/post/e2e/" class="link black dim">
        End2End
      </a>
    </h1>
    <div class="nested-links f5 lh-copy nested-copy-line-height">
      <p>对于由多个阶段组成的学习系统，端到端学习捕获所有阶段，将其替代为单个神经网络。</p>
<ul>
<li>优点：
<ul>
<li>Let the data speak</li>
<li>Less hand-designing of components needed</li>
</ul>
</li>
<li>缺点：
<ul>
<li>May need large amount of data</li>
<li>Excludes potentially useful hand-designed components</li>
</ul>
</li>
</ul>
<p><strong>关键</strong>：是否有足够的数据</p>
<p><img src="/images/e2e/1.png" alt="1"></p>
    </div>
  <a href="/post/e2e/" class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a>
  </div>

        </div>
      
    </section>
  </div>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://localhost:1313/" >
    &copy;  HomePage 2025 
  </a>
    <div><div class="ananke-socials"><a href="https://www.facebook.com/patrick.kollitsch" target="_blank" rel="noopener"
        class="ananke-social-link link-transition facebook link dib z-999 pt3 pt0-l mr1"
        title="follow on Facebook - Opens in a new window"
        aria-label="follow on Facebook - Opens in a new window">
      <span class="icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M512 256C512 114.6 397.4 0 256 0S0 114.6 0 256C0 376 82.7 476.8 194.2 504.5V334.2H141.4V256h52.8V222.3c0-87.1 39.4-127.5 125-127.5c16.2 0 44.2 3.2 55.7 6.4V172c-6-.6-16.5-1-29.6-1c-42 0-58.2 15.9-58.2 57.2V256h83.6l-14.4 78.2H287V510.1C413.8 494.8 512 386.9 512 256h0z"/></svg>
            
          </span></a><a href="https://bsky.app/profile/kollitsch.dev" target="_blank" rel="noopener"
        class="ananke-social-link link-transition bluesky link dib z-999 pt3 pt0-l mr1"
        title="follow on Bluesky - Opens in a new window"
        aria-label="follow on Bluesky - Opens in a new window">
      <span class="icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M407.8 294.7c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3zM288 227.1C261.9 176.4 190.9 81.9 124.9 35.3C61.6-9.4 37.5-1.7 21.6 5.5C3.3 13.8 0 41.9 0 58.4S9.1 194 15 213.9c19.5 65.7 89.1 87.9 153.2 80.7c3.3-.5 6.6-.9 10-1.4c-3.3 .5-6.6 1-10 1.4C74.3 308.6-9.1 342.8 100.3 464.5C220.6 589.1 265.1 437.8 288 361.1c22.9 76.7 49.2 222.5 185.6 103.4c102.4-103.4 28.1-156-65.8-169.9c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3c64.1 7.1 133.6-15.1 153.2-80.7C566.9 194 576 75 576 58.4s-3.3-44.7-21.6-52.9c-15.8-7.1-40-14.9-103.2 29.8C385.1 81.9 314.1 176.4 288 227.1z"/></svg>
            
          </span></a><a href="http://linkedin.com/in/patrickkollitsch" target="_blank" rel="noopener"
        class="ananke-social-link link-transition linkedin link dib z-999 pt3 pt0-l mr1"
        title="follow on LinkedIn - Opens in a new window"
        aria-label="follow on LinkedIn - Opens in a new window">
      <span class="icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc. --><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
            
          </span></a></div>
</div>
  </div>
</footer>

  </body>
</html>
