<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>RL on HomePage</title>
    <link>http://localhost:1313/tags/rl/</link>
    <description>Recent content in RL on HomePage</description>
    <generator>Hugo</generator>
    <language>en-US</language>
    <lastBuildDate>Thu, 20 Mar 2025 11:00:59 -0400</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/rl/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>PPO</title>
      <link>http://localhost:1313/post/ppo/</link>
      <pubDate>Thu, 20 Mar 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/ppo/</guid>
      <description>&lt;h2 id=&#34;1-基础概念&#34;&gt;1. 基础概念&lt;/h2&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/PPO/1.png&#34; alt=&#34;1&#34;&gt;&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;enviroment&lt;/strong&gt;：看到的画面+看不到的后台画面，不了解细节&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;agent(智能体)&lt;/strong&gt;：根据策略得到尽可能多的奖励&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;state&lt;/strong&gt;：当前状态&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;observation&lt;/strong&gt;：state的一部分（有时候agent无法看全）&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;action&lt;/strong&gt;：agent做出的动作&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;reward&lt;/strong&gt;：agent做出一个动作后环境给予的奖励&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;action space&lt;/strong&gt;：可以选择的动作，如上下左右&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;policy&lt;/strong&gt;：策略函数，输入state，输出Action的&lt;strong&gt;概率分布&lt;/strong&gt;。一般用π表示。&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;训练时应尝试各种action&lt;/li&gt;&#xA;&lt;li&gt;输出应具有多样性&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Trajectory/Episode/Rollout&lt;/strong&gt;：轨迹，用t表示，一连串状态和动作的序列。&lt;/p&gt;&#xA;&lt;p&gt;有的状态转移是确定的，也有的是不确定的。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Return&lt;/strong&gt;：回报，从当前时间点到游戏结束的 Reward 的累积和。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;强化学习目标：训练一个Policy神经网络π，在所有状态S下，给出相应的Action，得到Return的期望最大。&lt;/p&gt;</description>
    </item>
    <item>
      <title>微调</title>
      <link>http://localhost:1313/post/%E5%BE%AE%E8%B0%83/</link>
      <pubDate>Wed, 19 Mar 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/%E5%BE%AE%E8%B0%83/</guid>
      <description>&lt;h2 id=&#34;大模型预训练&#34;&gt;大模型预训练&lt;/h2&gt;&#xA;&lt;h4 id=&#34;1-从零开始的预训练&#34;&gt;1 从零开始的预训练&lt;/h4&gt;&#xA;&lt;h4 id=&#34;2-在已有开源模型基础上针对特定任务进行训练&#34;&gt;2 在已有开源模型基础上针对特定任务进行训练&lt;/h4&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h3 id=&#34;lora&#34;&gt;LoRa&lt;/h3&gt;&#xA;&lt;p&gt;通过化简权重矩阵，实现高效微调&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/Users/aijunyang/DearAJ.github.io/static/images/preTrain/1.png&#34; alt=&#34;1&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;将loraA与loraB相乘得到一个lora权重矩阵，将lora权重矩阵加在原始权重矩阵上，就得到了对原始网络的更新。&lt;/p&gt;&#xA;&lt;p&gt;训练参数量减少，但微调效果基本不变。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;两个重要参数：&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/Users/aijunyang/DearAJ.github.io/static/images/preTrain/2.png&#34; alt=&#34;2&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Lora 的&lt;strong&gt;优点&lt;/strong&gt;：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;﻿﻿﻿大大节省微调大模型的参数量&lt;/li&gt;&#xA;&lt;li&gt;﻿﻿﻿效果和全量微调差不多。&lt;/li&gt;&#xA;&lt;li&gt;﻿﻿微调完的Lora模型，权重可以Merge回原来的权重，不会改变模型结构，推理时不增加额外计算量。&lt;/li&gt;&#xA;&lt;li&gt;﻿﻿你可以通过改变r参数，最高情况等同于全量微调。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h2 id=&#34;大模型微调sft-trainer&#34;&gt;大模型微调：SFT Trainer&lt;/h2&gt;&#xA;&lt;p&gt;SFT：Supervised Fine-Tuning&lt;/p&gt;&#xA;&lt;h4 id=&#34;1-chat-tempate&#34;&gt;1 Chat Tempate&lt;/h4&gt;&#xA;&lt;p&gt;在预训练基础上，更好回答人类的问题。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;网络结构：基本相同&lt;/li&gt;&#xA;&lt;li&gt;loss函数：基本相同&lt;/li&gt;&#xA;&lt;li&gt;训练数据：不同&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;微调时，格式需要与原厂一致效果更好。&lt;/p&gt;&#xA;&lt;p&gt;输入整个对话，对整个对话文本进行学习，&lt;strong&gt;对每个输出token计算loss&lt;/strong&gt;。&lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h4 id=&#34;2-completions-only&#34;&gt;2 Completions only&lt;/h4&gt;&#xA;&lt;p&gt;输入整个对话，对整个对话文本进行学习，&lt;strong&gt;只对回答部分进行loss计算&lt;/strong&gt;。&lt;/p&gt;&#xA;&lt;p&gt;用一个loss mask来实现，query部分不需要计算loss。&lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h4 id=&#34;3-neftunenoisy-embeddings-finetuning&#34;&gt;3 NEFTune：Noisy Embeddings Finetuning&lt;/h4&gt;&#xA;&lt;p&gt;图像领域的数据增强：通过旋转等操作生成更多样本。&lt;/p&gt;&#xA;&lt;p&gt;对文本数据的增强：给每个token的embeding随机加上一些噪声，让原有token变成其他相近的token。如，&lt;em&gt;漂亮&lt;/em&gt;变成了&lt;em&gt;美丽&lt;/em&gt;。&lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;</description>
    </item>
    <item>
      <title>Lift-splat-shoot</title>
      <link>http://localhost:1313/post/lift-splat-shoot/</link>
      <pubDate>Fri, 21 Feb 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/lift-splat-shoot/</guid>
      <description>&lt;h3 id=&#34;1-语义分割&#34;&gt;1. 语义分割&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;目标&lt;/strong&gt;：将图像中的每个像素分配一个语义类别标签。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;输入&lt;/strong&gt;：一张RGB图像（或其他类型的图像，如深度图、红外图等）。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;输出&lt;/strong&gt;：像素级标签图，标注出道路、车辆、行人、交通标志等类别。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;在自动驾驶中，多个传感器作为输入，每个传感器都有不同的坐标系，感知模型最终的任务是在**新的坐标系（自我汽车的坐标系）**中产生预测，供下游规划者使用。&lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h3 id=&#34;2-论文阅读&#34;&gt;2. 论文阅读&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2008.05711&#34;&gt;Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;本文提出了一种架构，旨在从任意摄像机装备推断鸟瞰图表示。&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;h4 id=&#34;introduction&#34;&gt;Introduction&lt;/h4&gt;&#xA;&lt;p&gt;目标：从任意数量的摄像机中直接提取给定图像数据的场景的鸟瞰图表示。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;单视图扩展成多视图的对称性：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;平移等方差&lt;/strong&gt;： 如果图像中的像素坐标全部偏移，则输出将偏移相同的量。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Permutation invariance&lt;/strong&gt;： 最终输出不取决于 n 相机的特定顺序。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;自我框架等距等方差&lt;/strong&gt;： 无论捕获图像的相机相对于自我汽车的位置如何，都会在给定图像中检测到相同的对象。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;缺点：反向传播不能用于使用来自下游规划器的反馈来自动改进感知系统。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;传统在与输入图像相同的坐标系中进行预测，我们的模型遵循上述对称性，直接在给定的鸟瞰图框架中进行预测，以便从多视图图像进行端到端规划。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>强化学习-基础工具（1-3）</title>
      <link>http://localhost:1313/post/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E5%9F%BA%E7%A1%80%E5%B7%A5%E5%85%B7/</link>
      <pubDate>Wed, 19 Feb 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E5%9F%BA%E7%A1%80%E5%B7%A5%E5%85%B7/</guid>
      <description>&lt;h2 id=&#34;总述&#34;&gt;总述&lt;/h2&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/RL2/1.png&#34; alt=&#34;1&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;h4 id=&#34;基础工具&#34;&gt;&lt;strong&gt;基础工具&lt;/strong&gt;&lt;/h4&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;基本概念&lt;/strong&gt;：state, action, reward, return, episode, policy, mdp&amp;hellip;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;贝尔曼公式&lt;/strong&gt;：用于评价策略&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;贝尔曼最优公式&lt;/strong&gt;：强化学习的最终目标是求解最优策略&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h4 id=&#34;算法方法&#34;&gt;算法/方法&lt;/h4&gt;&#xA;&lt;ol start=&#34;4&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;值迭代、策略迭代—— truncated policy iteration&lt;/strong&gt;：值和策略update不断迭代&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Monte Carlo Learning&lt;/strong&gt;：无模型学习&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;随即近似理论&lt;/strong&gt;：from non-incremental to incremental&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;时序差分方法(TD)&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;值函数估计&lt;/strong&gt;：tabular representation to function representation，引入神经网络&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Policy Gradient Methods&lt;/strong&gt;：from value-based to policy-based&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Actor-Critic Methods&lt;/strong&gt;：policy-based + value-based&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h2 id=&#34;1-基本概念&#34;&gt;1 基本概念&lt;/h2&gt;&#xA;&lt;h4 id=&#34;1-专有名词&#34;&gt;1. 专有名词&lt;/h4&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;grid-world&lt;/strong&gt;：小机器人在网格里走路&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;state&lt;/strong&gt;：agent在环境中的状态，用s1、s2&amp;hellip;表示；s是列向量，可表示速度、加速度等&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;state space&lt;/strong&gt;：即把所有的state放在一起的集合&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;action&lt;/strong&gt;：可采取的行动，如往上走、往右走&amp;hellip;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;action-space&lt;/strong&gt;：所有的action放在一起的集合，用A表示&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;state transition&lt;/strong&gt;：采取一个action后，从一个state转到另一个state的过程；定义了agent与环境的一种交互行为&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;forbidden area&lt;/strong&gt;：进去后受到惩罚/不可进入&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>强化学习</title>
      <link>http://localhost:1313/post/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</link>
      <pubDate>Thu, 02 Jan 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</guid>
      <description>&lt;p&gt;不用告诉该怎么做，而是给定奖励函数，什么时候做好。&lt;/p&gt;&#xA;&lt;h3 id=&#34;回归&#34;&gt;回归&lt;/h3&gt;&#xA;&lt;p&gt;增加折现因子&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/RL/1.png&#34; alt=&#34;1&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/RL/2.png&#34; alt=&#34;2&#34;&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;强化学习的形式化&#34;&gt;强化学习的形式化&lt;/h3&gt;&#xA;&lt;p&gt;A policy is a function $\pi(s) = a$ mapping from states to actions, that tells you what $action \space a$ to take in a given $state \space s$.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;goal&lt;/strong&gt;: Find a $policy \space \pi$ that tells you what $action (a = (s))$ to take in every $state (s)$ so as to maximize the return.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/RL/3.png&#34; alt=&#34;3&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;状态动作值函数&lt;/strong&gt;（Q-Function）&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Q(s,a)&lt;/strong&gt; = Return if you:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;start in state &lt;em&gt;s&lt;/em&gt;.&lt;/li&gt;&#xA;&lt;li&gt;take action &lt;em&gt;a&lt;/em&gt; (once).&lt;/li&gt;&#xA;&lt;li&gt;then behave optimally after that.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;The best possible return from state s is max$Q(s, a)$. The best possible action in state s is the action a that gives max$Q(s, a)$.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>End2End</title>
      <link>http://localhost:1313/post/e2e/</link>
      <pubDate>Thu, 26 Dec 2024 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/e2e/</guid>
      <description>&lt;p&gt;对于由多个阶段组成的学习系统，端到端学习捕获所有阶段，将其替代为单个神经网络。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;优点：&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Let the data speak&lt;/li&gt;&#xA;&lt;li&gt;Less hand-designing of components needed&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;缺点：&#xA;&lt;ul&gt;&#xA;&lt;li&gt;May need large amount of data&lt;/li&gt;&#xA;&lt;li&gt;Excludes potentially useful hand-designed components&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;strong&gt;关键&lt;/strong&gt;：是否有足够的数据&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/e2e/1.png&#34; alt=&#34;1&#34;&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
