<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>RL on HomePage</title>
    <link>http://localhost:1313/tags/rl/</link>
    <description>Recent content in RL on HomePage</description>
    <generator>Hugo</generator>
    <language>en-US</language>
    <lastBuildDate>Thu, 03 Apr 2025 04:00:59 -0700</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/rl/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>PPO</title>
      <link>http://localhost:1313/post/ppo/</link>
      <pubDate>Thu, 03 Apr 2025 04:00:59 -0700</pubDate>
      <guid>http://localhost:1313/post/ppo/</guid>
      <description>&lt;h2 id=&#34;1-论文详读&#34;&gt;1. 论文详读&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1707.06347&#34;&gt;Proximal Policy Optimization Algorithms&lt;/a&gt;（Proximal：近似）&lt;/p&gt;&#xA;&lt;h2 id=&#34;2-ppo&#34;&gt;2. PPO&lt;/h2&gt;&#xA;&lt;h4 id=&#34;回顾-trpo&#34;&gt;回顾 TRPO&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;使用 KL 散度约束 policy 的更新幅度；使用重要性采样&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;缺点&lt;/strong&gt;：近似会带来误差（重要性采样的通病）；解带约束的优化问题困难&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h4 id=&#34;ppo-的改进&#34;&gt;PPO 的改进&lt;/h4&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;TRPO 采用重要性采样 &amp;mdash;-&amp;gt; PPO 采用 &lt;strong&gt;clip 截断&lt;/strong&gt;，限制新旧策略差异，避免更新过大。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;优势函数 At 选用多步时序差分&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;自适应的 KL 惩罚项&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;Critic网络训练&lt;/strong&gt;：&#xA;&lt;ul&gt;&#xA;&lt;li&gt;通过最小化&lt;code&gt;critic_loss = MSE(critic(states), td_target)&lt;/code&gt;，让critic的价值估计更准确&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Actor网络更新&lt;/strong&gt;：&#xA;&lt;ul&gt;&#xA;&lt;li&gt;TD误差的广义形式（GAE）被用作优势函数，指导策略更新方向&lt;/li&gt;&#xA;&lt;li&gt;优势函数越大，表示该动作比平均表现更好，应被加强&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h2 id=&#34;3-ppo-惩罚&#34;&gt;3. PPO-惩罚&lt;/h2&gt;&#xA;&lt;p&gt;PPO-惩罚（PPO-Penalty）：用拉格朗日乘数法将 KL 散度的限制放进了目标函数中，使其变成了一个无约束的优化问题，在迭代的过程中不断更新 KL 散度前的系数 beta。&lt;/p&gt;</description>
    </item>
    <item>
      <title>TRPO</title>
      <link>http://localhost:1313/post/trpo/</link>
      <pubDate>Wed, 02 Apr 2025 12:00:59 -0500</pubDate>
      <guid>http://localhost:1313/post/trpo/</guid>
      <description>&lt;p&gt;基于策略的方法的缺点：当策略网络是深度模型时沿着策略梯度更新参数，很有可能由于步长太长，策略突然显著变差，进而影响训练效果。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;信任区域策略优化&lt;/strong&gt;（TRPO）算法的核心思想：&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;信任区域&lt;/strong&gt;（trust region）：在这个区域上更新策略时能够得到某种策略性能的安全性保证。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h2 id=&#34;1-策略目标&#34;&gt;1. 策略目标&lt;/h2&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/trpo/1.png&#34; alt=&#34;1&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/trpo/2.png&#34; alt=&#34;2&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Actor-Critic</title>
      <link>http://localhost:1313/post/actor-critic/</link>
      <pubDate>Wed, 02 Apr 2025 01:00:59 -0500</pubDate>
      <guid>http://localhost:1313/post/actor-critic/</guid>
      <description>&lt;p&gt;Actor-Critic 算法本质上是基于策略的算法，因为这一系列算法的&lt;strong&gt;目标都是优化一个带参数的策略，只是会额外学习价值函数&lt;/strong&gt;，从而帮助策略函数更好地学习。&lt;/p&gt;&#xA;&lt;p&gt;Actor-Critic 算法则可以在每一步之后都进行更新，并且不对任务的步数做限制。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;更一般形式的策略梯度&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/ac/1.png&#34; alt=&#34;1&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h3 id=&#34;1-actor策略网络&#34;&gt;1. Actor（策略网络）&lt;/h3&gt;&#xA;&lt;p&gt;Actor 要做的是与环境交互，并在 Critic 价值函数的指导下用策略梯度学习一个更好的策略。&lt;/p&gt;&#xA;&lt;p&gt;Actor 的更新采用策略梯度的原则。&lt;/p&gt;&#xA;&lt;h3 id=&#34;2--critic价值网络&#34;&gt;2.  Critic（价值网络）&lt;/h3&gt;&#xA;&lt;p&gt;Critic 要做的是通过 Actor 与环境交互收集的数据学习一个价值函数，这个价值函数会用于判断在当前状态什么动作是好的，什么动作不是好的，进而帮助 Actor 进行策略更新。&lt;/p&gt;</description>
    </item>
    <item>
      <title>REINFORCE</title>
      <link>http://localhost:1313/post/reinforce/</link>
      <pubDate>Tue, 01 Apr 2025 12:00:59 -0500</pubDate>
      <guid>http://localhost:1313/post/reinforce/</guid>
      <description>&lt;p&gt;Q-learning、DQN 算法都是&lt;strong&gt;基于价值&lt;/strong&gt;（value-based）的方法&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Q-learning 是处理有限状态的算法&lt;/li&gt;&#xA;&lt;li&gt;DQN 可以用来解决连续状态的问题&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;在强化学习中，除了基于值函数的方法，还有一支非常经典的方法，那就是&lt;strong&gt;基于策略&lt;/strong&gt;（policy-based）的方法。&lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p&gt;对比 value-based 和 policy-based&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;基于值函数：主要是学习&lt;strong&gt;值函数&lt;/strong&gt;，然后根据值函数导出一个策略，学习过程中并不存在一个显式的策略；&lt;/li&gt;&#xA;&lt;li&gt;基于策略：直接显式地学习一个&lt;strong&gt;目标策略&lt;/strong&gt;。策略梯度是基于策略的方法的基础。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h2 id=&#34;1-策略梯度&#34;&gt;1. 策略梯度&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;将策略参数化：寻找一个最优策略并最大化这个策略在环境中的期望回报，即调整策略参数使平均回报最大化。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;策略学习的目标函数&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/REINFORCE/1.png&#34; alt=&#34;1&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;J(θ) 是策略的目标函数（想要最大化的量）；&lt;/li&gt;&#xA;&lt;li&gt;πθ 是参数为θ的随机性策略，并且处处可微（可以理解为AI的决策规则）；&lt;/li&gt;&#xA;&lt;li&gt;Vπθ(s0) 指从初始状态s₀开始&lt;strong&gt;遵循策略π能获得的预期总回报&lt;/strong&gt;；&lt;/li&gt;&#xA;&lt;li&gt;Es0 是对所有可能的初始状态求期望。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>DQN (deep Q network)</title>
      <link>http://localhost:1313/post/dqn/</link>
      <pubDate>Tue, 01 Apr 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/dqn/</guid>
      <description>&lt;p&gt;Q-learning 算法用表格存储动作价值的做法只在 环境的状态和动作都是离散的，并且空间都比较小 的情况下适用.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;DQN&lt;/strong&gt;：用来解决连续状态下离散动作的问题，是离线策略算法，可以使用ε-贪婪策略来平衡探索与利用。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Q 网络&lt;/strong&gt;：用于拟合函数Q函数的神经网络&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/DQN/1.png&#34; alt=&#34;1&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Q 网络的损失函数&lt;/strong&gt;（均方误差形式）&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/DQN/2.png&#34; alt=&#34;2&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Q-learing</title>
      <link>http://localhost:1313/post/qlearing/</link>
      <pubDate>Mon, 31 Mar 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/qlearing/</guid>
      <description>&lt;p&gt;&lt;strong&gt;无模型的强化学习&lt;/strong&gt;：不需要事先知道环境的奖励函数和状态转移函数，而是直接使用和环境交互的过程中采样到的数据来学习。&lt;/p&gt;&#xA;&lt;h2 id=&#34;1-时序差分方法&#34;&gt;1. 时序差分方法&lt;/h2&gt;&#xA;&lt;p&gt;时序差分方法核心：对未来动作选择的价值估计来更新对当前动作选择的价值估计。&lt;/p&gt;&#xA;&lt;h3 id=&#34;蒙特卡洛方法monte-carlo-methods&#34;&gt;蒙特卡洛方法（Monte-Carlo methods）&lt;/h3&gt;&#xA;&lt;p&gt;使用重复随机抽样，然后运用概率统计方法来从抽样结果中归纳出我们想求的目标的数值估计。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;用蒙特卡洛方法来估计一个策略在一个马尔可夫决策过程中的状态价值函数：&lt;strong&gt;用样本均值作为期望值的估计&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;在 MDP 上采样很多条序列，计算从这个状态出发的回报再求其期望&lt;/li&gt;&#xA;&lt;li&gt;一条序列只计算一次回报，也就是这条序列第一次出现该状态时计算后面的累积奖励，而后面再次出现该状态时，该状态就被忽略了。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;蒙特卡洛方法对价值函数的增量更新方式&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/QLearning/1.png&#34; alt=&#34;1&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;时序差分方法只需要当前步结束即可进行计算&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>强化学习-易混淆点</title>
      <link>http://localhost:1313/post/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E6%98%93%E6%B7%B7%E6%B7%86%E7%82%B9/</link>
      <pubDate>Mon, 31 Mar 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E6%98%93%E6%B7%B7%E6%B7%86%E7%82%B9/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;状态价值函数-vs-动作价值函数&#34;&gt;状态价值函数 vs 动作价值函数&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;状态价值函数&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/RLconfused/1.png&#34; alt=&#34;1&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;动作价值函数&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/RLconfused/2.png&#34; alt=&#34;2&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;优势函数&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;在状态 s 下选择动作 a 比平均情况（即遵循当前策略）好多少&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;A(s,a)=Q(s,a)−V(s)&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;求解优势函数：&lt;strong&gt;广义优势估计&lt;/strong&gt;(GAE)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;广义优势估计&lt;/strong&gt;(GAE)&lt;/p&gt;&#xA;&lt;p&gt;通过指数加权平均不同步长的优势估计（从1步到无穷步），结合γ&lt;em&gt;γ&lt;/em&gt;和λ&lt;em&gt;λ&lt;/em&gt;的幂次衰减，实现平滑的回报估计。&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/RLconfused/5.png&#34; alt=&#34;5&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;关系&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/RLconfused/3.png&#34; alt=&#34;3&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/RLconfused/4.png&#34; alt=&#34;4&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;ol start=&#34;2&#34;&gt;&#xA;&lt;li&gt;&#xA;&lt;h3 id=&#34;常见强化学习算法优缺点&#34;&gt;常见强化学习算法优缺点&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Q-Learning&lt;/strong&gt; - Off-policy - 值函数&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/QLearning/6.png&#34; alt=&#34;6&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;缺点：用表格存储动作价值。只在 环境的状态和动作都是离散的，并且空间都比较小 的情况下适用。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>PPO-直观理解</title>
      <link>http://localhost:1313/post/ppo-%E5%8E%9F%E7%90%86/</link>
      <pubDate>Sat, 29 Mar 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/ppo-%E5%8E%9F%E7%90%86/</guid>
      <description>&lt;h2 id=&#34;1-基础概念&#34;&gt;1. 基础概念&lt;/h2&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/PPO/1.png&#34; alt=&#34;1&#34;&gt;&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;enviroment&lt;/strong&gt;：看到的画面+看不到的后台画面，不了解细节&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;agent(智能体)&lt;/strong&gt;：根据策略得到尽可能多的奖励&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;state&lt;/strong&gt;：当前状态&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;observation&lt;/strong&gt;：state的一部分（有时候agent无法看全）&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;action&lt;/strong&gt;：agent做出的动作&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;reward&lt;/strong&gt;：agent做出一个动作后环境给予的奖励&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;action space&lt;/strong&gt;：可以选择的动作，如上下左右&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;policy&lt;/strong&gt;：策略函数，输入state，输出Action的&lt;strong&gt;概率分布&lt;/strong&gt;。一般用π表示。&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;训练时应尝试各种action&lt;/li&gt;&#xA;&lt;li&gt;输出应具有多样性&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Trajectory/Episode/Rollout&lt;/strong&gt;：轨迹，用 t 表示一连串状态和动作的序列。有的状态转移是确定的，也有的是不确定的。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Return&lt;/strong&gt;：回报，从当前时间点到游戏结束的 Reward 的累积和。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;强化学习目标：训练一个Policy神经网络π，在所有状态S下，给出相应的Action，得到Return的期望最大。&lt;/p&gt;</description>
    </item>
    <item>
      <title>微调</title>
      <link>http://localhost:1313/post/%E5%BE%AE%E8%B0%83/</link>
      <pubDate>Wed, 19 Mar 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/%E5%BE%AE%E8%B0%83/</guid>
      <description>&lt;h2 id=&#34;大模型预训练&#34;&gt;大模型预训练&lt;/h2&gt;&#xA;&lt;h4 id=&#34;1-从零开始的预训练&#34;&gt;1 从零开始的预训练&lt;/h4&gt;&#xA;&lt;h4 id=&#34;2-在已有开源模型基础上针对特定任务进行训练&#34;&gt;2 在已有开源模型基础上针对特定任务进行训练&lt;/h4&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h3 id=&#34;lora&#34;&gt;LoRa&lt;/h3&gt;&#xA;&lt;p&gt;通过化简权重矩阵，实现高效微调&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/preTrain/1.png&#34; alt=&#34;1&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;将loraA与loraB相乘得到一个lora权重矩阵，将lora权重矩阵加在原始权重矩阵上，就得到了对原始网络的更新。&lt;/p&gt;&#xA;&lt;p&gt;训练参数量减少，但微调效果基本不变。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;两个重要参数：&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/preTrain/2.png&#34; alt=&#34;2&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>强化学习-数学基础</title>
      <link>http://localhost:1313/post/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/</link>
      <pubDate>Wed, 19 Feb 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/</guid>
      <description>&lt;h2 id=&#34;总述&#34;&gt;总述&lt;/h2&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/RL2/1.png&#34; alt=&#34;1&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;h4 id=&#34;基础工具&#34;&gt;&lt;strong&gt;基础工具&lt;/strong&gt;&lt;/h4&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;基本概念&lt;/strong&gt;：state, action, reward, return, episode, policy, mdp&amp;hellip;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;贝尔曼公式&lt;/strong&gt;：用于评价策略&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;贝尔曼最优公式&lt;/strong&gt;：强化学习的最终目标是求解最优策略&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h4 id=&#34;算法方法&#34;&gt;算法/方法&lt;/h4&gt;&#xA;&lt;ol start=&#34;4&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;值迭代、策略迭代—— truncated policy iteration&lt;/strong&gt;：值和策略update不断迭代&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Monte Carlo Learning&lt;/strong&gt;：无模型学习&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;随即近似理论&lt;/strong&gt;：from non-incremental to incremental&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;时序差分方法(TD)&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;值函数估计&lt;/strong&gt;：tabular representation to function representation，引入神经网络&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Policy Gradient Methods&lt;/strong&gt;：from value-based to policy-based&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Actor-Critic Methods&lt;/strong&gt;：policy-based + value-based&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>强化学习-直观理解</title>
      <link>http://localhost:1313/post/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3/</link>
      <pubDate>Thu, 02 Jan 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3/</guid>
      <description>&lt;p&gt;不用告诉该怎么做，而是给定奖励函数，什么时候做好。&lt;/p&gt;&#xA;&lt;h3 id=&#34;回归&#34;&gt;回归&lt;/h3&gt;&#xA;&lt;p&gt;增加折现因子&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/RL/1.png&#34; alt=&#34;1&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/RL/2.png&#34; alt=&#34;2&#34;&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;强化学习的形式化&#34;&gt;强化学习的形式化&lt;/h3&gt;&#xA;&lt;p&gt;A policy is a function $\pi(s) = a$ mapping from states to actions, that tells you what $action \space a$ to take in a given $state \space s$.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;goal&lt;/strong&gt;: Find a $policy \space \pi$ that tells you what $action (a = (s))$ to take in every $state (s)$ so as to maximize the return.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/RL/3.png&#34; alt=&#34;3&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;状态动作值函数&lt;/strong&gt;（Q-Function）&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Q(s,a)&lt;/strong&gt; = Return if you:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;start in state &lt;em&gt;s&lt;/em&gt;.&lt;/li&gt;&#xA;&lt;li&gt;take action &lt;em&gt;a&lt;/em&gt; (once).&lt;/li&gt;&#xA;&lt;li&gt;then behave optimally after that.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;The best possible return from state s is max$Q(s, a)$. The best possible action in state s is the action a that gives max$Q(s, a)$.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>End2End</title>
      <link>http://localhost:1313/post/e2e/</link>
      <pubDate>Thu, 26 Dec 2024 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/e2e/</guid>
      <description>&lt;p&gt;对于由多个阶段组成的学习系统，端到端学习捕获所有阶段，将其替代为单个神经网络。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;优点：&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Let the data speak&lt;/li&gt;&#xA;&lt;li&gt;Less hand-designing of components needed&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;缺点：&#xA;&lt;ul&gt;&#xA;&lt;li&gt;May need large amount of data&lt;/li&gt;&#xA;&lt;li&gt;Excludes potentially useful hand-designed components&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;strong&gt;关键&lt;/strong&gt;：是否有足够的数据&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/e2e/1.png&#34; alt=&#34;1&#34;&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
