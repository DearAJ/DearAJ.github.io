<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>RL on HomePage</title>
    <link>http://localhost:1313/tags/rl/</link>
    <description>Recent content in RL on HomePage</description>
    <generator>Hugo</generator>
    <language>en-US</language>
    <lastBuildDate>Wed, 22 Jan 2025 11:00:59 -0400</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/rl/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>World Models</title>
      <link>http://localhost:1313/post/world-model/</link>
      <pubDate>Wed, 22 Jan 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/world-model/</guid>
      <description></description>
    </item>
    <item>
      <title>Reinforcement Learning</title>
      <link>http://localhost:1313/post/rl/</link>
      <pubDate>Thu, 02 Jan 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/rl/</guid>
      <description>&lt;p&gt;不用告诉该怎么做，而是给定奖励函数，什么时候做好。&lt;/p&gt;&#xA;&lt;h3 id=&#34;回归&#34;&gt;回归&lt;/h3&gt;&#xA;&lt;p&gt;增加折现因子&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/RL/1.png&#34; alt=&#34;1&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/RL/2.png&#34; alt=&#34;2&#34;&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;强化学习的形式化&#34;&gt;强化学习的形式化&lt;/h3&gt;&#xA;&lt;p&gt;A policy is a function $\pi(s) = a$ mapping from states to actions, that tells you what $action \space a$ to take in a given $state \space s$.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;goal&lt;/strong&gt;: Find a $policy \space \pi$ that tells you what $action (a = (s))$ to take in every $state (s)$ so as to maximize the return.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/RL/3.png&#34; alt=&#34;3&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;状态动作值函数&lt;/strong&gt;（Q-Function）&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Q(s,a)&lt;/strong&gt; = Return if you:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;start in state &lt;em&gt;s&lt;/em&gt;.&lt;/li&gt;&#xA;&lt;li&gt;take action &lt;em&gt;a&lt;/em&gt; (once).&lt;/li&gt;&#xA;&lt;li&gt;then behave optimally after that.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;The best possible return from state s is max$Q(s, a)$. The best possible action in state s is the action a that gives max$Q(s, a)$.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>强化学习-进阶</title>
      <link>http://localhost:1313/post/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E8%BF%9B%E9%98%B6/</link>
      <pubDate>Thu, 02 Jan 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E8%BF%9B%E9%98%B6/</guid>
      <description>&lt;h2 id=&#34;总述&#34;&gt;总述&lt;/h2&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/RL2/1.png&#34; alt=&#34;1&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;h4 id=&#34;基础工具&#34;&gt;&lt;strong&gt;基础工具&lt;/strong&gt;&lt;/h4&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;基本概念&lt;/strong&gt;：state, action, reward, return, episode, policy, mdp&amp;hellip;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;贝尔曼公式&lt;/strong&gt;：用于评价策略&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;贝尔曼最优公式&lt;/strong&gt;：强化学习的最终目标是求解最优策略&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h4 id=&#34;算法方法&#34;&gt;算法/方法&lt;/h4&gt;&#xA;&lt;ol start=&#34;4&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;值迭代、策略迭代—— truncated policy iteration&lt;/strong&gt;：值和策略update不断迭代&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Monte Carlo Learning&lt;/strong&gt;：无模型学习&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;随即近似理论&lt;/strong&gt;：from non-incremental to incremental&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;时序差分方法(TD)&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;值函数估计&lt;/strong&gt;：tabular representation to function representation，引入神经网络&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Policy Gradient Methods&lt;/strong&gt;：from value-based to policy-based&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Actor-Critic Methods&lt;/strong&gt;：policy-based + value-based&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h2 id=&#34;1-基本概念&#34;&gt;1 基本概念&lt;/h2&gt;&#xA;&lt;h4 id=&#34;1-专有名词&#34;&gt;1. 专有名词&lt;/h4&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;grid-world&lt;/strong&gt;：小机器人在网格里走路&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;state&lt;/strong&gt;：agent在环境中的状态，用s1、s2&amp;hellip;表示；s是列向量，可表示速度、加速度等&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;state space&lt;/strong&gt;：即把所有的state放在一起的集合&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;action&lt;/strong&gt;：可采取的行动，如往上走、往右走&amp;hellip;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;action-space&lt;/strong&gt;：所有的action放在一起的集合，用A表示&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;state transition&lt;/strong&gt;：采取一个action后，从一个state转到另一个state的过程；定义了agent与环境的一种交互行为&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;forbidden area&lt;/strong&gt;：进去后受到惩罚/不可进入&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;tabular representation&lt;/strong&gt;：使用表格描述state transition&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;state transition probability&lt;/strong&gt;：使用条件概率来表述tate transition，用于描述随机性&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;policy&lt;/strong&gt;：使用箭头表示，告诉agent在某state时应该采取哪个action。基于policy，可以得到path&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;mathematical representation&lt;/strong&gt;：条件概率，用π表示某状态对应的策略&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;stochastic policies&lt;/strong&gt;：某状态对应多个不同概率的action&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;reward：一个数(标量)；正奖励负惩罚&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/RL2/2.png&#34; alt=&#34;2&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>End2End</title>
      <link>http://localhost:1313/post/e2e/</link>
      <pubDate>Thu, 26 Dec 2024 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/e2e/</guid>
      <description>&lt;p&gt;对于由多个阶段组成的学习系统，端到端学习捕获所有阶段，将其替代为单个神经网络。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;优点：&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Let the data speak&lt;/li&gt;&#xA;&lt;li&gt;Less hand-designing of components needed&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;缺点：&#xA;&lt;ul&gt;&#xA;&lt;li&gt;May need large amount of data&lt;/li&gt;&#xA;&lt;li&gt;Excludes potentially useful hand-designed components&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;strong&gt;关键&lt;/strong&gt;：是否有足够的数据&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/e2e/1.png&#34; alt=&#34;1&#34;&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
