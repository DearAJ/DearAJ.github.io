<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>RL on HomePage</title>
    <link>http://localhost:1313/tags/rl/</link>
    <description>Recent content in RL on HomePage</description>
    <generator>Hugo</generator>
    <language>en-US</language>
    <lastBuildDate>Wed, 22 Jan 2025 11:00:59 -0400</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/rl/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>World Models</title>
      <link>http://localhost:1313/post/world-model/</link>
      <pubDate>Wed, 22 Jan 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/world-model/</guid>
      <description>&lt;h1 id=&#34;sem2&#34;&gt;SEM2&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.04017&#34;&gt;https://arxiv.org/abs/2210.04017&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;题目摘要解读&#34;&gt;题目＆摘要解读&lt;/h2&gt;&#xA;&lt;h2 id=&#34;背景相关工作&#34;&gt;背景&amp;amp;相关工作&lt;/h2&gt;&#xA;&lt;h2 id=&#34;方法&#34;&gt;﻿﻿方法&lt;/h2&gt;&#xA;&lt;h2 id=&#34;实验&#34;&gt;实验&lt;/h2&gt;&#xA;&lt;h2 id=&#34;回顾总结思考&#34;&gt;回顾、总结、思考&lt;/h2&gt;</description>
    </item>
    <item>
      <title>Reinforcement Learning</title>
      <link>http://localhost:1313/post/rl/</link>
      <pubDate>Thu, 02 Jan 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/rl/</guid>
      <description>&lt;p&gt;The core idea is &lt;strong&gt;trial-and-error learning&lt;/strong&gt;: the agent takes actions, observes the outcomes, and receives &lt;strong&gt;rewards&lt;/strong&gt; or &lt;strong&gt;penalties&lt;/strong&gt; as feedback. Over time, the agent improves its &lt;strong&gt;policy&lt;/strong&gt; (decision-making strategy) to maximize cumulative rewards.&lt;/p&gt;&#xA;&lt;p&gt;不用告诉该怎么做，而是给定奖励函数，什么时候做好。&lt;/p&gt;&#xA;&lt;h3 id=&#34;回归&#34;&gt;回归&lt;/h3&gt;&#xA;&lt;p&gt;增加折现因子&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/RL/1.png&#34; alt=&#34;1&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/RL/2.png&#34; alt=&#34;2&#34;&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;强化学习的形式化&#34;&gt;强化学习的形式化&lt;/h3&gt;&#xA;&lt;p&gt;A policy is a function $\pi(s) = a$ mapping from states to actions, that tells you what $action \space a$ to take in a given $state \space s$.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;goal&lt;/strong&gt;: Find a $policy \space \pi$ that tells you what $action (a = (s))$ to take in every $state (s)$ so as to maximize the return.&lt;/p&gt;</description>
    </item>
    <item>
      <title>VAE</title>
      <link>http://localhost:1313/post/vae/</link>
      <pubDate>Sun, 29 Dec 2024 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/vae/</guid>
      <description>&lt;h3 id=&#34;key-concepts-of-vaes&#34;&gt;Key Concepts of VAEs&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;Latent Variables&lt;/strong&gt;: VAEs assume that the observed data (e.g., images) is generated from a set of unobserved, lower-dimensional latent variables.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Probabilistic Framework&lt;/strong&gt;: VAEs are based on &lt;strong&gt;variational inference&lt;/strong&gt;, a method for approximating complex probability distributions.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Encoder-Decoder Architecture&lt;/strong&gt;:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Encoder&lt;/strong&gt;: Maps input data x&lt;em&gt;x&lt;/em&gt; to a distribution over latent variables z&lt;em&gt;z&lt;/em&gt;. This is often parameterized as a Gaussian distribution with mean μ&lt;em&gt;μ&lt;/em&gt; and variance σ2&lt;em&gt;σ&lt;/em&gt;2.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Decoder&lt;/strong&gt;: Maps latent variables z&lt;em&gt;z&lt;/em&gt; back to the data space, generating new samples x′&lt;em&gt;x&lt;/em&gt;′ that resemble the original data.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;普通自动编码器&#34;&gt;普通自动编码器&lt;/h2&gt;&#xA;&lt;p&gt;目标：将高维度数据压缩成较小的表示&lt;/p&gt;</description>
    </item>
    <item>
      <title>End2End</title>
      <link>http://localhost:1313/post/e2e/</link>
      <pubDate>Thu, 26 Dec 2024 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/e2e/</guid>
      <description>&lt;p&gt;对于由多个阶段组成的学习系统，端到端学习捕获所有阶段，将其替代为单个神经网络。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;优点：&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Let the data speak&lt;/li&gt;&#xA;&lt;li&gt;Less hand-designing of components needed&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;缺点：&#xA;&lt;ul&gt;&#xA;&lt;li&gt;May need large amount of data&lt;/li&gt;&#xA;&lt;li&gt;Excludes potentially useful hand-designed components&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;strong&gt;关键&lt;/strong&gt;：是否有足够的数据&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/e2e/1.png&#34; alt=&#34;1&#34;&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
