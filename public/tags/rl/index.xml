<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>RL on HomePage</title>
    <link>http://localhost:1313/tags/rl/</link>
    <description>Recent content in RL on HomePage</description>
    <generator>Hugo</generator>
    <language>en-US</language>
    <lastBuildDate>Thu, 20 Feb 2025 11:00:59 -0400</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/rl/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>PPO</title>
      <link>http://localhost:1313/post/ppo/</link>
      <pubDate>Thu, 20 Feb 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/ppo/</guid>
      <description>&lt;h3 id=&#34;1-基础概念&#34;&gt;1. 基础概念&lt;/h3&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/PPO/1.png&#34; alt=&#34;1&#34;&gt;&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;enviroment&lt;/strong&gt;：看到的画面+看不到的后台画面，不了解细节&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;agent(智能体)&lt;/strong&gt;：根据策略得到尽可能多的奖励&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;state&lt;/strong&gt;：当前状态&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;observation&lt;/strong&gt;：state的一部分（有时候agent无法看全）&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;action&lt;/strong&gt;：agent做出的动作&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;reward&lt;/strong&gt;：agent做出一个动作后环境给予的奖励&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;action space&lt;/strong&gt;：可以选择的动作，如上下左右&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;policy&lt;/strong&gt;：策略函数，输入state，输出Action的&lt;strong&gt;概率分布&lt;/strong&gt;。一般用π表示。&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;训练时应尝试各种action&lt;/li&gt;&#xA;&lt;li&gt;输出应具有多样性&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Trajectory/Episode/Rollout&lt;/strong&gt;：轨迹，用t表示，一连串状态和动作的序列。&lt;/p&gt;&#xA;&lt;p&gt;有的状态转移是确定的，也有的是不确定的。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Return&lt;/strong&gt;：回报，从当前时间点到游戏结束的 Reward 的累积和。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;强化学习目标：训练一个Policy神经网络π，在所有状态S下，给出相应的Action，得到Return的期望最大。&lt;/p&gt;&#xA;&lt;h3 id=&#34;2-policy-gradient&#34;&gt;2. Policy gradient&lt;/h3&gt;&#xA;&lt;p&gt;目标：求return期望的最大值&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;h4 id=&#34;计算过程&#34;&gt;&lt;strong&gt;计算过程&lt;/strong&gt;&lt;/h4&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/PPO/2.png&#34; alt=&#34;2&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/PPO/3.png&#34; alt=&#34;3&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;直观理解：&lt;/p&gt;&#xA;&lt;p&gt;对所有可能的 trajectory 期望最大的梯度。可以用这个梯度乘学习率去更新神经网络里的参数。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;若去掉梯度，则表达式的意义：&lt;/p&gt;&#xA;&lt;p&gt;若一个&lt;strong&gt;trajectory 得到的 return 大于零&lt;/strong&gt;，则&lt;strong&gt;增大&lt;/strong&gt;这个trajectory里所有状态下，采取当前action的概率。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h4 id=&#34;训练policy神经网络&#34;&gt;&lt;strong&gt;训练policy神经网络&lt;/strong&gt;&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;输入&lt;/strong&gt;：当前画面&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;输出&lt;/strong&gt;：action 的概率&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/PPO/4.png&#34; alt=&#34;4&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;玩n场游戏后，得到n个trajectory的最后的return值&lt;/p&gt;&#xA;&lt;p&gt;此时可以得到loss里的所有值，可以进行一个batch训练，来更新policy神经网络&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/PPO/5.png&#34; alt=&#34;5&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;存在问题：大部分时间在采集数据，很慢&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;π&lt;/p&gt;</description>
    </item>
    <item>
      <title>强化学习-基础工具（1-3）</title>
      <link>http://localhost:1313/post/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E5%9F%BA%E7%A1%80%E5%B7%A5%E5%85%B7/</link>
      <pubDate>Wed, 19 Feb 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E5%9F%BA%E7%A1%80%E5%B7%A5%E5%85%B7/</guid>
      <description>&lt;h2 id=&#34;总述&#34;&gt;总述&lt;/h2&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/RL2/1.png&#34; alt=&#34;1&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;h4 id=&#34;基础工具&#34;&gt;&lt;strong&gt;基础工具&lt;/strong&gt;&lt;/h4&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;基本概念&lt;/strong&gt;：state, action, reward, return, episode, policy, mdp&amp;hellip;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;贝尔曼公式&lt;/strong&gt;：用于评价策略&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;贝尔曼最优公式&lt;/strong&gt;：强化学习的最终目标是求解最优策略&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;h4 id=&#34;算法方法&#34;&gt;算法/方法&lt;/h4&gt;&#xA;&lt;ol start=&#34;4&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;值迭代、策略迭代—— truncated policy iteration&lt;/strong&gt;：值和策略update不断迭代&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Monte Carlo Learning&lt;/strong&gt;：无模型学习&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;随即近似理论&lt;/strong&gt;：from non-incremental to incremental&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;时序差分方法(TD)&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;值函数估计&lt;/strong&gt;：tabular representation to function representation，引入神经网络&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Policy Gradient Methods&lt;/strong&gt;：from value-based to policy-based&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Actor-Critic Methods&lt;/strong&gt;：policy-based + value-based&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h2 id=&#34;1-基本概念&#34;&gt;1 基本概念&lt;/h2&gt;&#xA;&lt;h4 id=&#34;1-专有名词&#34;&gt;1. 专有名词&lt;/h4&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;grid-world&lt;/strong&gt;：小机器人在网格里走路&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;state&lt;/strong&gt;：agent在环境中的状态，用s1、s2&amp;hellip;表示；s是列向量，可表示速度、加速度等&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;state space&lt;/strong&gt;：即把所有的state放在一起的集合&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;action&lt;/strong&gt;：可采取的行动，如往上走、往右走&amp;hellip;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;action-space&lt;/strong&gt;：所有的action放在一起的集合，用A表示&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;state transition&lt;/strong&gt;：采取一个action后，从一个state转到另一个state的过程；定义了agent与环境的一种交互行为&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;forbidden area&lt;/strong&gt;：进去后受到惩罚/不可进入&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>强化学习</title>
      <link>http://localhost:1313/post/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</link>
      <pubDate>Thu, 02 Jan 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</guid>
      <description>&lt;p&gt;不用告诉该怎么做，而是给定奖励函数，什么时候做好。&lt;/p&gt;&#xA;&lt;h3 id=&#34;回归&#34;&gt;回归&lt;/h3&gt;&#xA;&lt;p&gt;增加折现因子&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/RL/1.png&#34; alt=&#34;1&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/RL/2.png&#34; alt=&#34;2&#34;&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;强化学习的形式化&#34;&gt;强化学习的形式化&lt;/h3&gt;&#xA;&lt;p&gt;A policy is a function $\pi(s) = a$ mapping from states to actions, that tells you what $action \space a$ to take in a given $state \space s$.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;goal&lt;/strong&gt;: Find a $policy \space \pi$ that tells you what $action (a = (s))$ to take in every $state (s)$ so as to maximize the return.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/RL/3.png&#34; alt=&#34;3&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;状态动作值函数&lt;/strong&gt;（Q-Function）&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Q(s,a)&lt;/strong&gt; = Return if you:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;start in state &lt;em&gt;s&lt;/em&gt;.&lt;/li&gt;&#xA;&lt;li&gt;take action &lt;em&gt;a&lt;/em&gt; (once).&lt;/li&gt;&#xA;&lt;li&gt;then behave optimally after that.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;The best possible return from state s is max$Q(s, a)$. The best possible action in state s is the action a that gives max$Q(s, a)$.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>End2End</title>
      <link>http://localhost:1313/post/e2e/</link>
      <pubDate>Thu, 26 Dec 2024 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/e2e/</guid>
      <description>&lt;p&gt;对于由多个阶段组成的学习系统，端到端学习捕获所有阶段，将其替代为单个神经网络。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;优点：&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Let the data speak&lt;/li&gt;&#xA;&lt;li&gt;Less hand-designing of components needed&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;缺点：&#xA;&lt;ul&gt;&#xA;&lt;li&gt;May need large amount of data&lt;/li&gt;&#xA;&lt;li&gt;Excludes potentially useful hand-designed components&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;strong&gt;关键&lt;/strong&gt;：是否有足够的数据&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/e2e/1.png&#34; alt=&#34;1&#34;&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
