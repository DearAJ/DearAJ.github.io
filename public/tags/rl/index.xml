<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>RL on HomePage</title>
    <link>http://localhost:1313/tags/rl/</link>
    <description>Recent content in RL on HomePage</description>
    <generator>Hugo</generator>
    <language>en-US</language>
    <lastBuildDate>Wed, 22 Jan 2025 11:00:59 -0400</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/rl/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>World Models</title>
      <link>http://localhost:1313/post/world-model/</link>
      <pubDate>Wed, 22 Jan 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/world-model/</guid>
      <description></description>
    </item>
    <item>
      <title>Reinforcement Learning</title>
      <link>http://localhost:1313/post/rl/</link>
      <pubDate>Thu, 02 Jan 2025 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/rl/</guid>
      <description>&lt;p&gt;不用告诉该怎么做，而是给定奖励函数，什么时候做好。&lt;/p&gt;&#xA;&lt;h3 id=&#34;回归&#34;&gt;回归&lt;/h3&gt;&#xA;&lt;p&gt;增加折现因子&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/RL/1.png&#34; alt=&#34;1&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/RL/2.png&#34; alt=&#34;2&#34;&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;强化学习的形式化&#34;&gt;强化学习的形式化&lt;/h3&gt;&#xA;&lt;p&gt;A policy is a function $\pi(s) = a$ mapping from states to actions, that tells you what $action \space a$ to take in a given $state \space s$.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;goal&lt;/strong&gt;: Find a $policy \space \pi$ that tells you what $action (a = (s))$ to take in every $state (s)$ so as to maximize the return.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/RL/3.png&#34; alt=&#34;3&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;状态动作值函数&lt;/strong&gt;（Q-Function）&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Q(s,a)&lt;/strong&gt; = Return if you:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;start in state &lt;em&gt;s&lt;/em&gt;.&lt;/li&gt;&#xA;&lt;li&gt;take action &lt;em&gt;a&lt;/em&gt; (once).&lt;/li&gt;&#xA;&lt;li&gt;then behave optimally after that.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;The best possible return from state s is max$Q(s, a)$. The best possible action in state s is the action a that gives max$Q(s, a)$.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>End2End</title>
      <link>http://localhost:1313/post/e2e/</link>
      <pubDate>Thu, 26 Dec 2024 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/e2e/</guid>
      <description>&lt;p&gt;对于由多个阶段组成的学习系统，端到端学习捕获所有阶段，将其替代为单个神经网络。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;优点：&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Let the data speak&lt;/li&gt;&#xA;&lt;li&gt;Less hand-designing of components needed&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;缺点：&#xA;&lt;ul&gt;&#xA;&lt;li&gt;May need large amount of data&lt;/li&gt;&#xA;&lt;li&gt;Excludes potentially useful hand-designed components&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;strong&gt;关键&lt;/strong&gt;：是否有足够的数据&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/e2e/1.png&#34; alt=&#34;1&#34;&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
